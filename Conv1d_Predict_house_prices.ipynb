{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conv1d Predict house prices.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sBtnKdYEFcVh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmkarakaya/ML_tutorials/blob/master/Conv1d_Predict_house_prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# 1 Dimensional Convolution (Conv1D) for Regression: Predict house prices \n",
        "\n",
        "[In a previous tutorial](https://youtu.be/WZdxt9xatrY), we focus on  **1 Dimensional Convolution (Conv1D)** and discuss how it works in a simple example. ***As I received several questions*** about how to apply 1 Dimensional Convolution onto a regression problem, I develop this notebook. If you need to **refresh your information** about 1 Dimensional Convolution, please **watch** the previous tutorial [on my Youtube channel](https://youtu.be/WZdxt9xatrY)\n",
        "\n",
        "Thus, today, we will use **Keras Conv1d** layer for a regression problem. \n",
        "As you might know, **Boston House Prices** data set is a well known data set. Below, we will **first** train a **Multi-Layer Perceptron (MLP) model** to predict house prices. Then, we will develop a model using **Keras Conv1D** layer. To train and test the Conv1D model, we will **reshape the train and test data** such that Conv1D can work on them.\n",
        "\n",
        "If you are interested in **Deep Neural Networks** and want to **learn them by coding**, please **subcribe** to [my YouTube Channel](https://www.youtube.com/channel/UCrCxCxTFL2ytaDrDYrN4_eA/playlists) or **follow** [my blog on Medium](https://medium.com/@kmkarakaya). Do not forget to turn on **Notifications** so that you will be notified when ***new content is uploaded***.\n",
        "\n",
        "You can access this **Colab Notebook** using [the link](https://colab.research.google.com/drive/1zjh0tUPYJYgJJunpLC9fW5uf--O0LKeZ?usp=sharing) given in the video descriptions below.\n",
        "\n",
        "If you are ready, let's get started!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1_ClZgzs6Qc"
      },
      "source": [
        "##You can watch this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "rGfAVTa8tA-_",
        "outputId": "1a50f716-73d7-4e31-95a2-f5ee9801eff8"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/JzoIHdkFcQU\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/JzoIHdkFcQU\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy4cJ5Asqv0b"
      },
      "source": [
        "https://youtu.be/JzoIHdkFcQU\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBtnKdYEFcVh"
      },
      "source": [
        "# Include Libraries and Auxiliary Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rRo8oNqZ-Rj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3ecd7f-955c-43af-c3af-27c0932148f9"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA26DoVLwSKU"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_history(history):\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Abs Error [1000$]')\n",
        "  plt.plot(history.epoch, np.array(history.history['mae']), \n",
        "           label='Train')\n",
        "  plt.plot(history.epoch, np.array(history.history['val_mae']),\n",
        "           label = 'Val')\n",
        "  plt.legend()\n",
        "  plt.ylim([0,max(history.history['val_mae'])])\n",
        "\n",
        "def plot_prediction(test_labels, test_predictions):\n",
        "  plt.figure()\n",
        "  plt.scatter(test_labels, test_predictions)\n",
        "  plt.xlabel('True Values [1000$]')\n",
        "  plt.ylabel('Predictions [1000$]')\n",
        "  plt.axis('equal')\n",
        "  plt.xlim(plt.xlim())\n",
        "  plt.ylim(plt.ylim())\n",
        "  _ = plt.plot([-100, 100],[-100,100])\n",
        "\n",
        "  plt.figure()\n",
        "  error = test_predictions - test_labels\n",
        "  plt.hist(error, bins = 50)\n",
        "  plt.xlabel(\"Prediction Error [1000$]\")\n",
        "  _ = plt.ylabel(\"Count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdecPqz1F6wx"
      },
      "source": [
        "# Notice\n",
        "***As a base model***, I will use the **TensorFlow official example** for ***MLP model*** and compare its performance with **my Conv1D model**. Thus, we will be able to observe the relative success of **Conv1D model** with respect to **a professional sample model**.\n",
        "\n",
        "You can access the original notebook [\"Predict house prices: regression\" with Multi-layer Perceptron here.](https://colab.research.google.com/github/MarkDaoust/models/blob/add-regression-plots/samples/core/tutorials/keras/basic_regression.ipynb) \n",
        "\n",
        "If you run this notebook,  you would generate mean absolute error values different than the reported ones here due to stochastic nature of ANNs.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9xy7IeeFuKI"
      },
      "source": [
        "# What is regression? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHp3M9ZmrIxj"
      },
      "source": [
        "In a *regression* problem, we aim to predict the output of a continuous value, like a price or a probability. Contrast this with a *classification* problem, where we aim to predict a discrete label (for example, where a picture contains an apple or an orange). \n",
        "\n",
        "This notebook builds two different models to predict the median price of homes in a Boston suburb during the mid-1970s. To do this, I'll provide the models with some data points about the suburb, such as the crime rate and the local property tax rate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_72b0LCNbjx"
      },
      "source": [
        "## The Boston Housing Prices dataset\n",
        "\n",
        "This [dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) is accessible directly in TensorFlow. Download and shuffle the training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9kxxgzvzlyz"
      },
      "source": [
        "boston_housing = tf.keras.datasets.boston_housing\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = boston_housing.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwEKwRJgsgJ6"
      },
      "source": [
        "### Examples and features \n",
        "\n",
        "This dataset has 506 total examples which are split between **404** training examples and **102** test examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujqcgkipr65P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f30405-5baf-4ddf-d367-64dab03672a3"
      },
      "source": [
        "print(\"Training set: {}\".format(train_data.shape))  # 404 examples, 13 features\n",
        "print(\"Testing set:  {}\".format(test_data.shape))   # 102 examples, 13 features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set: (404, 13)\n",
            "Testing set:  (102, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LRPXE3Oz3Nq"
      },
      "source": [
        "The dataset contains 13 different features:\n",
        "\n",
        "1.   Per capita crime rate.\n",
        "2.   The proportion of residential land zoned for lots over 25,000 square feet.\n",
        "3.   The proportion of non-retail business acres per town.\n",
        "4.   Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
        "5.   Nitric oxides concentration (parts per 10 million).\n",
        "6.   The average number of rooms per dwelling.\n",
        "7.   The proportion of owner-occupied units built before 1940.\n",
        "8.   Weighted distances to five Boston employment centers.\n",
        "9.   Index of accessibility to radial highways.\n",
        "10.  Full-value property-tax rate per $10,000.\n",
        "11.  Pupil-teacher ratio by town.\n",
        "12.  1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.\n",
        "13.  Percentage lower status of the population.\n",
        "\n",
        "Each one of these input data features is stored using a different scale. Some features are represented by a proportion between 0 and 1, other features are ranges between 1 and 12, some are ranges between 0 and 100, and so on. This is often the case with real-world data, and understanding how to explore and clean such data is an important skill to develop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tYsm8Gs03J4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095bb87e-72cd-4fec-fbe3-2e1167878d4c"
      },
      "source": [
        "print(train_data[0])  # Display sample features, notice the different scales"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1.23247   0.        8.14      0.        0.538     6.142    91.7\n",
            "   3.9769    4.      307.       21.      396.9      18.72   ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7muNf-d1-ne"
      },
      "source": [
        "Use the [pandas](https://pandas.pydata.org) library to display the first few rows of the dataset in a nicely formatted table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYVyGhdyCpIM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "bf5242d0-e5f5-42fd-fd90-75b299a232fd"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
        "                'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "\n",
        "df = pd.DataFrame(train_data, columns=column_names)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.23247</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.142</td>\n",
              "      <td>91.7</td>\n",
              "      <td>3.9769</td>\n",
              "      <td>4.0</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>18.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02177</td>\n",
              "      <td>82.5</td>\n",
              "      <td>2.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.415</td>\n",
              "      <td>7.610</td>\n",
              "      <td>15.7</td>\n",
              "      <td>6.2700</td>\n",
              "      <td>2.0</td>\n",
              "      <td>348.0</td>\n",
              "      <td>14.7</td>\n",
              "      <td>395.38</td>\n",
              "      <td>3.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.89822</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.631</td>\n",
              "      <td>4.970</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.3325</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>375.52</td>\n",
              "      <td>3.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.515</td>\n",
              "      <td>6.037</td>\n",
              "      <td>34.5</td>\n",
              "      <td>5.9853</td>\n",
              "      <td>5.0</td>\n",
              "      <td>224.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>8.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.69311</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.713</td>\n",
              "      <td>6.376</td>\n",
              "      <td>88.4</td>\n",
              "      <td>2.5671</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>391.43</td>\n",
              "      <td>14.65</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...   RAD    TAX  PTRATIO       B  LSTAT\n",
              "0  1.23247   0.0   8.14   0.0  0.538  ...   4.0  307.0     21.0  396.90  18.72\n",
              "1  0.02177  82.5   2.03   0.0  0.415  ...   2.0  348.0     14.7  395.38   3.11\n",
              "2  4.89822   0.0  18.10   0.0  0.631  ...  24.0  666.0     20.2  375.52   3.26\n",
              "3  0.03961   0.0   5.19   0.0  0.515  ...   5.0  224.0     20.2  396.90   8.01\n",
              "4  3.69311   0.0  18.10   0.0  0.713  ...  24.0  666.0     20.2  391.43  14.65\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb9S7Mia2lpf"
      },
      "source": [
        "### Labels\n",
        "\n",
        "The labels are the house prices in thousands of dollars. (You may notice the mid-1970s prices.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8NwI2ND2t4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee267161-8b93-4a48-c5be-55791409f08e"
      },
      "source": [
        "print(train_labels[0:10])  # Display first 10 entries"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[15.2 42.3 50.  21.1 17.7 18.5 11.3 15.6 15.6 14.4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRklxK5s388r"
      },
      "source": [
        "## Normalize features\n",
        "\n",
        "It's recommended to normalize features that use different scales and ranges. For each feature, subtract the mean of the feature and divide by the standard deviation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze5WQP8R1TYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a38253-1aa0-4b2e-bac7-617e5c6c6546"
      },
      "source": [
        "# Test data is *not* used when calculating the mean and std.\n",
        "mean = train_data.mean(axis=0)\n",
        "std = train_data.std(axis=0)\n",
        "train_data = (train_data - mean) / std\n",
        "test_data = (test_data - mean) / std\n",
        "\n",
        "print(train_data[0])  # First training sample, normalized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
            "  0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
            "  0.8252202 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuiClDk45eS4"
      },
      "source": [
        "Although the model *might* converge without feature normalization, it makes training more difficult, and it makes the resulting model more dependant on the choice of units used in the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmjdzxKzEu1-"
      },
      "source": [
        "## Create the MLP model\n",
        "\n",
        "Let's build an MLP model. Here, we'll use a `Sequential` model with two densely connected hidden layers, and an output layer that returns a single, continuous value. The model building steps are wrapped in a function, `build_model`, since we'll create a second model, later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c26juK7ZG8j-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "732c1e6f-eae7-4ce0-d414-4320ff11f135"
      },
      "source": [
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "    keras.layers.Input(shape=(train_data.shape[1],)),                  \n",
        "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(1)\n",
        "  ], name=\"MLP_model\")\n",
        "\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  model.compile(loss='mse',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['mae'])\n",
        "  return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"MLP_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_18 (Dense)             (None, 64)                896       \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,121\n",
            "Trainable params: 5,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-qWCsh6DlyH"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "The model is trained for 500 epochs, and record the training and validation accuracy in the `history` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD7qHCmNIOY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d34ce5c0-e68a-432d-a3d3-22ac906f2448"
      },
      "source": [
        "EPOCHS = 500\n",
        "# Store training stats\n",
        "history = model.fit(train_data, train_labels, epochs=EPOCHS,\n",
        "                    validation_split=0.2, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 543.1906 - mae: 21.3777 - val_loss: 566.2466 - val_mae: 21.9570\n",
            "Epoch 2/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 467.3811 - mae: 19.5663 - val_loss: 484.0857 - val_mae: 20.0287\n",
            "Epoch 3/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 386.2641 - mae: 17.4566 - val_loss: 391.4663 - val_mae: 17.6781\n",
            "Epoch 4/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 299.5808 - mae: 14.9836 - val_loss: 300.2606 - val_mae: 15.0185\n",
            "Epoch 5/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 217.6953 - mae: 12.3381 - val_loss: 214.4957 - val_mae: 12.0737\n",
            "Epoch 6/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 147.3429 - mae: 9.6540 - val_loss: 148.6694 - val_mae: 9.3483\n",
            "Epoch 7/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 97.5165 - mae: 7.4497 - val_loss: 99.5994 - val_mae: 7.4300\n",
            "Epoch 8/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 67.5433 - mae: 6.1088 - val_loss: 79.3926 - val_mae: 6.6994\n",
            "Epoch 9/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 55.3049 - mae: 5.4668 - val_loss: 65.8572 - val_mae: 6.0942\n",
            "Epoch 10/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 46.3245 - mae: 5.0191 - val_loss: 52.6883 - val_mae: 5.5123\n",
            "Epoch 11/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 39.3703 - mae: 4.5684 - val_loss: 44.5246 - val_mae: 5.1139\n",
            "Epoch 12/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 33.8339 - mae: 4.2322 - val_loss: 39.0401 - val_mae: 4.7022\n",
            "Epoch 13/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 29.5287 - mae: 3.8923 - val_loss: 34.5773 - val_mae: 4.6537\n",
            "Epoch 14/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 26.7774 - mae: 3.7650 - val_loss: 30.2465 - val_mae: 4.2587\n",
            "Epoch 15/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.9298 - mae: 3.4831 - val_loss: 26.9676 - val_mae: 4.1372\n",
            "Epoch 16/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 21.7230 - mae: 3.3206 - val_loss: 25.7264 - val_mae: 4.0082\n",
            "Epoch 17/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 20.3496 - mae: 3.1761 - val_loss: 23.2766 - val_mae: 3.7565\n",
            "Epoch 18/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 19.2674 - mae: 3.0460 - val_loss: 22.1464 - val_mae: 3.6798\n",
            "Epoch 19/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 17.8722 - mae: 2.9549 - val_loss: 21.7296 - val_mae: 3.6917\n",
            "Epoch 20/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 17.1390 - mae: 2.8945 - val_loss: 20.1918 - val_mae: 3.4987\n",
            "Epoch 21/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 16.3214 - mae: 2.8179 - val_loss: 19.1374 - val_mae: 3.4196\n",
            "Epoch 22/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 15.5844 - mae: 2.7221 - val_loss: 18.9985 - val_mae: 3.4641\n",
            "Epoch 23/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14.9073 - mae: 2.7387 - val_loss: 17.2119 - val_mae: 3.2267\n",
            "Epoch 24/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14.0982 - mae: 2.6351 - val_loss: 17.1554 - val_mae: 3.1448\n",
            "Epoch 25/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 13.9018 - mae: 2.6032 - val_loss: 16.3826 - val_mae: 3.1359\n",
            "Epoch 26/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 13.2320 - mae: 2.4943 - val_loss: 15.9559 - val_mae: 3.0473\n",
            "Epoch 27/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12.5841 - mae: 2.5495 - val_loss: 15.2013 - val_mae: 3.0074\n",
            "Epoch 28/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12.3250 - mae: 2.4718 - val_loss: 14.8798 - val_mae: 2.9078\n",
            "Epoch 29/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12.1297 - mae: 2.4128 - val_loss: 14.7435 - val_mae: 2.9398\n",
            "Epoch 30/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 11.5987 - mae: 2.3569 - val_loss: 14.8376 - val_mae: 2.9446\n",
            "Epoch 31/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 11.3566 - mae: 2.3715 - val_loss: 14.1398 - val_mae: 2.9345\n",
            "Epoch 32/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 11.0479 - mae: 2.3306 - val_loss: 13.5415 - val_mae: 2.7888\n",
            "Epoch 33/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 10.8897 - mae: 2.3173 - val_loss: 13.5504 - val_mae: 2.8242\n",
            "Epoch 34/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 10.5388 - mae: 2.2399 - val_loss: 13.4986 - val_mae: 2.7448\n",
            "Epoch 35/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 10.2720 - mae: 2.2667 - val_loss: 12.9275 - val_mae: 2.7885\n",
            "Epoch 36/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 10.1870 - mae: 2.1690 - val_loss: 13.2894 - val_mae: 2.6643\n",
            "Epoch 37/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9698 - mae: 2.2038 - val_loss: 14.1644 - val_mae: 2.7668\n",
            "Epoch 38/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9339 - mae: 2.2677 - val_loss: 12.5428 - val_mae: 2.7471\n",
            "Epoch 39/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5525 - mae: 2.1280 - val_loss: 12.9358 - val_mae: 2.7894\n",
            "Epoch 40/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6609 - mae: 2.1274 - val_loss: 13.0982 - val_mae: 2.7067\n",
            "Epoch 41/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4215 - mae: 2.1706 - val_loss: 12.2766 - val_mae: 2.6818\n",
            "Epoch 42/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.0854 - mae: 2.0943 - val_loss: 13.5079 - val_mae: 2.8018\n",
            "Epoch 43/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.0515 - mae: 2.1254 - val_loss: 12.0583 - val_mae: 2.6450\n",
            "Epoch 44/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.1209 - mae: 2.1152 - val_loss: 12.1914 - val_mae: 2.7226\n",
            "Epoch 45/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.8816 - mae: 2.0263 - val_loss: 12.3009 - val_mae: 2.7435\n",
            "Epoch 46/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.6570 - mae: 1.9791 - val_loss: 12.4047 - val_mae: 2.5842\n",
            "Epoch 47/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.6188 - mae: 2.0326 - val_loss: 13.6232 - val_mae: 2.7426\n",
            "Epoch 48/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.6621 - mae: 2.0760 - val_loss: 12.3438 - val_mae: 2.6716\n",
            "Epoch 49/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.4466 - mae: 2.0263 - val_loss: 12.3999 - val_mae: 2.6421\n",
            "Epoch 50/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.2789 - mae: 1.9966 - val_loss: 14.0140 - val_mae: 2.9184\n",
            "Epoch 51/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.5423 - mae: 1.9992 - val_loss: 12.4729 - val_mae: 2.6921\n",
            "Epoch 52/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1471 - mae: 2.0177 - val_loss: 12.7165 - val_mae: 2.6074\n",
            "Epoch 53/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1023 - mae: 2.0043 - val_loss: 12.6796 - val_mae: 2.6969\n",
            "Epoch 54/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.9653 - mae: 1.9556 - val_loss: 13.3002 - val_mae: 2.6425\n",
            "Epoch 55/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1739 - mae: 2.0225 - val_loss: 13.7122 - val_mae: 2.6707\n",
            "Epoch 56/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1850 - mae: 2.0177 - val_loss: 13.5675 - val_mae: 2.7677\n",
            "Epoch 57/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.9202 - mae: 1.9783 - val_loss: 13.1865 - val_mae: 2.6664\n",
            "Epoch 58/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.9113 - mae: 1.9520 - val_loss: 12.2741 - val_mae: 2.6053\n",
            "Epoch 59/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.8666 - mae: 1.9771 - val_loss: 13.3074 - val_mae: 2.7562\n",
            "Epoch 60/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.7484 - mae: 1.9736 - val_loss: 12.7544 - val_mae: 2.6306\n",
            "Epoch 61/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.5634 - mae: 1.9312 - val_loss: 12.4453 - val_mae: 2.7073\n",
            "Epoch 62/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.7566 - mae: 1.9477 - val_loss: 12.2380 - val_mae: 2.5784\n",
            "Epoch 63/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.4176 - mae: 1.9015 - val_loss: 12.4989 - val_mae: 2.5747\n",
            "Epoch 64/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.3337 - mae: 1.8856 - val_loss: 15.2055 - val_mae: 2.6833\n",
            "Epoch 65/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.6487 - mae: 1.9256 - val_loss: 12.7271 - val_mae: 2.6332\n",
            "Epoch 66/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.3214 - mae: 1.8810 - val_loss: 13.0599 - val_mae: 2.5954\n",
            "Epoch 67/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.1964 - mae: 1.8649 - val_loss: 13.9084 - val_mae: 2.7472\n",
            "Epoch 68/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.0765 - mae: 1.8567 - val_loss: 13.2558 - val_mae: 2.5802\n",
            "Epoch 69/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.1118 - mae: 1.8738 - val_loss: 13.6023 - val_mae: 2.6319\n",
            "Epoch 70/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.2822 - mae: 1.8871 - val_loss: 12.9038 - val_mae: 2.5525\n",
            "Epoch 71/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.0762 - mae: 1.8658 - val_loss: 13.3333 - val_mae: 2.7702\n",
            "Epoch 72/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.1266 - mae: 1.8689 - val_loss: 13.0009 - val_mae: 2.5424\n",
            "Epoch 73/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.9409 - mae: 1.8528 - val_loss: 12.7347 - val_mae: 2.5804\n",
            "Epoch 74/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.9911 - mae: 1.8516 - val_loss: 12.7736 - val_mae: 2.6416\n",
            "Epoch 75/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.9303 - mae: 1.8256 - val_loss: 15.1965 - val_mae: 2.7492\n",
            "Epoch 76/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.0518 - mae: 1.8833 - val_loss: 14.5818 - val_mae: 2.7503\n",
            "Epoch 77/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.8868 - mae: 1.8273 - val_loss: 12.9802 - val_mae: 2.6574\n",
            "Epoch 78/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.8041 - mae: 1.7900 - val_loss: 15.8485 - val_mae: 2.6489\n",
            "Epoch 79/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.8958 - mae: 1.8797 - val_loss: 13.9340 - val_mae: 2.5927\n",
            "Epoch 80/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.6057 - mae: 1.8147 - val_loss: 14.5762 - val_mae: 2.6460\n",
            "Epoch 81/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4765 - mae: 1.8082 - val_loss: 13.3290 - val_mae: 2.7149\n",
            "Epoch 82/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.5893 - mae: 1.7834 - val_loss: 16.1088 - val_mae: 2.7473\n",
            "Epoch 83/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.5632 - mae: 1.8395 - val_loss: 14.0738 - val_mae: 2.7188\n",
            "Epoch 84/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4874 - mae: 1.7816 - val_loss: 16.2755 - val_mae: 2.7469\n",
            "Epoch 85/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.8307 - mae: 1.8276 - val_loss: 13.5391 - val_mae: 2.5996\n",
            "Epoch 86/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3614 - mae: 1.7541 - val_loss: 13.7641 - val_mae: 2.7597\n",
            "Epoch 87/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.5955 - mae: 1.7625 - val_loss: 14.3588 - val_mae: 2.5500\n",
            "Epoch 88/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3829 - mae: 1.7821 - val_loss: 14.2893 - val_mae: 2.7864\n",
            "Epoch 89/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4125 - mae: 1.7956 - val_loss: 13.6314 - val_mae: 2.6248\n",
            "Epoch 90/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3145 - mae: 1.7495 - val_loss: 14.0422 - val_mae: 2.6348\n",
            "Epoch 91/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.2737 - mae: 1.7644 - val_loss: 17.3599 - val_mae: 2.7814\n",
            "Epoch 92/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3020 - mae: 1.8327 - val_loss: 13.8667 - val_mae: 2.5920\n",
            "Epoch 93/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1324 - mae: 1.7318 - val_loss: 13.4265 - val_mae: 2.6256\n",
            "Epoch 94/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1451 - mae: 1.7097 - val_loss: 13.8812 - val_mae: 2.6155\n",
            "Epoch 95/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.0537 - mae: 1.7169 - val_loss: 14.7216 - val_mae: 2.6147\n",
            "Epoch 96/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.2518 - mae: 1.7476 - val_loss: 15.9236 - val_mae: 2.7077\n",
            "Epoch 97/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9424 - mae: 1.7308 - val_loss: 14.5788 - val_mae: 2.6082\n",
            "Epoch 98/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.0372 - mae: 1.7437 - val_loss: 15.0020 - val_mae: 2.8351\n",
            "Epoch 99/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.0527 - mae: 1.7097 - val_loss: 14.3791 - val_mae: 2.6210\n",
            "Epoch 100/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.0812 - mae: 1.7458 - val_loss: 14.9749 - val_mae: 2.7525\n",
            "Epoch 101/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9581 - mae: 1.6939 - val_loss: 14.4441 - val_mae: 2.6055\n",
            "Epoch 102/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.8579 - mae: 1.6947 - val_loss: 14.2226 - val_mae: 2.6492\n",
            "Epoch 103/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.8579 - mae: 1.6778 - val_loss: 14.8839 - val_mae: 2.6125\n",
            "Epoch 104/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.7147 - mae: 1.6497 - val_loss: 14.3179 - val_mae: 2.6327\n",
            "Epoch 105/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9598 - mae: 1.7065 - val_loss: 14.8245 - val_mae: 2.7490\n",
            "Epoch 106/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.7005 - mae: 1.6764 - val_loss: 13.9684 - val_mae: 2.6255\n",
            "Epoch 107/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.6106 - mae: 1.6179 - val_loss: 14.3891 - val_mae: 2.6065\n",
            "Epoch 108/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.6690 - mae: 1.6828 - val_loss: 15.6604 - val_mae: 2.6868\n",
            "Epoch 109/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.7211 - mae: 1.6713 - val_loss: 15.0761 - val_mae: 2.6717\n",
            "Epoch 110/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.6495 - mae: 1.6628 - val_loss: 17.5127 - val_mae: 2.7071\n",
            "Epoch 111/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.7724 - mae: 1.7284 - val_loss: 14.9401 - val_mae: 2.6583\n",
            "Epoch 112/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.4867 - mae: 1.6545 - val_loss: 14.3414 - val_mae: 2.6937\n",
            "Epoch 113/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.5587 - mae: 1.6470 - val_loss: 14.1618 - val_mae: 2.6258\n",
            "Epoch 114/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.5865 - mae: 1.6174 - val_loss: 15.2486 - val_mae: 2.7050\n",
            "Epoch 115/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.4302 - mae: 1.6317 - val_loss: 17.2845 - val_mae: 2.6712\n",
            "Epoch 116/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.5830 - mae: 1.6990 - val_loss: 15.5644 - val_mae: 2.6043\n",
            "Epoch 117/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.3082 - mae: 1.6309 - val_loss: 15.3653 - val_mae: 2.6444\n",
            "Epoch 118/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.2229 - mae: 1.6158 - val_loss: 14.4065 - val_mae: 2.5587\n",
            "Epoch 119/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.2594 - mae: 1.6170 - val_loss: 15.2427 - val_mae: 2.6173\n",
            "Epoch 120/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.2027 - mae: 1.6000 - val_loss: 14.8537 - val_mae: 2.6985\n",
            "Epoch 121/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.4589 - mae: 1.6322 - val_loss: 14.7071 - val_mae: 2.5477\n",
            "Epoch 122/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.3042 - mae: 1.5819 - val_loss: 14.0946 - val_mae: 2.6221\n",
            "Epoch 123/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.1878 - mae: 1.5985 - val_loss: 16.9224 - val_mae: 2.7229\n",
            "Epoch 124/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.2739 - mae: 1.6200 - val_loss: 15.2697 - val_mae: 2.6847\n",
            "Epoch 125/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.2253 - mae: 1.5723 - val_loss: 16.6908 - val_mae: 2.6221\n",
            "Epoch 126/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.2355 - mae: 1.5996 - val_loss: 15.3312 - val_mae: 2.5364\n",
            "Epoch 127/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.1928 - mae: 1.6087 - val_loss: 16.6223 - val_mae: 2.7266\n",
            "Epoch 128/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.1862 - mae: 1.6017 - val_loss: 14.9657 - val_mae: 2.6196\n",
            "Epoch 129/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 4.9219 - mae: 1.5372 - val_loss: 15.1331 - val_mae: 2.5344\n",
            "Epoch 130/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.0186 - mae: 1.5567 - val_loss: 16.7501 - val_mae: 2.8454\n",
            "Epoch 131/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.0126 - mae: 1.6076 - val_loss: 14.0461 - val_mae: 2.5817\n",
            "Epoch 132/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.0455 - mae: 1.5597 - val_loss: 14.1944 - val_mae: 2.6597\n",
            "Epoch 133/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9420 - mae: 1.5533 - val_loss: 14.1155 - val_mae: 2.7134\n",
            "Epoch 134/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.1440 - mae: 1.5486 - val_loss: 15.0464 - val_mae: 2.6832\n",
            "Epoch 135/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9309 - mae: 1.5326 - val_loss: 14.7327 - val_mae: 2.5624\n",
            "Epoch 136/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9137 - mae: 1.5443 - val_loss: 14.5699 - val_mae: 2.5924\n",
            "Epoch 137/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9208 - mae: 1.5056 - val_loss: 14.2886 - val_mae: 2.5478\n",
            "Epoch 138/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9483 - mae: 1.5448 - val_loss: 15.4471 - val_mae: 2.5942\n",
            "Epoch 139/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9658 - mae: 1.5769 - val_loss: 15.1386 - val_mae: 2.6099\n",
            "Epoch 140/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.8743 - mae: 1.5296 - val_loss: 15.4887 - val_mae: 2.5943\n",
            "Epoch 141/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6780 - mae: 1.4731 - val_loss: 17.1608 - val_mae: 2.6460\n",
            "Epoch 142/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7459 - mae: 1.5033 - val_loss: 16.6886 - val_mae: 2.6155\n",
            "Epoch 143/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9711 - mae: 1.5475 - val_loss: 16.7047 - val_mae: 2.7070\n",
            "Epoch 144/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7073 - mae: 1.5158 - val_loss: 18.0058 - val_mae: 2.6985\n",
            "Epoch 145/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6836 - mae: 1.5300 - val_loss: 15.0125 - val_mae: 2.5509\n",
            "Epoch 146/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.4701 - mae: 1.4691 - val_loss: 14.9598 - val_mae: 2.5321\n",
            "Epoch 147/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.5835 - mae: 1.5068 - val_loss: 16.8203 - val_mae: 2.6477\n",
            "Epoch 148/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5990 - mae: 1.5060 - val_loss: 15.7899 - val_mae: 2.5590\n",
            "Epoch 149/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6589 - mae: 1.5071 - val_loss: 17.6253 - val_mae: 2.7678\n",
            "Epoch 150/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.4600 - mae: 1.4985 - val_loss: 15.9579 - val_mae: 2.6664\n",
            "Epoch 151/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5849 - mae: 1.4845 - val_loss: 14.8122 - val_mae: 2.5391\n",
            "Epoch 152/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.4873 - mae: 1.4730 - val_loss: 14.9398 - val_mae: 2.5212\n",
            "Epoch 153/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5726 - mae: 1.4788 - val_loss: 16.1408 - val_mae: 2.6999\n",
            "Epoch 154/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6951 - mae: 1.5013 - val_loss: 16.1267 - val_mae: 2.6538\n",
            "Epoch 155/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5858 - mae: 1.4956 - val_loss: 15.0709 - val_mae: 2.6386\n",
            "Epoch 156/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5188 - mae: 1.4636 - val_loss: 16.4064 - val_mae: 2.7130\n",
            "Epoch 157/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5338 - mae: 1.4937 - val_loss: 14.8399 - val_mae: 2.6722\n",
            "Epoch 158/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6431 - mae: 1.4763 - val_loss: 14.7923 - val_mae: 2.5394\n",
            "Epoch 159/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3090 - mae: 1.4214 - val_loss: 16.4976 - val_mae: 2.5990\n",
            "Epoch 160/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.4218 - mae: 1.4563 - val_loss: 15.0857 - val_mae: 2.6627\n",
            "Epoch 161/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3173 - mae: 1.4301 - val_loss: 16.8634 - val_mae: 2.6285\n",
            "Epoch 162/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.3905 - mae: 1.4634 - val_loss: 14.8216 - val_mae: 2.5779\n",
            "Epoch 163/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3950 - mae: 1.4477 - val_loss: 15.7471 - val_mae: 2.5197\n",
            "Epoch 164/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3447 - mae: 1.4283 - val_loss: 15.0181 - val_mae: 2.5589\n",
            "Epoch 165/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2494 - mae: 1.4406 - val_loss: 16.1227 - val_mae: 2.6556\n",
            "Epoch 166/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2868 - mae: 1.4122 - val_loss: 15.5157 - val_mae: 2.6311\n",
            "Epoch 167/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2095 - mae: 1.4255 - val_loss: 15.3986 - val_mae: 2.7717\n",
            "Epoch 168/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3147 - mae: 1.4391 - val_loss: 17.4089 - val_mae: 2.8903\n",
            "Epoch 169/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3365 - mae: 1.4440 - val_loss: 15.9104 - val_mae: 2.6518\n",
            "Epoch 170/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1295 - mae: 1.4164 - val_loss: 16.2436 - val_mae: 2.5940\n",
            "Epoch 171/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1337 - mae: 1.4497 - val_loss: 16.4937 - val_mae: 2.7024\n",
            "Epoch 172/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1521 - mae: 1.4098 - val_loss: 20.7653 - val_mae: 2.9413\n",
            "Epoch 173/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5228 - mae: 1.5130 - val_loss: 16.0982 - val_mae: 2.6300\n",
            "Epoch 174/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0780 - mae: 1.3910 - val_loss: 16.8260 - val_mae: 2.5864\n",
            "Epoch 175/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1462 - mae: 1.3880 - val_loss: 17.8864 - val_mae: 2.6747\n",
            "Epoch 176/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1350 - mae: 1.4446 - val_loss: 15.5552 - val_mae: 2.5379\n",
            "Epoch 177/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9495 - mae: 1.3568 - val_loss: 17.4828 - val_mae: 2.7212\n",
            "Epoch 178/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.1556 - mae: 1.4297 - val_loss: 17.2998 - val_mae: 2.7264\n",
            "Epoch 179/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0634 - mae: 1.3768 - val_loss: 15.0877 - val_mae: 2.5369\n",
            "Epoch 180/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8350 - mae: 1.3555 - val_loss: 17.2353 - val_mae: 2.6468\n",
            "Epoch 181/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1277 - mae: 1.3764 - val_loss: 15.6640 - val_mae: 2.5957\n",
            "Epoch 182/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9727 - mae: 1.3562 - val_loss: 17.5520 - val_mae: 2.6884\n",
            "Epoch 183/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1607 - mae: 1.4269 - val_loss: 16.4801 - val_mae: 2.7208\n",
            "Epoch 184/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9671 - mae: 1.3719 - val_loss: 15.4370 - val_mae: 2.5213\n",
            "Epoch 185/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7909 - mae: 1.3306 - val_loss: 14.8005 - val_mae: 2.5974\n",
            "Epoch 186/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.9381 - mae: 1.3572 - val_loss: 15.2930 - val_mae: 2.6195\n",
            "Epoch 187/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9129 - mae: 1.3507 - val_loss: 15.1057 - val_mae: 2.5896\n",
            "Epoch 188/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9423 - mae: 1.3579 - val_loss: 16.5075 - val_mae: 2.6729\n",
            "Epoch 189/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.7487 - mae: 1.3334 - val_loss: 15.2801 - val_mae: 2.7994\n",
            "Epoch 190/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1948 - mae: 1.3978 - val_loss: 15.6709 - val_mae: 2.5318\n",
            "Epoch 191/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7741 - mae: 1.3218 - val_loss: 18.4826 - val_mae: 2.8409\n",
            "Epoch 192/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9289 - mae: 1.3735 - val_loss: 15.1644 - val_mae: 2.5259\n",
            "Epoch 193/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7898 - mae: 1.3647 - val_loss: 16.4552 - val_mae: 2.6088\n",
            "Epoch 194/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8482 - mae: 1.3736 - val_loss: 15.3149 - val_mae: 2.5742\n",
            "Epoch 195/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9344 - mae: 1.3448 - val_loss: 16.3098 - val_mae: 2.5230\n",
            "Epoch 196/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.8468 - mae: 1.3493 - val_loss: 16.9511 - val_mae: 2.7832\n",
            "Epoch 197/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7483 - mae: 1.3313 - val_loss: 18.1405 - val_mae: 2.6942\n",
            "Epoch 198/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8880 - mae: 1.3906 - val_loss: 16.2266 - val_mae: 2.6327\n",
            "Epoch 199/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6808 - mae: 1.3505 - val_loss: 16.2612 - val_mae: 2.5955\n",
            "Epoch 200/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6291 - mae: 1.3654 - val_loss: 18.3078 - val_mae: 2.7007\n",
            "Epoch 201/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.8160 - mae: 1.3702 - val_loss: 16.5905 - val_mae: 2.6323\n",
            "Epoch 202/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.6264 - mae: 1.3271 - val_loss: 15.5770 - val_mae: 2.5453\n",
            "Epoch 203/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5867 - mae: 1.3427 - val_loss: 15.6106 - val_mae: 2.5050\n",
            "Epoch 204/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6248 - mae: 1.3545 - val_loss: 15.2119 - val_mae: 2.5685\n",
            "Epoch 205/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6792 - mae: 1.3042 - val_loss: 17.3492 - val_mae: 2.6306\n",
            "Epoch 206/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.7771 - mae: 1.3335 - val_loss: 16.9996 - val_mae: 2.6932\n",
            "Epoch 207/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.5812 - mae: 1.3082 - val_loss: 15.6728 - val_mae: 2.4914\n",
            "Epoch 208/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5824 - mae: 1.3179 - val_loss: 16.2953 - val_mae: 2.5250\n",
            "Epoch 209/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6240 - mae: 1.3242 - val_loss: 14.8134 - val_mae: 2.7120\n",
            "Epoch 210/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.5779 - mae: 1.3181 - val_loss: 17.4161 - val_mae: 2.6048\n",
            "Epoch 211/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6909 - mae: 1.3371 - val_loss: 15.6477 - val_mae: 2.6078\n",
            "Epoch 212/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.4495 - mae: 1.2799 - val_loss: 15.9068 - val_mae: 2.5266\n",
            "Epoch 213/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5280 - mae: 1.3078 - val_loss: 15.2587 - val_mae: 2.6796\n",
            "Epoch 214/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.6868 - mae: 1.2882 - val_loss: 15.1913 - val_mae: 2.7571\n",
            "Epoch 215/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5399 - mae: 1.3202 - val_loss: 17.1351 - val_mae: 2.6699\n",
            "Epoch 216/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6216 - mae: 1.3025 - val_loss: 15.0939 - val_mae: 2.5891\n",
            "Epoch 217/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.4201 - mae: 1.3053 - val_loss: 15.9312 - val_mae: 2.5671\n",
            "Epoch 218/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4381 - mae: 1.2611 - val_loss: 18.8414 - val_mae: 2.6978\n",
            "Epoch 219/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4529 - mae: 1.2845 - val_loss: 15.3979 - val_mae: 2.5203\n",
            "Epoch 220/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6038 - mae: 1.2797 - val_loss: 14.7306 - val_mae: 2.5440\n",
            "Epoch 221/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4690 - mae: 1.2781 - val_loss: 15.3433 - val_mae: 2.5512\n",
            "Epoch 222/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2560 - mae: 1.2493 - val_loss: 15.3432 - val_mae: 2.5726\n",
            "Epoch 223/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.3390 - mae: 1.2722 - val_loss: 15.6872 - val_mae: 2.8871\n",
            "Epoch 224/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.6535 - mae: 1.3319 - val_loss: 15.6903 - val_mae: 2.5537\n",
            "Epoch 225/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2569 - mae: 1.2474 - val_loss: 16.1625 - val_mae: 2.5303\n",
            "Epoch 226/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3600 - mae: 1.2992 - val_loss: 15.3385 - val_mae: 2.4771\n",
            "Epoch 227/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3301 - mae: 1.2656 - val_loss: 15.2367 - val_mae: 2.5304\n",
            "Epoch 228/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.2900 - mae: 1.2678 - val_loss: 18.0775 - val_mae: 2.6720\n",
            "Epoch 229/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3864 - mae: 1.2643 - val_loss: 15.9697 - val_mae: 2.7659\n",
            "Epoch 230/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3806 - mae: 1.2687 - val_loss: 15.3829 - val_mae: 2.6527\n",
            "Epoch 231/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.3714 - mae: 1.2702 - val_loss: 15.8065 - val_mae: 2.5688\n",
            "Epoch 232/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.2749 - mae: 1.2541 - val_loss: 18.8281 - val_mae: 2.7019\n",
            "Epoch 233/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5302 - mae: 1.3274 - val_loss: 17.3018 - val_mae: 2.6749\n",
            "Epoch 234/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.3316 - mae: 1.2449 - val_loss: 16.2539 - val_mae: 2.5390\n",
            "Epoch 235/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.2289 - mae: 1.2349 - val_loss: 15.5640 - val_mae: 2.5654\n",
            "Epoch 236/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2286 - mae: 1.2375 - val_loss: 15.7937 - val_mae: 2.5055\n",
            "Epoch 237/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3076 - mae: 1.2667 - val_loss: 16.5541 - val_mae: 2.6668\n",
            "Epoch 238/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.1498 - mae: 1.2281 - val_loss: 17.8875 - val_mae: 2.6574\n",
            "Epoch 239/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.1364 - mae: 1.2223 - val_loss: 14.8768 - val_mae: 2.5955\n",
            "Epoch 240/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2158 - mae: 1.2336 - val_loss: 15.2059 - val_mae: 2.4863\n",
            "Epoch 241/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1115 - mae: 1.2308 - val_loss: 15.7466 - val_mae: 2.5282\n",
            "Epoch 242/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.9938 - mae: 1.1904 - val_loss: 14.9728 - val_mae: 2.7187\n",
            "Epoch 243/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2991 - mae: 1.2931 - val_loss: 16.3036 - val_mae: 2.5964\n",
            "Epoch 244/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0981 - mae: 1.2227 - val_loss: 14.5213 - val_mae: 2.4750\n",
            "Epoch 245/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.1887 - mae: 1.2305 - val_loss: 16.1707 - val_mae: 2.7174\n",
            "Epoch 246/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3093 - mae: 1.2677 - val_loss: 16.6883 - val_mae: 2.6255\n",
            "Epoch 247/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1088 - mae: 1.2155 - val_loss: 14.8727 - val_mae: 2.5778\n",
            "Epoch 248/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.9803 - mae: 1.1987 - val_loss: 15.3414 - val_mae: 2.5234\n",
            "Epoch 249/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9308 - mae: 1.1993 - val_loss: 15.1489 - val_mae: 2.6122\n",
            "Epoch 250/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.1536 - mae: 1.2424 - val_loss: 17.3655 - val_mae: 2.5887\n",
            "Epoch 251/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.0772 - mae: 1.2239 - val_loss: 15.2967 - val_mae: 2.4877\n",
            "Epoch 252/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0986 - mae: 1.2352 - val_loss: 16.2473 - val_mae: 2.5562\n",
            "Epoch 253/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.0792 - mae: 1.2185 - val_loss: 16.4905 - val_mae: 2.5587\n",
            "Epoch 254/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9963 - mae: 1.1610 - val_loss: 18.7093 - val_mae: 2.6827\n",
            "Epoch 255/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.2697 - mae: 1.2760 - val_loss: 16.4362 - val_mae: 2.6356\n",
            "Epoch 256/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.9091 - mae: 1.1968 - val_loss: 16.8384 - val_mae: 2.7251\n",
            "Epoch 257/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0327 - mae: 1.2037 - val_loss: 16.7236 - val_mae: 2.5974\n",
            "Epoch 258/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1072 - mae: 1.2206 - val_loss: 18.5920 - val_mae: 2.7144\n",
            "Epoch 259/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8663 - mae: 1.1905 - val_loss: 19.4395 - val_mae: 2.7458\n",
            "Epoch 260/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3084 - mae: 1.2406 - val_loss: 15.7818 - val_mae: 2.5498\n",
            "Epoch 261/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.0303 - mae: 1.2228 - val_loss: 15.2315 - val_mae: 2.5393\n",
            "Epoch 262/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0191 - mae: 1.1624 - val_loss: 15.6499 - val_mae: 2.6896\n",
            "Epoch 263/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1880 - mae: 1.2145 - val_loss: 16.2349 - val_mae: 2.5280\n",
            "Epoch 264/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.9088 - mae: 1.1800 - val_loss: 15.1105 - val_mae: 2.6419\n",
            "Epoch 265/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.9924 - mae: 1.1817 - val_loss: 17.6741 - val_mae: 2.6359\n",
            "Epoch 266/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0565 - mae: 1.2480 - val_loss: 15.9526 - val_mae: 2.5344\n",
            "Epoch 267/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8684 - mae: 1.1700 - val_loss: 17.6277 - val_mae: 2.6157\n",
            "Epoch 268/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.0786 - mae: 1.2183 - val_loss: 16.7497 - val_mae: 2.6286\n",
            "Epoch 269/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9263 - mae: 1.1695 - val_loss: 15.9215 - val_mae: 2.6049\n",
            "Epoch 270/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.9147 - mae: 1.1878 - val_loss: 17.4135 - val_mae: 2.6055\n",
            "Epoch 271/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9410 - mae: 1.2063 - val_loss: 17.0870 - val_mae: 2.6144\n",
            "Epoch 272/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8095 - mae: 1.1543 - val_loss: 18.3815 - val_mae: 2.6545\n",
            "Epoch 273/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.0486 - mae: 1.2141 - val_loss: 14.5826 - val_mae: 2.6786\n",
            "Epoch 274/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8645 - mae: 1.1767 - val_loss: 15.9171 - val_mae: 2.5245\n",
            "Epoch 275/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8933 - mae: 1.1484 - val_loss: 20.3576 - val_mae: 2.8950\n",
            "Epoch 276/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0879 - mae: 1.2385 - val_loss: 16.8879 - val_mae: 2.6377\n",
            "Epoch 277/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7718 - mae: 1.1253 - val_loss: 18.8087 - val_mae: 2.6795\n",
            "Epoch 278/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.9727 - mae: 1.2114 - val_loss: 15.3671 - val_mae: 2.5755\n",
            "Epoch 279/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7349 - mae: 1.1443 - val_loss: 17.2873 - val_mae: 2.5932\n",
            "Epoch 280/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8009 - mae: 1.1527 - val_loss: 15.4773 - val_mae: 2.5057\n",
            "Epoch 281/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6862 - mae: 1.1366 - val_loss: 15.8761 - val_mae: 2.5238\n",
            "Epoch 282/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8236 - mae: 1.1617 - val_loss: 15.3266 - val_mae: 2.5099\n",
            "Epoch 283/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8686 - mae: 1.1790 - val_loss: 14.5489 - val_mae: 2.5465\n",
            "Epoch 284/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8561 - mae: 1.1582 - val_loss: 16.6313 - val_mae: 2.6288\n",
            "Epoch 285/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8302 - mae: 1.1683 - val_loss: 17.5764 - val_mae: 2.7329\n",
            "Epoch 286/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7032 - mae: 1.1761 - val_loss: 19.1280 - val_mae: 2.7402\n",
            "Epoch 287/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8933 - mae: 1.1763 - val_loss: 14.9121 - val_mae: 2.6068\n",
            "Epoch 288/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8334 - mae: 1.1599 - val_loss: 15.9368 - val_mae: 2.5380\n",
            "Epoch 289/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7624 - mae: 1.1311 - val_loss: 17.3335 - val_mae: 2.8247\n",
            "Epoch 290/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.9078 - mae: 1.2135 - val_loss: 18.5549 - val_mae: 2.6619\n",
            "Epoch 291/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6302 - mae: 1.1199 - val_loss: 16.3058 - val_mae: 2.5979\n",
            "Epoch 292/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6355 - mae: 1.1175 - val_loss: 16.2278 - val_mae: 2.5847\n",
            "Epoch 293/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8864 - mae: 1.1874 - val_loss: 15.7127 - val_mae: 2.6958\n",
            "Epoch 294/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8873 - mae: 1.1375 - val_loss: 15.4357 - val_mae: 2.5739\n",
            "Epoch 295/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.9610 - mae: 1.1869 - val_loss: 17.7286 - val_mae: 2.8084\n",
            "Epoch 296/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.9468 - mae: 1.1947 - val_loss: 17.1298 - val_mae: 2.5692\n",
            "Epoch 297/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6172 - mae: 1.1178 - val_loss: 16.3405 - val_mae: 2.5571\n",
            "Epoch 298/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 2.6870 - mae: 1.1193 - val_loss: 16.7058 - val_mae: 2.5653\n",
            "Epoch 299/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7774 - mae: 1.1564 - val_loss: 15.6578 - val_mae: 2.6723\n",
            "Epoch 300/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7752 - mae: 1.1668 - val_loss: 16.2526 - val_mae: 2.5581\n",
            "Epoch 301/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6241 - mae: 1.1357 - val_loss: 15.2253 - val_mae: 2.5277\n",
            "Epoch 302/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7549 - mae: 1.1460 - val_loss: 15.3568 - val_mae: 2.5771\n",
            "Epoch 303/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6051 - mae: 1.1059 - val_loss: 17.6361 - val_mae: 2.6532\n",
            "Epoch 304/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7867 - mae: 1.1936 - val_loss: 17.6390 - val_mae: 2.6178\n",
            "Epoch 305/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7053 - mae: 1.1575 - val_loss: 15.6555 - val_mae: 2.5084\n",
            "Epoch 306/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5551 - mae: 1.0791 - val_loss: 15.0690 - val_mae: 2.6262\n",
            "Epoch 307/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5814 - mae: 1.1012 - val_loss: 16.4134 - val_mae: 2.5714\n",
            "Epoch 308/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7725 - mae: 1.1401 - val_loss: 16.1309 - val_mae: 2.5760\n",
            "Epoch 309/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6507 - mae: 1.0963 - val_loss: 16.7553 - val_mae: 2.6357\n",
            "Epoch 310/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6081 - mae: 1.1192 - val_loss: 14.9527 - val_mae: 2.5313\n",
            "Epoch 311/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7250 - mae: 1.1436 - val_loss: 16.6885 - val_mae: 2.5837\n",
            "Epoch 312/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4847 - mae: 1.0961 - val_loss: 15.8747 - val_mae: 2.6204\n",
            "Epoch 313/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5279 - mae: 1.1030 - val_loss: 16.8916 - val_mae: 2.6699\n",
            "Epoch 314/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7396 - mae: 1.1488 - val_loss: 15.6216 - val_mae: 2.5920\n",
            "Epoch 315/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5473 - mae: 1.0913 - val_loss: 15.0037 - val_mae: 2.7143\n",
            "Epoch 316/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5709 - mae: 1.1048 - val_loss: 19.1066 - val_mae: 2.8314\n",
            "Epoch 317/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6007 - mae: 1.1361 - val_loss: 15.1831 - val_mae: 2.5414\n",
            "Epoch 318/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4858 - mae: 1.0735 - val_loss: 16.4179 - val_mae: 2.5882\n",
            "Epoch 319/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6452 - mae: 1.1136 - val_loss: 15.8653 - val_mae: 2.5676\n",
            "Epoch 320/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5549 - mae: 1.0829 - val_loss: 15.7600 - val_mae: 2.6734\n",
            "Epoch 321/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4546 - mae: 1.1001 - val_loss: 20.7468 - val_mae: 2.7968\n",
            "Epoch 322/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7782 - mae: 1.1408 - val_loss: 16.1960 - val_mae: 2.5755\n",
            "Epoch 323/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4889 - mae: 1.0890 - val_loss: 15.4202 - val_mae: 2.6018\n",
            "Epoch 324/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4865 - mae: 1.1041 - val_loss: 15.3597 - val_mae: 2.6605\n",
            "Epoch 325/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5683 - mae: 1.1393 - val_loss: 15.1556 - val_mae: 2.5681\n",
            "Epoch 326/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2884 - mae: 1.0333 - val_loss: 16.6759 - val_mae: 2.6144\n",
            "Epoch 327/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4648 - mae: 1.1107 - val_loss: 17.6573 - val_mae: 2.8108\n",
            "Epoch 328/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6622 - mae: 1.1409 - val_loss: 16.6418 - val_mae: 2.7013\n",
            "Epoch 329/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6284 - mae: 1.1189 - val_loss: 16.6514 - val_mae: 2.6526\n",
            "Epoch 330/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2973 - mae: 1.0597 - val_loss: 18.9623 - val_mae: 2.9142\n",
            "Epoch 331/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5544 - mae: 1.0973 - val_loss: 17.1772 - val_mae: 2.7610\n",
            "Epoch 332/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4205 - mae: 1.0878 - val_loss: 15.7976 - val_mae: 2.6688\n",
            "Epoch 333/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4569 - mae: 1.0796 - val_loss: 21.6069 - val_mae: 2.9059\n",
            "Epoch 334/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.0499 - mae: 1.1671 - val_loss: 16.3670 - val_mae: 2.5776\n",
            "Epoch 335/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3759 - mae: 1.0635 - val_loss: 17.4983 - val_mae: 2.6172\n",
            "Epoch 336/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4004 - mae: 1.0803 - val_loss: 15.2901 - val_mae: 2.5002\n",
            "Epoch 337/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4918 - mae: 1.0975 - val_loss: 16.5250 - val_mae: 2.6379\n",
            "Epoch 338/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3911 - mae: 1.0602 - val_loss: 18.3009 - val_mae: 2.6837\n",
            "Epoch 339/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3346 - mae: 1.1072 - val_loss: 15.4102 - val_mae: 2.5611\n",
            "Epoch 340/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2965 - mae: 1.0544 - val_loss: 17.0680 - val_mae: 2.6193\n",
            "Epoch 341/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3615 - mae: 1.0538 - val_loss: 15.6124 - val_mae: 2.5916\n",
            "Epoch 342/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4322 - mae: 1.0855 - val_loss: 15.4692 - val_mae: 2.7231\n",
            "Epoch 343/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3591 - mae: 1.0767 - val_loss: 17.1720 - val_mae: 2.6823\n",
            "Epoch 344/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3845 - mae: 1.0887 - val_loss: 15.9581 - val_mae: 2.5611\n",
            "Epoch 345/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4334 - mae: 1.0745 - val_loss: 16.0281 - val_mae: 2.5919\n",
            "Epoch 346/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5006 - mae: 1.0953 - val_loss: 16.1517 - val_mae: 2.6409\n",
            "Epoch 347/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2519 - mae: 1.0314 - val_loss: 15.9174 - val_mae: 2.5761\n",
            "Epoch 348/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4935 - mae: 1.1003 - val_loss: 16.0498 - val_mae: 2.5414\n",
            "Epoch 349/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3031 - mae: 1.0604 - val_loss: 15.4517 - val_mae: 2.5095\n",
            "Epoch 350/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4513 - mae: 1.0602 - val_loss: 15.4970 - val_mae: 2.6281\n",
            "Epoch 351/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3859 - mae: 1.0457 - val_loss: 16.9630 - val_mae: 2.6160\n",
            "Epoch 352/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2849 - mae: 1.0428 - val_loss: 15.0804 - val_mae: 2.5913\n",
            "Epoch 353/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3747 - mae: 1.0468 - val_loss: 14.9257 - val_mae: 2.5059\n",
            "Epoch 354/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2550 - mae: 1.0443 - val_loss: 19.7387 - val_mae: 2.8243\n",
            "Epoch 355/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4162 - mae: 1.0656 - val_loss: 14.8614 - val_mae: 2.5347\n",
            "Epoch 356/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3805 - mae: 1.0427 - val_loss: 15.5548 - val_mae: 2.5017\n",
            "Epoch 357/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 2.1306 - mae: 0.9990 - val_loss: 17.2241 - val_mae: 2.7781\n",
            "Epoch 358/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4311 - mae: 1.1010 - val_loss: 17.0324 - val_mae: 2.5847\n",
            "Epoch 359/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3442 - mae: 1.0341 - val_loss: 16.0107 - val_mae: 2.5792\n",
            "Epoch 360/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1640 - mae: 1.0128 - val_loss: 14.8392 - val_mae: 2.6104\n",
            "Epoch 361/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1970 - mae: 1.0287 - val_loss: 16.2864 - val_mae: 2.5254\n",
            "Epoch 362/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1387 - mae: 0.9980 - val_loss: 15.2284 - val_mae: 2.6404\n",
            "Epoch 363/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3089 - mae: 1.0577 - val_loss: 15.5940 - val_mae: 2.5815\n",
            "Epoch 364/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2940 - mae: 1.0264 - val_loss: 15.8223 - val_mae: 2.5399\n",
            "Epoch 365/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.0948 - mae: 1.0039 - val_loss: 17.4255 - val_mae: 2.5616\n",
            "Epoch 366/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3621 - mae: 1.0925 - val_loss: 17.6226 - val_mae: 2.8400\n",
            "Epoch 367/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4334 - mae: 1.0538 - val_loss: 14.6670 - val_mae: 2.6842\n",
            "Epoch 368/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3542 - mae: 1.0312 - val_loss: 16.0201 - val_mae: 2.5453\n",
            "Epoch 369/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1586 - mae: 1.0123 - val_loss: 17.2548 - val_mae: 2.5946\n",
            "Epoch 370/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1700 - mae: 1.0294 - val_loss: 16.9985 - val_mae: 2.6593\n",
            "Epoch 371/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3232 - mae: 1.0583 - val_loss: 17.2297 - val_mae: 2.5802\n",
            "Epoch 372/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2259 - mae: 1.0358 - val_loss: 18.2017 - val_mae: 2.6129\n",
            "Epoch 373/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2178 - mae: 1.0004 - val_loss: 15.6343 - val_mae: 2.6000\n",
            "Epoch 374/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1657 - mae: 1.0301 - val_loss: 19.5457 - val_mae: 2.8590\n",
            "Epoch 375/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3366 - mae: 1.0623 - val_loss: 15.9888 - val_mae: 2.5272\n",
            "Epoch 376/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0506 - mae: 0.9835 - val_loss: 15.1876 - val_mae: 2.6096\n",
            "Epoch 377/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1757 - mae: 1.0291 - val_loss: 16.7034 - val_mae: 2.5384\n",
            "Epoch 378/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1123 - mae: 1.0011 - val_loss: 15.3645 - val_mae: 2.5879\n",
            "Epoch 379/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0898 - mae: 0.9949 - val_loss: 18.3857 - val_mae: 2.6691\n",
            "Epoch 380/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2296 - mae: 1.0340 - val_loss: 15.3762 - val_mae: 2.5947\n",
            "Epoch 381/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1773 - mae: 1.0082 - val_loss: 18.4330 - val_mae: 2.6759\n",
            "Epoch 382/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3537 - mae: 1.0563 - val_loss: 15.9949 - val_mae: 2.6250\n",
            "Epoch 383/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2070 - mae: 1.0137 - val_loss: 15.7472 - val_mae: 2.5251\n",
            "Epoch 384/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2155 - mae: 1.0568 - val_loss: 17.3657 - val_mae: 2.6133\n",
            "Epoch 385/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1806 - mae: 1.0527 - val_loss: 16.1414 - val_mae: 2.5703\n",
            "Epoch 386/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.0109 - mae: 0.9896 - val_loss: 15.6152 - val_mae: 2.6236\n",
            "Epoch 387/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2326 - mae: 1.0563 - val_loss: 16.4981 - val_mae: 2.5807\n",
            "Epoch 388/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3129 - mae: 1.0560 - val_loss: 15.4632 - val_mae: 2.4967\n",
            "Epoch 389/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0938 - mae: 1.0181 - val_loss: 16.3418 - val_mae: 2.6022\n",
            "Epoch 390/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0843 - mae: 1.0021 - val_loss: 19.8069 - val_mae: 3.0300\n",
            "Epoch 391/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2896 - mae: 1.0854 - val_loss: 14.7980 - val_mae: 2.5770\n",
            "Epoch 392/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0676 - mae: 0.9850 - val_loss: 18.1081 - val_mae: 2.6425\n",
            "Epoch 393/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0799 - mae: 0.9749 - val_loss: 16.3391 - val_mae: 2.5594\n",
            "Epoch 394/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0759 - mae: 0.9805 - val_loss: 18.1624 - val_mae: 2.6483\n",
            "Epoch 395/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1393 - mae: 1.0333 - val_loss: 16.0685 - val_mae: 2.6959\n",
            "Epoch 396/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0163 - mae: 1.0192 - val_loss: 15.8685 - val_mae: 2.6424\n",
            "Epoch 397/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0171 - mae: 0.9910 - val_loss: 14.9998 - val_mae: 2.5167\n",
            "Epoch 398/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.0729 - mae: 1.0057 - val_loss: 14.5254 - val_mae: 2.4682\n",
            "Epoch 399/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1090 - mae: 1.0010 - val_loss: 17.4976 - val_mae: 2.5842\n",
            "Epoch 400/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0294 - mae: 0.9764 - val_loss: 16.9078 - val_mae: 2.5703\n",
            "Epoch 401/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9402 - mae: 0.9823 - val_loss: 15.2320 - val_mae: 2.5493\n",
            "Epoch 402/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0490 - mae: 1.0160 - val_loss: 15.8434 - val_mae: 2.5038\n",
            "Epoch 403/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0306 - mae: 0.9868 - val_loss: 16.2058 - val_mae: 2.5975\n",
            "Epoch 404/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0374 - mae: 1.0067 - val_loss: 17.1011 - val_mae: 2.5575\n",
            "Epoch 405/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1476 - mae: 1.0337 - val_loss: 15.4241 - val_mae: 2.5446\n",
            "Epoch 406/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9385 - mae: 0.9676 - val_loss: 17.1439 - val_mae: 2.7546\n",
            "Epoch 407/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1099 - mae: 1.0161 - val_loss: 15.3027 - val_mae: 2.4765\n",
            "Epoch 408/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8636 - mae: 0.9479 - val_loss: 18.0901 - val_mae: 2.6604\n",
            "Epoch 409/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0693 - mae: 1.0038 - val_loss: 18.3722 - val_mae: 2.7658\n",
            "Epoch 410/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1124 - mae: 1.0365 - val_loss: 15.4501 - val_mae: 2.5927\n",
            "Epoch 411/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9718 - mae: 1.0032 - val_loss: 17.0666 - val_mae: 2.8418\n",
            "Epoch 412/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2167 - mae: 1.0349 - val_loss: 15.8074 - val_mae: 2.5137\n",
            "Epoch 413/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9136 - mae: 0.9482 - val_loss: 15.0682 - val_mae: 2.5263\n",
            "Epoch 414/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1334 - mae: 1.0229 - val_loss: 16.8285 - val_mae: 2.5868\n",
            "Epoch 415/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9677 - mae: 0.9578 - val_loss: 15.1617 - val_mae: 2.5435\n",
            "Epoch 416/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 2.2039 - mae: 1.0297 - val_loss: 16.8103 - val_mae: 2.5342\n",
            "Epoch 417/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9184 - mae: 0.9475 - val_loss: 15.2946 - val_mae: 2.5005\n",
            "Epoch 418/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9270 - mae: 0.9683 - val_loss: 15.6224 - val_mae: 2.4911\n",
            "Epoch 419/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0929 - mae: 1.0277 - val_loss: 17.7900 - val_mae: 2.5834\n",
            "Epoch 420/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9036 - mae: 0.9448 - val_loss: 14.9798 - val_mae: 2.5533\n",
            "Epoch 421/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8222 - mae: 0.9269 - val_loss: 16.8832 - val_mae: 2.5681\n",
            "Epoch 422/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8583 - mae: 0.9551 - val_loss: 15.3136 - val_mae: 2.7248\n",
            "Epoch 423/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1017 - mae: 0.9951 - val_loss: 20.1477 - val_mae: 2.7334\n",
            "Epoch 424/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9967 - mae: 0.9480 - val_loss: 15.0326 - val_mae: 2.5571\n",
            "Epoch 425/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9812 - mae: 0.9870 - val_loss: 15.6647 - val_mae: 2.5731\n",
            "Epoch 426/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9981 - mae: 0.9970 - val_loss: 15.9848 - val_mae: 2.5115\n",
            "Epoch 427/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9539 - mae: 0.9594 - val_loss: 16.1675 - val_mae: 2.5713\n",
            "Epoch 428/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8019 - mae: 0.9245 - val_loss: 15.2988 - val_mae: 2.5011\n",
            "Epoch 429/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9197 - mae: 0.9670 - val_loss: 20.0821 - val_mae: 2.9090\n",
            "Epoch 430/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.1491 - mae: 1.0270 - val_loss: 17.6225 - val_mae: 2.6138\n",
            "Epoch 431/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8863 - mae: 0.9303 - val_loss: 15.9474 - val_mae: 2.6234\n",
            "Epoch 432/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9660 - mae: 0.9590 - val_loss: 16.9316 - val_mae: 2.7247\n",
            "Epoch 433/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9844 - mae: 0.9969 - val_loss: 19.5024 - val_mae: 2.8247\n",
            "Epoch 434/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9331 - mae: 0.9524 - val_loss: 16.7402 - val_mae: 2.5497\n",
            "Epoch 435/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0196 - mae: 0.9804 - val_loss: 16.0216 - val_mae: 2.5784\n",
            "Epoch 436/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9791 - mae: 0.9562 - val_loss: 15.6953 - val_mae: 2.6797\n",
            "Epoch 437/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0406 - mae: 0.9951 - val_loss: 16.3277 - val_mae: 2.5408\n",
            "Epoch 438/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9128 - mae: 0.9598 - val_loss: 16.2809 - val_mae: 2.4784\n",
            "Epoch 439/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8636 - mae: 0.9580 - val_loss: 18.5459 - val_mae: 2.5927\n",
            "Epoch 440/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.7591 - mae: 0.9222 - val_loss: 16.0875 - val_mae: 2.4452\n",
            "Epoch 441/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0813 - mae: 1.0102 - val_loss: 16.0785 - val_mae: 2.4950\n",
            "Epoch 442/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8139 - mae: 0.9034 - val_loss: 20.0220 - val_mae: 2.6685\n",
            "Epoch 443/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9273 - mae: 0.9556 - val_loss: 16.5066 - val_mae: 2.6636\n",
            "Epoch 444/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8238 - mae: 0.9557 - val_loss: 16.7763 - val_mae: 2.5146\n",
            "Epoch 445/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7641 - mae: 0.9166 - val_loss: 17.7262 - val_mae: 2.5721\n",
            "Epoch 446/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8518 - mae: 0.9504 - val_loss: 15.4969 - val_mae: 2.6180\n",
            "Epoch 447/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9783 - mae: 0.9982 - val_loss: 15.8760 - val_mae: 2.5434\n",
            "Epoch 448/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7304 - mae: 0.9117 - val_loss: 16.4800 - val_mae: 2.5658\n",
            "Epoch 449/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8736 - mae: 0.9548 - val_loss: 15.3597 - val_mae: 2.5397\n",
            "Epoch 450/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8518 - mae: 0.9668 - val_loss: 15.1942 - val_mae: 2.5313\n",
            "Epoch 451/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8448 - mae: 0.9415 - val_loss: 15.1417 - val_mae: 2.5789\n",
            "Epoch 452/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8646 - mae: 0.9742 - val_loss: 18.9044 - val_mae: 2.7639\n",
            "Epoch 453/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8381 - mae: 0.9666 - val_loss: 15.1044 - val_mae: 2.4850\n",
            "Epoch 454/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8034 - mae: 0.9433 - val_loss: 15.9889 - val_mae: 2.6222\n",
            "Epoch 455/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9282 - mae: 0.9445 - val_loss: 16.9245 - val_mae: 2.6385\n",
            "Epoch 456/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8416 - mae: 0.9106 - val_loss: 16.2046 - val_mae: 2.5278\n",
            "Epoch 457/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7572 - mae: 0.8888 - val_loss: 20.0554 - val_mae: 2.7655\n",
            "Epoch 458/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8704 - mae: 0.9575 - val_loss: 17.4438 - val_mae: 2.5816\n",
            "Epoch 459/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8870 - mae: 0.9680 - val_loss: 16.1903 - val_mae: 2.5857\n",
            "Epoch 460/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7252 - mae: 0.9355 - val_loss: 16.1455 - val_mae: 2.5642\n",
            "Epoch 461/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8928 - mae: 0.9451 - val_loss: 16.0251 - val_mae: 2.5869\n",
            "Epoch 462/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8213 - mae: 0.9227 - val_loss: 15.6135 - val_mae: 2.5202\n",
            "Epoch 463/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8698 - mae: 0.9700 - val_loss: 16.6936 - val_mae: 2.5646\n",
            "Epoch 464/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6462 - mae: 0.8984 - val_loss: 16.8423 - val_mae: 2.6740\n",
            "Epoch 465/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8800 - mae: 0.9591 - val_loss: 16.5414 - val_mae: 2.5829\n",
            "Epoch 466/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8141 - mae: 0.9065 - val_loss: 16.1903 - val_mae: 2.5509\n",
            "Epoch 467/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0283 - mae: 0.9839 - val_loss: 16.1923 - val_mae: 2.5330\n",
            "Epoch 468/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.7453 - mae: 0.9035 - val_loss: 17.2329 - val_mae: 2.6069\n",
            "Epoch 469/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8920 - mae: 0.9429 - val_loss: 15.4022 - val_mae: 2.5810\n",
            "Epoch 470/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7366 - mae: 0.8802 - val_loss: 16.4204 - val_mae: 2.4955\n",
            "Epoch 471/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6107 - mae: 0.8870 - val_loss: 19.9478 - val_mae: 2.7223\n",
            "Epoch 472/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9857 - mae: 0.9638 - val_loss: 17.2952 - val_mae: 2.5998\n",
            "Epoch 473/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6065 - mae: 0.8768 - val_loss: 15.6839 - val_mae: 2.5316\n",
            "Epoch 474/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8943 - mae: 0.9606 - val_loss: 15.6821 - val_mae: 2.4976\n",
            "Epoch 475/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6611 - mae: 0.8767 - val_loss: 16.1713 - val_mae: 2.4874\n",
            "Epoch 476/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7446 - mae: 0.9301 - val_loss: 15.6798 - val_mae: 2.5632\n",
            "Epoch 477/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8842 - mae: 0.9864 - val_loss: 14.7668 - val_mae: 2.5637\n",
            "Epoch 478/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8283 - mae: 0.9221 - val_loss: 16.4317 - val_mae: 2.5700\n",
            "Epoch 479/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8610 - mae: 0.9751 - val_loss: 16.1364 - val_mae: 2.5100\n",
            "Epoch 480/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6643 - mae: 0.8848 - val_loss: 15.4770 - val_mae: 2.6850\n",
            "Epoch 481/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8654 - mae: 0.9198 - val_loss: 17.6373 - val_mae: 2.5518\n",
            "Epoch 482/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6754 - mae: 0.9035 - val_loss: 16.8526 - val_mae: 2.5253\n",
            "Epoch 483/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6660 - mae: 0.8858 - val_loss: 19.2803 - val_mae: 2.6504\n",
            "Epoch 484/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7115 - mae: 0.8881 - val_loss: 18.4754 - val_mae: 2.6274\n",
            "Epoch 485/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6451 - mae: 0.8982 - val_loss: 15.9659 - val_mae: 2.5427\n",
            "Epoch 486/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6426 - mae: 0.8576 - val_loss: 17.0737 - val_mae: 2.5140\n",
            "Epoch 487/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8002 - mae: 0.9602 - val_loss: 17.3713 - val_mae: 2.5546\n",
            "Epoch 488/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5617 - mae: 0.8474 - val_loss: 15.4029 - val_mae: 2.5307\n",
            "Epoch 489/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8597 - mae: 0.9621 - val_loss: 16.5656 - val_mae: 2.6436\n",
            "Epoch 490/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8345 - mae: 0.9016 - val_loss: 17.0057 - val_mae: 2.6309\n",
            "Epoch 491/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.7024 - mae: 0.9128 - val_loss: 16.3476 - val_mae: 2.5280\n",
            "Epoch 492/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5527 - mae: 0.8578 - val_loss: 17.7478 - val_mae: 2.5546\n",
            "Epoch 493/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.7887 - mae: 0.9128 - val_loss: 17.0238 - val_mae: 2.5426\n",
            "Epoch 494/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8195 - mae: 0.9507 - val_loss: 15.6626 - val_mae: 2.7112\n",
            "Epoch 495/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8848 - mae: 0.9562 - val_loss: 17.2812 - val_mae: 2.6708\n",
            "Epoch 496/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6367 - mae: 0.8728 - val_loss: 16.9002 - val_mae: 2.5322\n",
            "Epoch 497/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4672 - mae: 0.8329 - val_loss: 16.3220 - val_mae: 2.5439\n",
            "Epoch 498/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5994 - mae: 0.8687 - val_loss: 16.1895 - val_mae: 2.5948\n",
            "Epoch 499/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8456 - mae: 0.9260 - val_loss: 16.7254 - val_mae: 2.5681\n",
            "Epoch 500/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6687 - mae: 0.9047 - val_loss: 17.8323 - val_mae: 2.5870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQm3pc0FYPQB"
      },
      "source": [
        "Visualize the model's training progress using the stats stored in the `history` object. We want to use this data to determine how long to train *before* the model stops making progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6XriGbVPh2t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "68b44d90-84e5-4bff-f806-5178da6fd424"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3St8-DmrX8P4"
      },
      "source": [
        "The graph shows the average error **in Validation set** is about $2,600 dollars. Is this good? \n",
        "\n",
        "Well, \\$2,600 is not an insignificant amount when some of the labels are only $15,000.\n",
        "\n",
        "Let's see how did the model performs on the **Test set**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl_yNr5n1kms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69622e1d-70d5-45c9-eb45-dc8e65ce0a58"
      },
      "source": [
        "[loss, mae] = model.evaluate(test_data, test_labels, verbose=0)\n",
        "print(\"Testing set Mean Abs Error: ${:7.2f}\".format(mae * 1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing set Mean Abs Error: $2872.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft603OzXuEZC"
      },
      "source": [
        "## Predict\n",
        "\n",
        "Finally, predict some housing prices using data in the testing set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe7RXH3N3CWU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "b100e61c-8fe6-4db7-8bb6-ef39ac189b3f"
      },
      "source": [
        "test_predictions = model.predict(test_data).flatten()\n",
        "plot_prediction(test_labels, test_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAT2klEQVR4nO3de7BlZX3m8e8jxESQBBxOCBE6hxikQpx4yTEhahxv0VYzgyRoZCaC8dJxMirkphirBipTU0UmJnGSzMh0kAEmDFZiYIJBg3jhkinAdLeoCFGJaQwE6XbIRCFMLPA3f+x1wq7tPt27m7P22qff76fq1Nnrstf7O7t3P+c9717rXakqJEnteMzQBUiS5svgl6TGGPyS1BiDX5IaY/BLUmMOHrqAWRx55JG1vLw8dBmStKFs3779K1W1NLl+QwT/8vIy27ZtG7oMSdpQktw5bb1DPZLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxvQV/kguT7Epy65Rtv5SkkhzZV/uSpOn67PFfBGyeXJnkWODFwJd6bFuStIbegr+qrgfum7Lpt4G3Ad4IQJIGMNcrd5OcDNxdVZ9Ksrd9twBbADZt2jSH6jSE5bOvmrp+53kvn3MlUjvm9uFukkOAXwX+/Sz7V9XWqlqpqpWlpW+aakKStJ/meVbPk4DjgE8l2QkcA+xI8l1zrEGSmje3oZ6q+gzwnavLXfivVNVX5lWDJKnf0zkvA24ETkhyV5LX99WWJGl2vfX4q+q0vWxf7qttSdLavHJXkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaM9dpmXVgW2uKZXCaZWmR2OOXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1Jjegj/JhUl2Jbl1bN1vJPnLJJ9OckWSw/tqX5I0XZ89/ouAzRPrrgGeUlU/CHweeEeP7UuSpugt+KvqeuC+iXUfrqqHusWbgGP6al+SNN2QY/yvAz601sYkW5JsS7Jt9+7dcyxLkg5sgwR/kncCDwGXrrVPVW2tqpWqWllaWppfcZJ0gJv7fPxJXgv8BPDCqqp5ty9JrZtr8CfZDLwN+BdV9Q/zbFuSNNLn6ZyXATcCJyS5K8nrgd8DDgOuSXJLkvP7al+SNF1vPf6qOm3K6vf21Z4kaTZeuStJjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY2Z+yRt2viWz75q6BIkPQr2+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMb0FvxJLkyyK8mtY+uekOSaJF/ovh/RV/uSpOn67PFfBGyeWHc28NGqOh74aLcsSZqj3oK/qq4H7ptYfTJwcff4YuAVfbUvSZpu3mP8R1XVPd3jLwNHrbVjki1JtiXZtnv37vlUJ0kNGOzD3aoqoPawfWtVrVTVytLS0hwrk6QD27yD/94kRwN033fNuX1Jat68g/9K4Izu8RnAn8y5fUlqXp+nc14G3AickOSuJK8HzgN+PMkXgBd1y5KkOertnrtVddoam17YV5uSpL3zyl1JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWpMb1fuSn1YPvuqqet3nvfyOVcibVz2+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1ZpDgT/ILST6b5NYklyX5tiHqkKQWzRT8SZ49y7oZj/VE4K3ASlU9BTgIePX+HEuStO9m7fH/7ozrZnUw8LgkBwOHAH/7KI4lSdoHe5ydM8mPAs8ClpL84timb2fUU99nVXV3kncBXwIeBD5cVR+e0vYWYAvApk2b9qcpPUprzYQ59LH25fjO2il9s731+B8LPJ7RL4jDxr6+Cpy6Pw0mOQI4GTgO+G7g0CQ/M7lfVW2tqpWqWllaWtqfpiRJU+yxx19V1wHXJbmoqu5cpzZfBPx1Ve0GSHI5o78q/mCdji9J2oNZb8TyrUm2Asvjz6mqF+xHm18CTkpyCKOhnhcC2/bjOJKk/TBr8P8RcD5wAfDwo2mwqm5O8n5gB/AQ8Elg66M5piRpdrMG/0NV9Z71arSqzgHOWa/jSZJmN+vpnB9I8vNJjk7yhNWvXiuTJPVi1h7/Gd33XxlbV8D3rm85kqS+zRT8VXVc34VIkuZjpuBPcvq09VV1yfqWI0nq26xDPc8ce/xtjE7B3AEY/JK0wcw61POW8eUkhwPv66UiSVKv9nda5gcYTbkgSdpgZh3j/wCjs3hgNDnb9wN/2FdRkqT+zDrG/66xxw8Bd1bVXT3UI0nq2UxDPd1kbX/JaGbOI4Cv91mUJKk/s96B61XAJ4BXAq8Cbk6yX9MyS5KGNetQzzuBZ1bVLoAkS8BHgPf3VZgkqR+zntXzmNXQ7/yffXiuJGmBzNrj/7MkVwOXdcs/DXywn5IkSX3a2z13vw84qqp+JclPAs/pNt0IXNp3cZKk9be3Hv+7gXcAVNXlwOUASf55t+1f9lqdJGnd7W2c/qiq+szkym7dci8VSZJ6tbfgP3wP2x63noVIkuZjb8G/LckbJ1cmeQOwvZ+SJEl92tsY/1nAFUn+DY8E/QrwWOCUPguTJPVjj8FfVfcCz0ryfOAp3eqrqupjvVcmSerFrPPxfxz4+Ho12s3nfwGjXyYFvK6qblyv40uS1jbrBVzr7T8Df1ZVpyZ5LHDIQHVIUnPmHvxJvgN4LvBagKr6Os72KUlzM0SP/zhgN/DfkzyV0YfGZ1bVA+M7JdkCbAHYtGnT3IvUsJbPvmroEqQD1hATrR0MPAN4T1U9ndFtHM+e3KmqtlbVSlWtLC0tzbtGSTpgDRH8dwF3VdXN3fL7Gf0ikCTNwdyDv6q+DPxNkhO6VS8Ebpt3HZLUqqHO6nkLcGl3Rs8XgZ8dqA5Jas4gwV9VtzC6AliSNGfeRUuSGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0ZLPiTHJTkk0n+dKgaJKlFQ/b4zwRuH7B9SWrSIMGf5Bjg5cAFQ7QvSS0bqsf/buBtwDcGal+SmjX34E/yE8Cuqtq+l/22JNmWZNvu3bvnVJ0kHfiG6PE/G/hXSXYC7wNekOQPJneqqq1VtVJVK0tLS/OuUZIOWHMP/qp6R1UdU1XLwKuBj1XVz8y7DklqlefxS1JjDh6y8aq6Frh2yBokqTX2+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTGDXrmrfiyffdXQJUhaYPb4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxsw9+JMcm+TjSW5L8tkkZ867Bklq2RCTtD0E/FJV7UhyGLA9yTVVddsAtUhSc+be46+qe6pqR/f4a8DtwBPnXYcktWrQaZmTLANPB26esm0LsAVg06ZNc61rvaw1PfLO814+yHG0/vy30UY02Ie7SR4P/DFwVlV9dXJ7VW2tqpWqWllaWpp/gZJ0gBok+JN8C6PQv7SqLh+iBklq1RBn9QR4L3B7Vf3WvNuXpNYN0eN/NvAa4AVJbum+XjZAHZLUpLl/uFtVfw5k3u1Kkka8cleSGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhoz6LTM87CI0+auVdNQxzmQ7eu//0Z6vzj18/5btPfFnv4v9/HvbI9fkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqzCDBn2Rzks8luSPJ2UPUIEmtmnvwJzkI+C/AS4ETgdOSnDjvOiSpVUP0+H8YuKOqvlhVXwfeB5w8QB2S1KRU1XwbTE4FNlfVG7rl1wA/UlVvnthvC7ClWzwB+NxcC4Ujga/Muc19tRFqhI1R50aoETZGnRuhRtgYdT7aGr+nqpYmVy7sfPxVtRXYOlT7SbZV1cpQ7c9iI9QIG6POjVAjbIw6N0KNsDHq7KvGIYZ67gaOHVs+plsnSZqDIYL/L4DjkxyX5LHAq4ErB6hDkpo096GeqnooyZuBq4GDgAur6rPzrmMGgw0z7YONUCNsjDo3Qo2wMercCDXCxqizlxrn/uGuJGlYXrkrSY0x+CWpMQb/hCSvTPLZJN9IsjKx7R3dNBOfS/KSoWocl+TcJHcnuaX7etnQNa3aKFNzJNmZ5DPd67dt6HpWJbkwya4kt46te0KSa5J8oft+xALWuFDvySTHJvl4ktu6/9tndusX5rXcQ429vJaO8U9I8v3AN4D/BvxyVW3r1p8IXMboyuPvBj4CPLmqHh6q1q6uc4H7q+pdQ9YxqZua4/PAjwN3MTqb67Squm3QwqZIshNYqaqFupgnyXOB+4FLquop3br/BNxXVed1v0yPqKq3L1iN57JA78kkRwNHV9WOJIcB24FXAK9lQV7LPdT4Knp4Le3xT6iq26tq2lXCJwPvq6p/rKq/Bu5g9EtA0zk1x6NUVdcD902sPhm4uHt8MaNwGMwaNS6UqrqnqnZ0j78G3A48kQV6LfdQYy8M/tk9EfibseW76PEfZh+9Ocmnuz+7B/3Tf8wiv16TCvhwku3dVCGL7Kiquqd7/GXgqCGL2YNFfE+SZBl4OnAzC/paTtQIPbyWTQZ/ko8kuXXK10L2SPdS73uAJwFPA+4BfnPQYjem51TVMxjNGPvvuuGLhVejcdpFHKtdyPdkkscDfwycVVVfHd+2KK/llBp7eS0Xdq6ePlXVi/bjaYNNNTFrvUl+H/jTnsuZ1YaZmqOq7u6+70pyBaNhquuHrWpN9yY5uqru6caFdw1d0KSqunf18aK8J5N8C6NAvbSqLu9WL9RrOa3Gvl7LJnv8++lK4NVJvjXJccDxwCcGrmn1Q6FVpwC3rrXvnG2IqTmSHNp9mEaSQ4EXsziv4TRXAmd0j88A/mTAWqZatPdkkgDvBW6vqt8a27Qwr+VaNfb1WnpWz4QkpwC/CywB/xe4pape0m17J/A64CFGf4p9aLBCO0n+B6M/AwvYCfzc2LjloLpTz97NI1Nz/MeBS/omSb4XuKJbPBj4n4tSZ5LLgOcxmpr3XuAc4H8BfwhsAu4EXlVVg324ukaNz2OB3pNJngPcAHyG0Rl7AL/KaAx9IV7LPdR4Gj28lga/JDXGoR5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINf6y7Jw90Usrcm+aMkhzyKY12U5NTu8QXdLKlr7fu8JM8aW35TktP3t+2x4ywneXBsatxb1uO4e2hvdZrolW75zRlNbV1JjhzbL0l+p9v26STPGNt2Rjfd8BeSnDG2/oe6Y9/RPTcTbZ87sfyk7ue9v6+fV/Nn8KsPD1bV07pper8OvGl8Y5L9miqkqt6wl2mdnwf8U/BX1flVdcn+tDXFX3U/0+rXNx23m4p6zeVpuvCe9v/w+atTggP/G3gRo4uMxr2U0RXkxwNbGM3rQpInMLqQ6kcYTT9xztjkXu8B3jj2vM3dc05Mch3wpiQ7kpwGUFV/VVVP29vPoY3F4FffbgC+r+uN35DkSuC2JAcl+Y0kf9H1Vn8O/ikIfy+jG7h8BPjO1QMluXasF7y5C6hPJfloRjMavgn4ha6H+mMZ3cTil7v9n5bkpq6tK1aDsDvmryf5RJLPJ/mxffnhktyf5DeTfAr40SnLv5hHJtU7q3vOcvfzXcLoEvxj99RGVX2yqnZO2XQyo3nwq6puAg7vLvF/CXBNVd1XVX8HXANs7rZ9e1Xd1E1KdgmPTEV8LnAhcD7wbEZTbugAZfCrN13P/qWMLkMHeAZwZlU9GXg98PdV9UzgmcAbM5oD6RTgBOBE4HTGevBjx10Cfh/4qap6KvDKLhjPB36765HfMPG0S4C3V9UPdvWcM7bt4Kr6YeCsifXjVoc8Vr9Wf0EcCtxcVU+tqj8fXwYeBH6WUc/7pO5nfHr3vOOB/1pVP1BVkz35Wa019fWe1t81ZT2M/jI7EnhMVT1YVXfsZ03aAAx+9eFxSW4BtgFfYjT5FMAnupvYwGgytNO7/W4G/hmjMHwucFlVPVxVfwt8bMrxTwKuXz3W3uZXSfIdwOFVdV236uKunVWrszVuB5bXOMzkUM/qL5aHGc2oyJTl5wBXVNUDVXV/187qL4w7u176ong78EOM5n7/QJKnDl2Q+tPktMzq3YOT48LdZ4gPjK8C3lJVV0/sN8T9Wf+x+/4w+/5/4v9N3H5zcnktD+x9l71aa+rruxl93jG+/tpu/TFT9l+dmvpfJ/k1RsM8lzOaB14HIHv8GsrVwL/NaA5ykjw5o2mRrwd+uvsM4Gjg+VOeexPw3G5oaPXDTICvAYdN7lxVfw/83djwzGuA6yb368ENwCuSHNL9bKd069bLlYz+akqSkxgNnd3D6LV9cZIjus8yXgxc3W37apKTurN5TqebijjJD3TH/Aajv3wOXcc6tWDs8WsoFzAaVtnRhdBuRh80XgG8ALiN0TDRjZNPrKrdGd0i8fLujJhdjG7q/gHg/RndmewtE087Azg/o1NLv8ho7H1fPKkbllp1YVX9zp6e0N04+yIeuW/DBVX1ye6D6JkleSvwNuC7gE8n+WBVvQH4IPAyRvd//ge6n6mq7kvyH3jkA9pfGxsO+3ngIuBxwIe6L4BTklzAaMz/VOCt+1KjNhanZZYWTJKdwEpVfWWAts+tqnOnrL+/qh4/73rUD4d6pMWzG/jo6qmrc3bt+MLqBVyMbrKiA4Q9fklqjD1+SWqMwS9JjTH4JakxBr8kNeb/Aym64wYZery4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgGQuV-yqYZH"
      },
      "source": [
        "## Observations\n",
        "\n",
        "So far, we implemented a MLP model to handle the \"**Boston House Prices**\" regression problem.\n",
        "\n",
        "Mean Absolute Error in Validation is around **\\$2,600** whereas in Testing, it is about **$2900**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JlpuqrDLRNH"
      },
      "source": [
        "## Create the Conv1D model\n",
        "\n",
        "Let's build an Conv1D model. Here, we'll use a `Sequential` model with 3 Conv1D layers, one MaxPooling1D layer, and an output layer that returns a single, continuous value. The model building steps are wrapped in a function, `build_model` as we did above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8btqTYvL4D7"
      },
      "source": [
        "## Reshape Data sets\n",
        "As you might remember, Conv1D layer expects input shape in 3D as\n",
        "\n",
        "  `[batch_size, time_steps, input_dimension]`\n",
        "\n",
        "However, current data is in the shape of\n",
        "\n",
        "`[batch_size, features]`\n",
        "\n",
        "See below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqqudq9qpxVj",
        "outputId": "0e01041a-664e-4402-8001-4f38732ba1ad"
      },
      "source": [
        "print(train_data.shape)\n",
        "print(train_data[0].shape)\n",
        "print(train_data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(404, 13)\n",
            "(13,)\n",
            "[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
            "  0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
            "  0.8252202 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5pUZ4KXNJVM"
      },
      "source": [
        "That is, in the current data set each sample has 13 features and no timesteps!\n",
        "\n",
        "**Basically, we convert features to timesteps**\n",
        "\n",
        "To convert 2D of input data into a 3D input, we simply **reshape** as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF55ueLqqBa6",
        "outputId": "71c31219-ed89-4b22-951b-4f2515e8dfca"
      },
      "source": [
        "sample_size = train_data.shape[0] # number of samples in train set\n",
        "time_steps  = train_data.shape[1] # number of features in train set\n",
        "input_dimension = 1               # each feature is represented by 1 number\n",
        "\n",
        "train_data_reshaped = train_data.reshape(sample_size,time_steps,input_dimension)\n",
        "print(\"After reshape train data set shape:\\n\", train_data_reshaped.shape)\n",
        "print(\"1 Sample shape:\\n\",train_data_reshaped[0].shape)\n",
        "print(\"An example sample:\\n\", train_data_reshaped[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After reshape train data set shape:\n",
            " (404, 13, 1)\n",
            "1 Sample shape:\n",
            " (13, 1)\n",
            "An example sample:\n",
            " [[-0.27224633]\n",
            " [-0.48361547]\n",
            " [-0.43576161]\n",
            " [-0.25683275]\n",
            " [-0.1652266 ]\n",
            " [-0.1764426 ]\n",
            " [ 0.81306188]\n",
            " [ 0.1166983 ]\n",
            " [-0.62624905]\n",
            " [-0.59517003]\n",
            " [ 1.14850044]\n",
            " [ 0.44807713]\n",
            " [ 0.8252202 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydUsxRzwNl4-"
      },
      "source": [
        "After conversion, we have a train data set whose shape is\n",
        "\n",
        "  `[batch_size, time_steps, input_dimension]` ---> `[404, 13, 1]`\n",
        "\n",
        "That is, each sample has **13 time steps with 1 input dimension**. You can also think as `each sample has 13 rows 1 column`!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXLomVo5O0qQ"
      },
      "source": [
        "##Reminder\n",
        "* `Conv1D(filters=1, kernel_size=7, activation='relu')` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuyXqm-GZtJ-"
      },
      "source": [
        "<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/conv1d.gif?raw=true\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUHhjsXaaWvN"
      },
      "source": [
        "We need to **reshape** the Test data as well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCaXUzV-f-zN"
      },
      "source": [
        "test_data_reshaped = test_data.reshape(test_data.shape[0],test_data.shape[1],1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJlS3rd5Oevi"
      },
      "source": [
        "Now, we can create Conv1D model as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4JNcPxXqrBo",
        "outputId": "6e1efb48-ee0e-4997-ebda-27994f4e9865"
      },
      "source": [
        "def build_conv1D_model():\n",
        "\n",
        "  n_timesteps = train_data_reshaped.shape[1] #13\n",
        "  n_features  = train_data_reshaped.shape[2] #1 \n",
        "  model = keras.Sequential(name=\"model_conv1D\")\n",
        "  model.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n",
        "  model.add(keras.layers.Conv1D(filters=64, kernel_size=7, activation='relu', name=\"Conv1D_1\"))\n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "  model.add(keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', name=\"Conv1D_2\"))\n",
        "  \n",
        "  model.add(keras.layers.Conv1D(filters=16, kernel_size=2, activation='relu', name=\"Conv1D_3\"))\n",
        "  \n",
        "  model.add(keras.layers.MaxPooling1D(pool_size=2, name=\"MaxPooling1D\"))\n",
        "  model.add(keras.layers.Flatten())\n",
        "  model.add(keras.layers.Dense(32, activation='relu', name=\"Dense_1\"))\n",
        "  model.add(keras.layers.Dense(n_features, name=\"Dense_2\"))\n",
        "\n",
        "\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  model.compile(loss='mse',optimizer=optimizer,metrics=['mae'])\n",
        "  return model\n",
        "\n",
        "model_conv1D = build_conv1D_model()\n",
        "model_conv1D.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_conv1D\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Conv1D_1 (Conv1D)            (None, 7, 64)             512       \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 7, 64)             0         \n",
            "_________________________________________________________________\n",
            "Conv1D_2 (Conv1D)            (None, 5, 32)             6176      \n",
            "_________________________________________________________________\n",
            "Conv1D_3 (Conv1D)            (None, 4, 16)             1040      \n",
            "_________________________________________________________________\n",
            "MaxPooling1D (MaxPooling1D)  (None, 2, 16)             0         \n",
            "_________________________________________________________________\n",
            "flatten_21 (Flatten)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "Dense_1 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "Dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 8,817\n",
            "Trainable params: 8,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUXn_-Zis8iM"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "The model is trained for 500 epochs, and record the training and validation accuracy in the `history` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8Vufz-zDCVR",
        "outputId": "12910a60-0726-418c-82ae-91eae4cad812"
      },
      "source": [
        "# Store training stats\n",
        "history = model_conv1D.fit(train_data_reshaped, train_labels, epochs=EPOCHS,\n",
        "                    validation_split=0.2, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 533.2971 - mae: 21.2021 - val_loss: 519.1226 - val_mae: 20.8930\n",
            "Epoch 2/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 336.0115 - mae: 15.9971 - val_loss: 218.5526 - val_mae: 12.3240\n",
            "Epoch 3/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 121.2078 - mae: 8.7730 - val_loss: 105.7616 - val_mae: 7.6652\n",
            "Epoch 4/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 80.5962 - mae: 6.8488 - val_loss: 78.4770 - val_mae: 6.4787\n",
            "Epoch 5/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 67.4674 - mae: 6.1211 - val_loss: 71.0500 - val_mae: 6.1121\n",
            "Epoch 6/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 64.5087 - mae: 5.8620 - val_loss: 70.0177 - val_mae: 6.2327\n",
            "Epoch 7/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 61.8386 - mae: 5.7873 - val_loss: 60.7205 - val_mae: 5.5974\n",
            "Epoch 8/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 57.0637 - mae: 5.5021 - val_loss: 53.6096 - val_mae: 5.2439\n",
            "Epoch 9/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 55.9071 - mae: 5.3629 - val_loss: 52.4533 - val_mae: 5.1475\n",
            "Epoch 10/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 52.6844 - mae: 5.1990 - val_loss: 50.1954 - val_mae: 5.1016\n",
            "Epoch 11/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 48.9001 - mae: 4.8062 - val_loss: 48.3391 - val_mae: 4.8880\n",
            "Epoch 12/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 44.1644 - mae: 4.6390 - val_loss: 45.0518 - val_mae: 4.8648\n",
            "Epoch 13/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 43.1569 - mae: 4.6113 - val_loss: 51.4180 - val_mae: 5.0124\n",
            "Epoch 14/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 42.0231 - mae: 4.5757 - val_loss: 45.0649 - val_mae: 5.0334\n",
            "Epoch 15/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 43.1079 - mae: 4.7403 - val_loss: 52.7398 - val_mae: 5.1537\n",
            "Epoch 16/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 40.4842 - mae: 4.4735 - val_loss: 40.3392 - val_mae: 4.4542\n",
            "Epoch 17/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 41.6264 - mae: 4.4278 - val_loss: 46.8418 - val_mae: 4.5854\n",
            "Epoch 18/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 39.1954 - mae: 4.4420 - val_loss: 46.7407 - val_mae: 4.6141\n",
            "Epoch 19/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 35.7875 - mae: 4.1362 - val_loss: 44.1996 - val_mae: 4.4720\n",
            "Epoch 20/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 36.8013 - mae: 4.2747 - val_loss: 36.0216 - val_mae: 4.2931\n",
            "Epoch 21/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 33.1717 - mae: 4.0003 - val_loss: 49.6555 - val_mae: 4.8422\n",
            "Epoch 22/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 30.9816 - mae: 3.8901 - val_loss: 33.7578 - val_mae: 4.1255\n",
            "Epoch 23/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 34.6127 - mae: 4.0205 - val_loss: 35.1431 - val_mae: 4.1284\n",
            "Epoch 24/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 33.0349 - mae: 4.0204 - val_loss: 48.8732 - val_mae: 4.6297\n",
            "Epoch 25/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 35.4599 - mae: 4.0682 - val_loss: 33.2135 - val_mae: 3.9988\n",
            "Epoch 26/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 29.8536 - mae: 3.7483 - val_loss: 34.3740 - val_mae: 3.9105\n",
            "Epoch 27/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 31.1580 - mae: 3.8715 - val_loss: 42.6847 - val_mae: 4.3333\n",
            "Epoch 28/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 30.0072 - mae: 3.8440 - val_loss: 28.8182 - val_mae: 3.7722\n",
            "Epoch 29/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 27.8947 - mae: 3.6932 - val_loss: 38.3830 - val_mae: 4.1505\n",
            "Epoch 30/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 26.7625 - mae: 3.5883 - val_loss: 28.0876 - val_mae: 3.6958\n",
            "Epoch 31/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 25.1433 - mae: 3.5107 - val_loss: 31.2456 - val_mae: 3.7950\n",
            "Epoch 32/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 31.0877 - mae: 3.9844 - val_loss: 38.8951 - val_mae: 4.1626\n",
            "Epoch 33/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 26.7276 - mae: 3.5665 - val_loss: 27.6844 - val_mae: 3.6137\n",
            "Epoch 34/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.7605 - mae: 3.3740 - val_loss: 26.3652 - val_mae: 3.5904\n",
            "Epoch 35/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 28.2529 - mae: 3.7097 - val_loss: 26.8750 - val_mae: 3.5053\n",
            "Epoch 36/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 25.4479 - mae: 3.4596 - val_loss: 24.6663 - val_mae: 3.6692\n",
            "Epoch 37/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 24.8435 - mae: 3.5998 - val_loss: 39.8523 - val_mae: 4.4333\n",
            "Epoch 38/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 24.5899 - mae: 3.5032 - val_loss: 23.0696 - val_mae: 3.4710\n",
            "Epoch 39/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 25.9038 - mae: 3.5745 - val_loss: 39.3526 - val_mae: 4.5789\n",
            "Epoch 40/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.5050 - mae: 3.3341 - val_loss: 22.7733 - val_mae: 3.4975\n",
            "Epoch 41/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.2165 - mae: 3.4124 - val_loss: 26.8649 - val_mae: 3.5026\n",
            "Epoch 42/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.7495 - mae: 3.2489 - val_loss: 27.5704 - val_mae: 3.5621\n",
            "Epoch 43/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23.9599 - mae: 3.4962 - val_loss: 22.3137 - val_mae: 3.4937\n",
            "Epoch 44/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 21.1479 - mae: 3.2309 - val_loss: 22.9469 - val_mae: 3.5778\n",
            "Epoch 45/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.0852 - mae: 3.5241 - val_loss: 22.2921 - val_mae: 3.4684\n",
            "Epoch 46/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 21.6774 - mae: 3.2233 - val_loss: 21.9001 - val_mae: 3.4411\n",
            "Epoch 47/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 20.7681 - mae: 3.2308 - val_loss: 22.9416 - val_mae: 3.3407\n",
            "Epoch 48/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 20.5657 - mae: 3.1351 - val_loss: 21.5554 - val_mae: 3.3329\n",
            "Epoch 49/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 20.3702 - mae: 2.9931 - val_loss: 28.0089 - val_mae: 3.5689\n",
            "Epoch 50/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 21.8080 - mae: 3.3783 - val_loss: 22.9674 - val_mae: 3.2702\n",
            "Epoch 51/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 20.6538 - mae: 3.1387 - val_loss: 23.0912 - val_mae: 3.3965\n",
            "Epoch 52/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 19.3246 - mae: 3.1539 - val_loss: 30.3367 - val_mae: 3.8881\n",
            "Epoch 53/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 22.0367 - mae: 3.2927 - val_loss: 24.2900 - val_mae: 3.3499\n",
            "Epoch 54/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 21.2369 - mae: 3.2722 - val_loss: 20.0760 - val_mae: 3.3340\n",
            "Epoch 55/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 18.9260 - mae: 3.1616 - val_loss: 29.6228 - val_mae: 4.4617\n",
            "Epoch 56/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 19.4082 - mae: 3.1363 - val_loss: 20.3208 - val_mae: 3.1835\n",
            "Epoch 57/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 20.1637 - mae: 3.3040 - val_loss: 36.5342 - val_mae: 5.0833\n",
            "Epoch 58/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.7736 - mae: 3.5060 - val_loss: 22.3551 - val_mae: 3.6557\n",
            "Epoch 59/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 18.0667 - mae: 3.0329 - val_loss: 19.5419 - val_mae: 3.2624\n",
            "Epoch 60/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 17.9005 - mae: 3.0518 - val_loss: 23.6148 - val_mae: 3.3851\n",
            "Epoch 61/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 17.8585 - mae: 3.0522 - val_loss: 22.6796 - val_mae: 3.6874\n",
            "Epoch 62/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17.7781 - mae: 3.0342 - val_loss: 17.8237 - val_mae: 3.0995\n",
            "Epoch 63/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 18.7413 - mae: 3.1158 - val_loss: 33.7063 - val_mae: 4.7338\n",
            "Epoch 64/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 16.7642 - mae: 3.0522 - val_loss: 21.6745 - val_mae: 3.2375\n",
            "Epoch 65/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 18.7029 - mae: 3.1134 - val_loss: 33.1585 - val_mae: 4.6506\n",
            "Epoch 66/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 20.7967 - mae: 3.2670 - val_loss: 28.5400 - val_mae: 3.9456\n",
            "Epoch 67/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 18.6055 - mae: 3.1542 - val_loss: 26.5643 - val_mae: 4.0702\n",
            "Epoch 68/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 16.7580 - mae: 3.0226 - val_loss: 26.7118 - val_mae: 3.6453\n",
            "Epoch 69/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 18.9775 - mae: 3.1567 - val_loss: 18.5573 - val_mae: 3.1115\n",
            "Epoch 70/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 19.6801 - mae: 3.2865 - val_loss: 20.0450 - val_mae: 3.1482\n",
            "Epoch 71/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17.7769 - mae: 3.0828 - val_loss: 17.6941 - val_mae: 3.1058\n",
            "Epoch 72/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 17.1687 - mae: 3.0108 - val_loss: 27.4216 - val_mae: 3.9131\n",
            "Epoch 73/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 18.5071 - mae: 3.0366 - val_loss: 38.6522 - val_mae: 5.1967\n",
            "Epoch 74/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 21.3726 - mae: 3.2424 - val_loss: 26.4965 - val_mae: 4.1079\n",
            "Epoch 75/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 17.5042 - mae: 3.1318 - val_loss: 21.4107 - val_mae: 3.3631\n",
            "Epoch 76/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15.5044 - mae: 2.9083 - val_loss: 18.9135 - val_mae: 3.3129\n",
            "Epoch 77/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17.0796 - mae: 3.0629 - val_loss: 17.4999 - val_mae: 3.0869\n",
            "Epoch 78/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 17.9874 - mae: 3.0086 - val_loss: 20.0030 - val_mae: 3.4903\n",
            "Epoch 79/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17.1142 - mae: 3.0288 - val_loss: 17.4284 - val_mae: 2.9943\n",
            "Epoch 80/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 14.6067 - mae: 2.8427 - val_loss: 26.0793 - val_mae: 4.0204\n",
            "Epoch 81/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17.0275 - mae: 3.0928 - val_loss: 17.4957 - val_mae: 3.0918\n",
            "Epoch 82/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 16.0306 - mae: 2.8672 - val_loss: 20.3008 - val_mae: 3.4856\n",
            "Epoch 83/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 16.2084 - mae: 2.8500 - val_loss: 18.3683 - val_mae: 3.1792\n",
            "Epoch 84/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 15.1978 - mae: 2.9005 - val_loss: 16.8486 - val_mae: 3.0105\n",
            "Epoch 85/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 16.1203 - mae: 2.9546 - val_loss: 27.2328 - val_mae: 3.8304\n",
            "Epoch 86/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 15.5645 - mae: 2.8111 - val_loss: 37.3120 - val_mae: 5.0202\n",
            "Epoch 87/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 17.7717 - mae: 3.0830 - val_loss: 17.7538 - val_mae: 3.0157\n",
            "Epoch 88/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 14.7820 - mae: 2.8052 - val_loss: 17.0609 - val_mae: 3.0164\n",
            "Epoch 89/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14.6403 - mae: 2.7642 - val_loss: 20.6830 - val_mae: 3.2070\n",
            "Epoch 90/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 16.8595 - mae: 3.0655 - val_loss: 20.0400 - val_mae: 3.1592\n",
            "Epoch 91/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 16.2755 - mae: 2.8762 - val_loss: 25.0822 - val_mae: 3.6738\n",
            "Epoch 92/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 15.8771 - mae: 2.9001 - val_loss: 16.8661 - val_mae: 2.9510\n",
            "Epoch 93/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 14.0203 - mae: 2.7849 - val_loss: 16.9483 - val_mae: 2.8853\n",
            "Epoch 94/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 13.1247 - mae: 2.7412 - val_loss: 23.0975 - val_mae: 3.4871\n",
            "Epoch 95/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 15.2713 - mae: 2.8670 - val_loss: 20.0166 - val_mae: 3.1818\n",
            "Epoch 96/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 14.8995 - mae: 2.7641 - val_loss: 16.7024 - val_mae: 3.0308\n",
            "Epoch 97/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 15.7462 - mae: 2.8881 - val_loss: 27.6222 - val_mae: 4.0286\n",
            "Epoch 98/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13.6465 - mae: 2.6849 - val_loss: 20.7986 - val_mae: 3.3455\n",
            "Epoch 99/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 14.8776 - mae: 2.8920 - val_loss: 15.2854 - val_mae: 2.8104\n",
            "Epoch 100/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 13.0894 - mae: 2.6685 - val_loss: 14.2542 - val_mae: 2.7682\n",
            "Epoch 101/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13.2409 - mae: 2.7642 - val_loss: 33.2086 - val_mae: 4.5881\n",
            "Epoch 102/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14.2820 - mae: 2.8427 - val_loss: 27.7240 - val_mae: 4.2849\n",
            "Epoch 103/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 14.8598 - mae: 2.8727 - val_loss: 20.4557 - val_mae: 3.2410\n",
            "Epoch 104/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13.2876 - mae: 2.7420 - val_loss: 15.9284 - val_mae: 3.0699\n",
            "Epoch 105/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12.4503 - mae: 2.5500 - val_loss: 15.9314 - val_mae: 2.9697\n",
            "Epoch 106/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15.3878 - mae: 2.8519 - val_loss: 19.3037 - val_mae: 3.3829\n",
            "Epoch 107/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 14.1780 - mae: 2.8428 - val_loss: 18.9094 - val_mae: 3.2381\n",
            "Epoch 108/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13.8523 - mae: 2.7140 - val_loss: 21.9738 - val_mae: 3.3833\n",
            "Epoch 109/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14.1777 - mae: 2.7134 - val_loss: 23.6636 - val_mae: 3.5070\n",
            "Epoch 110/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 15.7125 - mae: 2.8648 - val_loss: 17.3574 - val_mae: 2.9471\n",
            "Epoch 111/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 13.8136 - mae: 2.6829 - val_loss: 18.4596 - val_mae: 3.1054\n",
            "Epoch 112/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12.5775 - mae: 2.5888 - val_loss: 24.8886 - val_mae: 3.8868\n",
            "Epoch 113/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 14.1885 - mae: 2.8250 - val_loss: 18.3632 - val_mae: 3.0645\n",
            "Epoch 114/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.9204 - mae: 2.5978 - val_loss: 19.7547 - val_mae: 3.1494\n",
            "Epoch 115/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13.3255 - mae: 2.5924 - val_loss: 35.8424 - val_mae: 4.7320\n",
            "Epoch 116/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14.7960 - mae: 2.7776 - val_loss: 23.8634 - val_mae: 3.6532\n",
            "Epoch 117/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12.4991 - mae: 2.5393 - val_loss: 20.8591 - val_mae: 3.3200\n",
            "Epoch 118/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14.5581 - mae: 2.7418 - val_loss: 17.5234 - val_mae: 2.9631\n",
            "Epoch 119/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13.2483 - mae: 2.6111 - val_loss: 19.9376 - val_mae: 3.2359\n",
            "Epoch 120/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13.8217 - mae: 2.7299 - val_loss: 26.6232 - val_mae: 3.9369\n",
            "Epoch 121/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 15.7408 - mae: 2.7384 - val_loss: 16.6345 - val_mae: 2.8638\n",
            "Epoch 122/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13.6319 - mae: 2.6521 - val_loss: 17.1783 - val_mae: 2.9759\n",
            "Epoch 123/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.8325 - mae: 2.6219 - val_loss: 21.9765 - val_mae: 3.3944\n",
            "Epoch 124/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13.9189 - mae: 2.6825 - val_loss: 24.9333 - val_mae: 3.5089\n",
            "Epoch 125/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 14.9228 - mae: 2.7812 - val_loss: 31.4116 - val_mae: 4.2747\n",
            "Epoch 126/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12.1922 - mae: 2.5512 - val_loss: 18.6077 - val_mae: 3.0563\n",
            "Epoch 127/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14.2488 - mae: 2.6891 - val_loss: 24.7205 - val_mae: 3.7867\n",
            "Epoch 128/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.9169 - mae: 2.5718 - val_loss: 15.5960 - val_mae: 2.8849\n",
            "Epoch 129/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.7824 - mae: 2.6586 - val_loss: 26.7058 - val_mae: 3.9517\n",
            "Epoch 130/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13.2747 - mae: 2.6147 - val_loss: 17.2011 - val_mae: 2.8665\n",
            "Epoch 131/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 12.3731 - mae: 2.6670 - val_loss: 18.6899 - val_mae: 3.1196\n",
            "Epoch 132/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13.6589 - mae: 2.6289 - val_loss: 19.5517 - val_mae: 3.2067\n",
            "Epoch 133/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.6341 - mae: 2.6273 - val_loss: 17.3801 - val_mae: 3.0100\n",
            "Epoch 134/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 14.2975 - mae: 2.7532 - val_loss: 21.7820 - val_mae: 3.5725\n",
            "Epoch 135/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 13.0032 - mae: 2.5956 - val_loss: 20.9231 - val_mae: 3.5063\n",
            "Epoch 136/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.7731 - mae: 2.5681 - val_loss: 15.6039 - val_mae: 2.8837\n",
            "Epoch 137/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 13.4354 - mae: 2.7314 - val_loss: 16.2322 - val_mae: 2.9043\n",
            "Epoch 138/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 11.2153 - mae: 2.4133 - val_loss: 16.6375 - val_mae: 2.8715\n",
            "Epoch 139/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.4400 - mae: 2.6130 - val_loss: 18.8046 - val_mae: 3.0520\n",
            "Epoch 140/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.8583 - mae: 2.6307 - val_loss: 21.8502 - val_mae: 3.3900\n",
            "Epoch 141/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.6090 - mae: 2.5540 - val_loss: 23.4125 - val_mae: 3.8236\n",
            "Epoch 142/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11.9423 - mae: 2.5935 - val_loss: 16.6031 - val_mae: 2.9378\n",
            "Epoch 143/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11.2676 - mae: 2.4283 - val_loss: 17.1397 - val_mae: 3.0615\n",
            "Epoch 144/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 12.6862 - mae: 2.6116 - val_loss: 30.0826 - val_mae: 4.4643\n",
            "Epoch 145/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.3238 - mae: 2.5672 - val_loss: 22.4615 - val_mae: 3.6626\n",
            "Epoch 146/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 13.9773 - mae: 2.6955 - val_loss: 14.8687 - val_mae: 2.8932\n",
            "Epoch 147/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.0431 - mae: 2.4857 - val_loss: 14.5923 - val_mae: 2.7970\n",
            "Epoch 148/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.5559 - mae: 2.3998 - val_loss: 24.6077 - val_mae: 3.8745\n",
            "Epoch 149/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.6957 - mae: 2.6076 - val_loss: 15.4468 - val_mae: 2.8138\n",
            "Epoch 150/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11.5308 - mae: 2.4141 - val_loss: 20.2393 - val_mae: 3.3379\n",
            "Epoch 151/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.5333 - mae: 2.6576 - val_loss: 15.1153 - val_mae: 2.8217\n",
            "Epoch 152/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 11.2319 - mae: 2.4494 - val_loss: 14.6208 - val_mae: 2.9168\n",
            "Epoch 153/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 10.9061 - mae: 2.3646 - val_loss: 25.5457 - val_mae: 4.0657\n",
            "Epoch 154/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12.2844 - mae: 2.5712 - val_loss: 16.3901 - val_mae: 2.9881\n",
            "Epoch 155/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.2433 - mae: 2.3239 - val_loss: 22.0996 - val_mae: 3.7335\n",
            "Epoch 156/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11.1768 - mae: 2.4859 - val_loss: 13.7207 - val_mae: 2.6864\n",
            "Epoch 157/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.5910 - mae: 2.3002 - val_loss: 16.9582 - val_mae: 2.9802\n",
            "Epoch 158/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11.3159 - mae: 2.4360 - val_loss: 25.1044 - val_mae: 3.8488\n",
            "Epoch 159/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 10.8054 - mae: 2.4502 - val_loss: 15.3988 - val_mae: 2.8549\n",
            "Epoch 160/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.3953 - mae: 2.4459 - val_loss: 17.4428 - val_mae: 3.0973\n",
            "Epoch 161/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 11.6691 - mae: 2.5652 - val_loss: 14.9732 - val_mae: 2.7567\n",
            "Epoch 162/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11.4301 - mae: 2.4358 - val_loss: 14.3600 - val_mae: 2.7209\n",
            "Epoch 163/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.7776 - mae: 2.4071 - val_loss: 21.2079 - val_mae: 3.5152\n",
            "Epoch 164/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11.5783 - mae: 2.5581 - val_loss: 32.1462 - val_mae: 4.6444\n",
            "Epoch 165/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11.1538 - mae: 2.5251 - val_loss: 14.7725 - val_mae: 2.8063\n",
            "Epoch 166/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6333 - mae: 2.2740 - val_loss: 14.0225 - val_mae: 2.7932\n",
            "Epoch 167/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 13.5204 - mae: 2.6327 - val_loss: 16.5195 - val_mae: 3.0738\n",
            "Epoch 168/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 10.8856 - mae: 2.4356 - val_loss: 15.4372 - val_mae: 2.8689\n",
            "Epoch 169/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.8265 - mae: 2.4142 - val_loss: 16.0673 - val_mae: 2.9578\n",
            "Epoch 170/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11.9446 - mae: 2.4340 - val_loss: 24.7330 - val_mae: 3.9156\n",
            "Epoch 171/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.9162 - mae: 2.3927 - val_loss: 14.0433 - val_mae: 2.7517\n",
            "Epoch 172/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6780 - mae: 2.3436 - val_loss: 23.3491 - val_mae: 3.6643\n",
            "Epoch 173/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9301 - mae: 2.3535 - val_loss: 24.0978 - val_mae: 3.8442\n",
            "Epoch 174/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11.7730 - mae: 2.5120 - val_loss: 39.8306 - val_mae: 5.3239\n",
            "Epoch 175/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12.5432 - mae: 2.6152 - val_loss: 13.8343 - val_mae: 2.7486\n",
            "Epoch 176/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.1602 - mae: 2.2260 - val_loss: 14.1527 - val_mae: 2.7854\n",
            "Epoch 177/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.0218 - mae: 2.3842 - val_loss: 13.6198 - val_mae: 2.6877\n",
            "Epoch 178/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.3118 - mae: 2.2554 - val_loss: 20.4739 - val_mae: 3.6143\n",
            "Epoch 179/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.2277 - mae: 2.4080 - val_loss: 27.3054 - val_mae: 4.0697\n",
            "Epoch 180/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11.8690 - mae: 2.6066 - val_loss: 17.2263 - val_mae: 3.0909\n",
            "Epoch 181/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9455 - mae: 2.3387 - val_loss: 23.2060 - val_mae: 3.7353\n",
            "Epoch 182/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7270 - mae: 2.1893 - val_loss: 14.1758 - val_mae: 2.7026\n",
            "Epoch 183/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8282 - mae: 2.3150 - val_loss: 18.6688 - val_mae: 3.2150\n",
            "Epoch 184/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 11.5905 - mae: 2.5122 - val_loss: 16.1539 - val_mae: 2.9882\n",
            "Epoch 185/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.5969 - mae: 2.4178 - val_loss: 12.7758 - val_mae: 2.6940\n",
            "Epoch 186/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.1495 - mae: 2.2985 - val_loss: 18.0633 - val_mae: 3.2069\n",
            "Epoch 187/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.7063 - mae: 2.3431 - val_loss: 16.1996 - val_mae: 2.9876\n",
            "Epoch 188/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.1713 - mae: 2.1888 - val_loss: 28.9551 - val_mae: 4.3494\n",
            "Epoch 189/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 12.0086 - mae: 2.5158 - val_loss: 12.5317 - val_mae: 2.6651\n",
            "Epoch 190/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8111 - mae: 2.3185 - val_loss: 12.7517 - val_mae: 2.6811\n",
            "Epoch 191/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9850 - mae: 2.3016 - val_loss: 14.2722 - val_mae: 2.7952\n",
            "Epoch 192/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7932 - mae: 2.3096 - val_loss: 25.7135 - val_mae: 4.0854\n",
            "Epoch 193/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9106 - mae: 2.3808 - val_loss: 14.6832 - val_mae: 2.9028\n",
            "Epoch 194/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.1144 - mae: 2.3268 - val_loss: 16.8499 - val_mae: 3.1507\n",
            "Epoch 195/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1670 - mae: 2.0503 - val_loss: 12.3165 - val_mae: 2.6654\n",
            "Epoch 196/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.4071 - mae: 2.3991 - val_loss: 12.9414 - val_mae: 2.7336\n",
            "Epoch 197/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.3345 - mae: 2.2964 - val_loss: 13.6374 - val_mae: 2.7719\n",
            "Epoch 198/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.4793 - mae: 2.2755 - val_loss: 18.3631 - val_mae: 3.2673\n",
            "Epoch 199/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.6756 - mae: 2.1636 - val_loss: 13.0335 - val_mae: 2.6693\n",
            "Epoch 200/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.3434 - mae: 2.3281 - val_loss: 11.2304 - val_mae: 2.5781\n",
            "Epoch 201/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.0159 - mae: 2.3087 - val_loss: 14.2531 - val_mae: 2.7957\n",
            "Epoch 202/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.9556 - mae: 2.2492 - val_loss: 12.1855 - val_mae: 2.6090\n",
            "Epoch 203/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.0904 - mae: 2.1824 - val_loss: 12.4202 - val_mae: 2.6377\n",
            "Epoch 204/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.2166 - mae: 2.1245 - val_loss: 12.7819 - val_mae: 2.6599\n",
            "Epoch 205/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.9717 - mae: 2.4184 - val_loss: 12.2214 - val_mae: 2.6651\n",
            "Epoch 206/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.9087 - mae: 2.1258 - val_loss: 13.6759 - val_mae: 2.8154\n",
            "Epoch 207/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.3386 - mae: 2.1880 - val_loss: 12.4956 - val_mae: 2.6779\n",
            "Epoch 208/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.1520 - mae: 2.1295 - val_loss: 11.8863 - val_mae: 2.6335\n",
            "Epoch 209/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 8.0523 - mae: 2.1473 - val_loss: 17.9389 - val_mae: 3.3081\n",
            "Epoch 210/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.8789 - mae: 2.0910 - val_loss: 11.4621 - val_mae: 2.6317\n",
            "Epoch 211/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.6876 - mae: 2.1451 - val_loss: 17.5592 - val_mae: 3.2781\n",
            "Epoch 212/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.3740 - mae: 2.2516 - val_loss: 21.6529 - val_mae: 3.6759\n",
            "Epoch 213/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7442 - mae: 2.2864 - val_loss: 21.8562 - val_mae: 3.6579\n",
            "Epoch 214/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11.4141 - mae: 2.4683 - val_loss: 14.8412 - val_mae: 2.8762\n",
            "Epoch 215/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.2676 - mae: 2.1705 - val_loss: 14.5340 - val_mae: 2.8648\n",
            "Epoch 216/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.5450 - mae: 2.3142 - val_loss: 14.0316 - val_mae: 2.7819\n",
            "Epoch 217/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.2708 - mae: 2.1995 - val_loss: 17.2082 - val_mae: 3.1420\n",
            "Epoch 218/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.1306 - mae: 2.4087 - val_loss: 16.5952 - val_mae: 2.9699\n",
            "Epoch 219/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.8596 - mae: 2.1003 - val_loss: 19.6272 - val_mae: 3.4024\n",
            "Epoch 220/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.1818 - mae: 2.2569 - val_loss: 13.4748 - val_mae: 2.7720\n",
            "Epoch 221/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.8691 - mae: 2.0642 - val_loss: 25.8235 - val_mae: 4.0563\n",
            "Epoch 222/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.8267 - mae: 2.1936 - val_loss: 16.6170 - val_mae: 3.0553\n",
            "Epoch 223/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.7590 - mae: 2.1508 - val_loss: 15.8336 - val_mae: 3.0252\n",
            "Epoch 224/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.1299 - mae: 2.2157 - val_loss: 13.2709 - val_mae: 2.6466\n",
            "Epoch 225/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.2981 - mae: 2.2153 - val_loss: 14.0278 - val_mae: 2.6431\n",
            "Epoch 226/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.0002 - mae: 2.0499 - val_loss: 12.6316 - val_mae: 2.6020\n",
            "Epoch 227/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.5082 - mae: 2.2155 - val_loss: 13.2067 - val_mae: 2.7030\n",
            "Epoch 228/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.4914 - mae: 2.1225 - val_loss: 11.7914 - val_mae: 2.5818\n",
            "Epoch 229/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.8857 - mae: 1.9686 - val_loss: 23.3936 - val_mae: 3.8615\n",
            "Epoch 230/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.5114 - mae: 2.1368 - val_loss: 13.6718 - val_mae: 2.7910\n",
            "Epoch 231/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10.3691 - mae: 2.3744 - val_loss: 11.1195 - val_mae: 2.4401\n",
            "Epoch 232/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1308 - mae: 2.1029 - val_loss: 16.2475 - val_mae: 3.0813\n",
            "Epoch 233/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.1278 - mae: 2.0095 - val_loss: 14.0765 - val_mae: 2.7448\n",
            "Epoch 234/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.6856 - mae: 2.1449 - val_loss: 15.4423 - val_mae: 2.9065\n",
            "Epoch 235/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1144 - mae: 2.1714 - val_loss: 11.9447 - val_mae: 2.5701\n",
            "Epoch 236/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.8990 - mae: 1.9631 - val_loss: 11.1829 - val_mae: 2.5214\n",
            "Epoch 237/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8326 - mae: 2.2383 - val_loss: 10.4275 - val_mae: 2.4223\n",
            "Epoch 238/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.7086 - mae: 2.0636 - val_loss: 15.2998 - val_mae: 2.9724\n",
            "Epoch 239/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.5064 - mae: 2.0833 - val_loss: 12.2040 - val_mae: 2.5506\n",
            "Epoch 240/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.1169 - mae: 2.2199 - val_loss: 18.5928 - val_mae: 3.3174\n",
            "Epoch 241/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.3029 - mae: 2.3120 - val_loss: 15.2698 - val_mae: 2.8987\n",
            "Epoch 242/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.8542 - mae: 1.9601 - val_loss: 11.6780 - val_mae: 2.5223\n",
            "Epoch 243/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.5255 - mae: 2.1434 - val_loss: 14.7257 - val_mae: 2.9038\n",
            "Epoch 244/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.0868 - mae: 2.3178 - val_loss: 11.2386 - val_mae: 2.5193\n",
            "Epoch 245/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.8212 - mae: 2.0387 - val_loss: 13.1982 - val_mae: 2.7500\n",
            "Epoch 246/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 8.5928 - mae: 2.1537 - val_loss: 27.7484 - val_mae: 4.3668\n",
            "Epoch 247/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9588 - mae: 2.2827 - val_loss: 13.1117 - val_mae: 2.7178\n",
            "Epoch 248/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.1093 - mae: 2.1315 - val_loss: 13.1903 - val_mae: 2.7562\n",
            "Epoch 249/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.8121 - mae: 2.1062 - val_loss: 12.1621 - val_mae: 2.6345\n",
            "Epoch 250/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.1228 - mae: 2.1460 - val_loss: 16.9941 - val_mae: 3.1653\n",
            "Epoch 251/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 7.8770 - mae: 2.1023 - val_loss: 25.9025 - val_mae: 4.0643\n",
            "Epoch 252/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.3575 - mae: 2.1641 - val_loss: 10.7723 - val_mae: 2.5150\n",
            "Epoch 253/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.3295 - mae: 2.0884 - val_loss: 16.3007 - val_mae: 3.0706\n",
            "Epoch 254/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.1048 - mae: 2.1021 - val_loss: 11.2289 - val_mae: 2.5371\n",
            "Epoch 255/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.1809 - mae: 2.0600 - val_loss: 11.7135 - val_mae: 2.6045\n",
            "Epoch 256/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 6.8166 - mae: 1.9073 - val_loss: 15.2224 - val_mae: 2.9965\n",
            "Epoch 257/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 7.4570 - mae: 2.1417 - val_loss: 11.7871 - val_mae: 2.5190\n",
            "Epoch 258/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.7978 - mae: 1.9441 - val_loss: 13.8085 - val_mae: 2.8034\n",
            "Epoch 259/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.6538 - mae: 2.0648 - val_loss: 18.6429 - val_mae: 3.1545\n",
            "Epoch 260/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.2564 - mae: 2.1789 - val_loss: 12.9385 - val_mae: 2.7424\n",
            "Epoch 261/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.6767 - mae: 2.1867 - val_loss: 13.7570 - val_mae: 2.8398\n",
            "Epoch 262/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.9970 - mae: 1.9736 - val_loss: 12.8289 - val_mae: 2.7143\n",
            "Epoch 263/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.6909 - mae: 1.9235 - val_loss: 24.6524 - val_mae: 3.9923\n",
            "Epoch 264/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.2258 - mae: 2.0220 - val_loss: 12.2008 - val_mae: 2.5927\n",
            "Epoch 265/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.0631 - mae: 2.0779 - val_loss: 11.4681 - val_mae: 2.5817\n",
            "Epoch 266/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.6474 - mae: 1.9755 - val_loss: 17.4685 - val_mae: 3.2546\n",
            "Epoch 267/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.8992 - mae: 2.1107 - val_loss: 16.9845 - val_mae: 3.2247\n",
            "Epoch 268/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.8710 - mae: 1.9993 - val_loss: 18.1496 - val_mae: 3.4010\n",
            "Epoch 269/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.3750 - mae: 2.0018 - val_loss: 15.1231 - val_mae: 2.7682\n",
            "Epoch 270/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.3389 - mae: 2.2302 - val_loss: 13.1265 - val_mae: 2.7236\n",
            "Epoch 271/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.5438 - mae: 2.0306 - val_loss: 12.0687 - val_mae: 2.6385\n",
            "Epoch 272/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.0380 - mae: 2.2711 - val_loss: 13.4590 - val_mae: 2.8003\n",
            "Epoch 273/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.6918 - mae: 2.0423 - val_loss: 13.1412 - val_mae: 2.7804\n",
            "Epoch 274/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.6415 - mae: 2.1757 - val_loss: 17.1324 - val_mae: 3.2267\n",
            "Epoch 275/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.1265 - mae: 2.0879 - val_loss: 16.6845 - val_mae: 3.2045\n",
            "Epoch 276/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.0119 - mae: 1.9347 - val_loss: 11.3783 - val_mae: 2.5906\n",
            "Epoch 277/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.0335 - mae: 1.9779 - val_loss: 15.8090 - val_mae: 3.0622\n",
            "Epoch 278/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.4796 - mae: 2.0766 - val_loss: 10.5255 - val_mae: 2.4969\n",
            "Epoch 279/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.4779 - mae: 1.8954 - val_loss: 12.9539 - val_mae: 2.6611\n",
            "Epoch 280/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.7714 - mae: 2.0555 - val_loss: 19.5859 - val_mae: 3.5264\n",
            "Epoch 281/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.9598 - mae: 2.1166 - val_loss: 12.7758 - val_mae: 2.7356\n",
            "Epoch 282/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.7955 - mae: 1.7795 - val_loss: 13.7335 - val_mae: 2.7157\n",
            "Epoch 283/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.2725 - mae: 2.0854 - val_loss: 16.7875 - val_mae: 3.1939\n",
            "Epoch 284/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.3554 - mae: 1.9940 - val_loss: 14.8664 - val_mae: 2.9290\n",
            "Epoch 285/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.7353 - mae: 2.0367 - val_loss: 10.9723 - val_mae: 2.5592\n",
            "Epoch 286/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.8284 - mae: 2.1670 - val_loss: 12.5711 - val_mae: 2.7304\n",
            "Epoch 287/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.0507 - mae: 2.1236 - val_loss: 11.2091 - val_mae: 2.5578\n",
            "Epoch 288/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.5762 - mae: 2.1173 - val_loss: 10.1952 - val_mae: 2.4377\n",
            "Epoch 289/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.3039 - mae: 2.0043 - val_loss: 10.8010 - val_mae: 2.4689\n",
            "Epoch 290/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.1311 - mae: 1.8917 - val_loss: 10.7352 - val_mae: 2.4997\n",
            "Epoch 291/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.4075 - mae: 2.0642 - val_loss: 13.5627 - val_mae: 2.8635\n",
            "Epoch 292/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 8.3611 - mae: 2.1614 - val_loss: 11.3551 - val_mae: 2.5492\n",
            "Epoch 293/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.2670 - mae: 2.1450 - val_loss: 16.2114 - val_mae: 3.0696\n",
            "Epoch 294/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 6.4391 - mae: 1.8953 - val_loss: 11.6768 - val_mae: 2.5391\n",
            "Epoch 295/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.2833 - mae: 1.8940 - val_loss: 17.6818 - val_mae: 3.2345\n",
            "Epoch 296/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.8562 - mae: 2.0223 - val_loss: 17.6282 - val_mae: 3.2006\n",
            "Epoch 297/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.7499 - mae: 2.0260 - val_loss: 11.4599 - val_mae: 2.6130\n",
            "Epoch 298/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.9041 - mae: 1.9899 - val_loss: 11.9013 - val_mae: 2.5987\n",
            "Epoch 299/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.0500 - mae: 2.0832 - val_loss: 13.1943 - val_mae: 2.7434\n",
            "Epoch 300/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.5416 - mae: 1.9078 - val_loss: 9.9767 - val_mae: 2.4549\n",
            "Epoch 301/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.6784 - mae: 2.1800 - val_loss: 12.0059 - val_mae: 2.6448\n",
            "Epoch 302/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 7.2937 - mae: 1.9545 - val_loss: 9.8598 - val_mae: 2.4534\n",
            "Epoch 303/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.4011 - mae: 1.8919 - val_loss: 9.7721 - val_mae: 2.4350\n",
            "Epoch 304/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.3909 - mae: 1.9370 - val_loss: 23.1511 - val_mae: 3.7594\n",
            "Epoch 305/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.0784 - mae: 2.1242 - val_loss: 10.0619 - val_mae: 2.4754\n",
            "Epoch 306/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 6.1755 - mae: 1.7884 - val_loss: 11.6281 - val_mae: 2.6176\n",
            "Epoch 307/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.6382 - mae: 2.0783 - val_loss: 13.6640 - val_mae: 2.8287\n",
            "Epoch 308/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.2976 - mae: 1.7102 - val_loss: 20.0275 - val_mae: 3.5564\n",
            "Epoch 309/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.5710 - mae: 1.9773 - val_loss: 10.3512 - val_mae: 2.4860\n",
            "Epoch 310/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.2979 - mae: 1.8964 - val_loss: 12.7829 - val_mae: 2.5774\n",
            "Epoch 311/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.5642 - mae: 1.9219 - val_loss: 11.6149 - val_mae: 2.5554\n",
            "Epoch 312/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.4933 - mae: 2.0847 - val_loss: 11.1738 - val_mae: 2.4872\n",
            "Epoch 313/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.7646 - mae: 1.9371 - val_loss: 11.4288 - val_mae: 2.6487\n",
            "Epoch 314/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.4391 - mae: 1.8746 - val_loss: 19.8931 - val_mae: 3.5699\n",
            "Epoch 315/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.0858 - mae: 1.8245 - val_loss: 12.7847 - val_mae: 2.7386\n",
            "Epoch 316/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 7.4883 - mae: 2.0188 - val_loss: 11.3132 - val_mae: 2.5378\n",
            "Epoch 317/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.6678 - mae: 1.9705 - val_loss: 10.3832 - val_mae: 2.4788\n",
            "Epoch 318/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 7.4521 - mae: 2.0314 - val_loss: 14.6824 - val_mae: 2.9350\n",
            "Epoch 319/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.5918 - mae: 2.1306 - val_loss: 11.6020 - val_mae: 2.5867\n",
            "Epoch 320/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.5981 - mae: 2.0372 - val_loss: 12.0269 - val_mae: 2.6220\n",
            "Epoch 321/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.5876 - mae: 1.9128 - val_loss: 17.2984 - val_mae: 3.3169\n",
            "Epoch 322/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.2702 - mae: 2.0227 - val_loss: 10.3766 - val_mae: 2.4911\n",
            "Epoch 323/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.1733 - mae: 2.0507 - val_loss: 10.6388 - val_mae: 2.5088\n",
            "Epoch 324/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.4807 - mae: 1.8953 - val_loss: 11.7446 - val_mae: 2.6023\n",
            "Epoch 325/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.9742 - mae: 1.8584 - val_loss: 12.3453 - val_mae: 2.6686\n",
            "Epoch 326/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.9033 - mae: 1.8676 - val_loss: 12.9940 - val_mae: 2.7858\n",
            "Epoch 327/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.8775 - mae: 1.9868 - val_loss: 11.4636 - val_mae: 2.5677\n",
            "Epoch 328/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.9961 - mae: 1.7743 - val_loss: 13.0300 - val_mae: 2.7303\n",
            "Epoch 329/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.8949 - mae: 1.9221 - val_loss: 11.5596 - val_mae: 2.5582\n",
            "Epoch 330/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.0934 - mae: 1.9225 - val_loss: 11.7523 - val_mae: 2.5649\n",
            "Epoch 331/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.8797 - mae: 1.8727 - val_loss: 10.9461 - val_mae: 2.5790\n",
            "Epoch 332/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.8575 - mae: 1.9442 - val_loss: 14.4545 - val_mae: 2.9172\n",
            "Epoch 333/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 7.3939 - mae: 2.0412 - val_loss: 11.1309 - val_mae: 2.5915\n",
            "Epoch 334/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.9250 - mae: 1.8628 - val_loss: 10.7976 - val_mae: 2.5579\n",
            "Epoch 335/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.6549 - mae: 2.0175 - val_loss: 11.6764 - val_mae: 2.5317\n",
            "Epoch 336/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.2333 - mae: 1.9277 - val_loss: 12.8975 - val_mae: 2.7378\n",
            "Epoch 337/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.7984 - mae: 1.9366 - val_loss: 12.2092 - val_mae: 2.6550\n",
            "Epoch 338/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.7839 - mae: 1.8618 - val_loss: 10.5347 - val_mae: 2.4933\n",
            "Epoch 339/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.9169 - mae: 1.8189 - val_loss: 10.9039 - val_mae: 2.5752\n",
            "Epoch 340/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.6006 - mae: 1.8910 - val_loss: 11.3092 - val_mae: 2.5960\n",
            "Epoch 341/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 6.8126 - mae: 1.9554 - val_loss: 11.5042 - val_mae: 2.5972\n",
            "Epoch 342/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.7922 - mae: 1.8162 - val_loss: 20.7045 - val_mae: 3.5776\n",
            "Epoch 343/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.3418 - mae: 1.9963 - val_loss: 13.0057 - val_mae: 2.8244\n",
            "Epoch 344/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.4318 - mae: 1.8507 - val_loss: 12.7553 - val_mae: 2.7655\n",
            "Epoch 345/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.6694 - mae: 1.8130 - val_loss: 12.4967 - val_mae: 2.7514\n",
            "Epoch 346/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.2931 - mae: 2.0609 - val_loss: 12.3233 - val_mae: 2.6167\n",
            "Epoch 347/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 6.7008 - mae: 1.9548 - val_loss: 13.2736 - val_mae: 2.7857\n",
            "Epoch 348/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.9157 - mae: 1.8116 - val_loss: 10.8095 - val_mae: 2.5214\n",
            "Epoch 349/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.5143 - mae: 1.8284 - val_loss: 12.1523 - val_mae: 2.6431\n",
            "Epoch 350/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.6151 - mae: 1.8803 - val_loss: 12.3766 - val_mae: 2.7313\n",
            "Epoch 351/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.6885 - mae: 1.8173 - val_loss: 11.4578 - val_mae: 2.5243\n",
            "Epoch 352/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.8868 - mae: 1.8289 - val_loss: 9.7864 - val_mae: 2.4458\n",
            "Epoch 353/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.3462 - mae: 1.9427 - val_loss: 11.9562 - val_mae: 2.6664\n",
            "Epoch 354/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.2577 - mae: 1.8421 - val_loss: 10.8375 - val_mae: 2.5112\n",
            "Epoch 355/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.1336 - mae: 1.7595 - val_loss: 10.8903 - val_mae: 2.4889\n",
            "Epoch 356/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.1366 - mae: 1.9912 - val_loss: 12.3816 - val_mae: 2.6832\n",
            "Epoch 357/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 6.8732 - mae: 1.9634 - val_loss: 10.0260 - val_mae: 2.4620\n",
            "Epoch 358/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 6.1508 - mae: 1.8807 - val_loss: 14.5621 - val_mae: 2.9938\n",
            "Epoch 359/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.2193 - mae: 1.8587 - val_loss: 19.5862 - val_mae: 3.5206\n",
            "Epoch 360/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.1344 - mae: 1.9221 - val_loss: 9.6104 - val_mae: 2.3729\n",
            "Epoch 361/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.3746 - mae: 1.7699 - val_loss: 13.0644 - val_mae: 2.7788\n",
            "Epoch 362/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 6.4001 - mae: 1.8906 - val_loss: 10.2048 - val_mae: 2.4487\n",
            "Epoch 363/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.7935 - mae: 1.7790 - val_loss: 13.9139 - val_mae: 2.9199\n",
            "Epoch 364/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.4938 - mae: 1.7664 - val_loss: 16.8263 - val_mae: 3.1633\n",
            "Epoch 365/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.2421 - mae: 2.0854 - val_loss: 10.8007 - val_mae: 2.4835\n",
            "Epoch 366/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.6898 - mae: 1.8408 - val_loss: 12.8106 - val_mae: 2.7954\n",
            "Epoch 367/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.1860 - mae: 1.8492 - val_loss: 11.0938 - val_mae: 2.5199\n",
            "Epoch 368/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.2225 - mae: 1.8988 - val_loss: 10.9105 - val_mae: 2.4854\n",
            "Epoch 369/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.3720 - mae: 1.7535 - val_loss: 9.9638 - val_mae: 2.4303\n",
            "Epoch 370/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.0111 - mae: 1.8461 - val_loss: 9.4723 - val_mae: 2.4241\n",
            "Epoch 371/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.6768 - mae: 1.7680 - val_loss: 9.5475 - val_mae: 2.4593\n",
            "Epoch 372/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.9577 - mae: 1.8060 - val_loss: 17.0432 - val_mae: 3.1777\n",
            "Epoch 373/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.5959 - mae: 1.9910 - val_loss: 9.4031 - val_mae: 2.3807\n",
            "Epoch 374/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.7205 - mae: 1.6226 - val_loss: 9.3725 - val_mae: 2.3551\n",
            "Epoch 375/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.8371 - mae: 1.7915 - val_loss: 11.5661 - val_mae: 2.6517\n",
            "Epoch 376/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 6.2044 - mae: 1.8982 - val_loss: 8.8287 - val_mae: 2.3578\n",
            "Epoch 377/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.8063 - mae: 1.7890 - val_loss: 20.3469 - val_mae: 3.6026\n",
            "Epoch 378/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.0916 - mae: 1.9602 - val_loss: 10.8390 - val_mae: 2.5896\n",
            "Epoch 379/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.5774 - mae: 1.9248 - val_loss: 9.5936 - val_mae: 2.4306\n",
            "Epoch 380/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.3715 - mae: 1.8757 - val_loss: 11.9487 - val_mae: 2.6945\n",
            "Epoch 381/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.6700 - mae: 1.9440 - val_loss: 10.9981 - val_mae: 2.5762\n",
            "Epoch 382/500\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.5654 - mae: 1.7338 - val_loss: 14.5674 - val_mae: 2.9705\n",
            "Epoch 383/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.4781 - mae: 1.9686 - val_loss: 11.4714 - val_mae: 2.6508\n",
            "Epoch 384/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.6434 - mae: 1.7991 - val_loss: 9.3542 - val_mae: 2.4274\n",
            "Epoch 385/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.7963 - mae: 1.8886 - val_loss: 9.8855 - val_mae: 2.4449\n",
            "Epoch 386/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.7654 - mae: 1.9088 - val_loss: 10.7630 - val_mae: 2.5408\n",
            "Epoch 387/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.6589 - mae: 1.7848 - val_loss: 9.4965 - val_mae: 2.4300\n",
            "Epoch 388/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.2507 - mae: 1.8180 - val_loss: 11.1940 - val_mae: 2.5904\n",
            "Epoch 389/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.1445 - mae: 1.8272 - val_loss: 8.5338 - val_mae: 2.2758\n",
            "Epoch 390/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.7601 - mae: 1.7640 - val_loss: 10.0401 - val_mae: 2.4317\n",
            "Epoch 391/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.8162 - mae: 1.8417 - val_loss: 17.4305 - val_mae: 3.2760\n",
            "Epoch 392/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 7.3637 - mae: 1.8859 - val_loss: 10.7249 - val_mae: 2.5839\n",
            "Epoch 393/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.3745 - mae: 1.6877 - val_loss: 9.9409 - val_mae: 2.4463\n",
            "Epoch 394/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.7174 - mae: 1.8267 - val_loss: 15.6337 - val_mae: 3.0790\n",
            "Epoch 395/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.6681 - mae: 1.8928 - val_loss: 12.2946 - val_mae: 2.5317\n",
            "Epoch 396/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.6161 - mae: 1.7882 - val_loss: 13.0550 - val_mae: 2.7558\n",
            "Epoch 397/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.1449 - mae: 1.9304 - val_loss: 13.3774 - val_mae: 2.7627\n",
            "Epoch 398/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.5418 - mae: 1.8154 - val_loss: 12.4259 - val_mae: 2.7547\n",
            "Epoch 399/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.5616 - mae: 1.7405 - val_loss: 10.0326 - val_mae: 2.4951\n",
            "Epoch 400/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.9304 - mae: 1.7226 - val_loss: 13.7405 - val_mae: 2.8587\n",
            "Epoch 401/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.0470 - mae: 1.8653 - val_loss: 9.1918 - val_mae: 2.3636\n",
            "Epoch 402/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.0781 - mae: 1.7063 - val_loss: 9.7687 - val_mae: 2.4340\n",
            "Epoch 403/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.4025 - mae: 1.8463 - val_loss: 9.1044 - val_mae: 2.3256\n",
            "Epoch 404/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.3848 - mae: 1.8381 - val_loss: 17.9188 - val_mae: 3.2997\n",
            "Epoch 405/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.1416 - mae: 1.8673 - val_loss: 9.4879 - val_mae: 2.4096\n",
            "Epoch 406/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.8704 - mae: 1.6587 - val_loss: 10.7513 - val_mae: 2.5152\n",
            "Epoch 407/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.1195 - mae: 1.7299 - val_loss: 16.6157 - val_mae: 3.2454\n",
            "Epoch 408/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.9531 - mae: 1.7765 - val_loss: 9.5213 - val_mae: 2.4253\n",
            "Epoch 409/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.5312 - mae: 1.8014 - val_loss: 14.1323 - val_mae: 2.9958\n",
            "Epoch 410/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.8842 - mae: 1.6112 - val_loss: 14.8746 - val_mae: 2.9611\n",
            "Epoch 411/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.2192 - mae: 1.8497 - val_loss: 11.1283 - val_mae: 2.6531\n",
            "Epoch 412/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.7728 - mae: 1.6063 - val_loss: 9.8554 - val_mae: 2.4426\n",
            "Epoch 413/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.1421 - mae: 1.7587 - val_loss: 9.5704 - val_mae: 2.4771\n",
            "Epoch 414/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 6.7433 - mae: 1.9075 - val_loss: 11.2093 - val_mae: 2.5549\n",
            "Epoch 415/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.7228 - mae: 1.8127 - val_loss: 10.9011 - val_mae: 2.5874\n",
            "Epoch 416/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.8483 - mae: 1.8210 - val_loss: 9.0822 - val_mae: 2.4154\n",
            "Epoch 417/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.5989 - mae: 1.7950 - val_loss: 16.9210 - val_mae: 3.3257\n",
            "Epoch 418/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.3943 - mae: 1.8688 - val_loss: 10.3396 - val_mae: 2.5490\n",
            "Epoch 419/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.1946 - mae: 1.7204 - val_loss: 16.7621 - val_mae: 3.1653\n",
            "Epoch 420/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.2495 - mae: 1.8633 - val_loss: 11.7623 - val_mae: 2.6125\n",
            "Epoch 421/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.5271 - mae: 1.7603 - val_loss: 9.4696 - val_mae: 2.3690\n",
            "Epoch 422/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.2468 - mae: 1.6948 - val_loss: 9.4280 - val_mae: 2.4489\n",
            "Epoch 423/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.6109 - mae: 1.7711 - val_loss: 12.7640 - val_mae: 2.7864\n",
            "Epoch 424/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.3470 - mae: 1.7283 - val_loss: 11.0717 - val_mae: 2.5206\n",
            "Epoch 425/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.4704 - mae: 1.9106 - val_loss: 11.1063 - val_mae: 2.5432\n",
            "Epoch 426/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.0158 - mae: 1.7045 - val_loss: 10.5278 - val_mae: 2.4657\n",
            "Epoch 427/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.8827 - mae: 1.6329 - val_loss: 22.3253 - val_mae: 3.8177\n",
            "Epoch 428/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.0061 - mae: 1.9370 - val_loss: 9.7939 - val_mae: 2.3916\n",
            "Epoch 429/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.0382 - mae: 1.6486 - val_loss: 11.5899 - val_mae: 2.4925\n",
            "Epoch 430/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.7373 - mae: 2.0062 - val_loss: 11.3473 - val_mae: 2.5664\n",
            "Epoch 431/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.0853 - mae: 1.8325 - val_loss: 10.9595 - val_mae: 2.5419\n",
            "Epoch 432/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.9047 - mae: 1.7774 - val_loss: 10.9033 - val_mae: 2.4751\n",
            "Epoch 433/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.2255 - mae: 1.7297 - val_loss: 13.4126 - val_mae: 2.8446\n",
            "Epoch 434/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.1626 - mae: 1.7502 - val_loss: 12.3105 - val_mae: 2.7029\n",
            "Epoch 435/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.6511 - mae: 1.5890 - val_loss: 8.7523 - val_mae: 2.3342\n",
            "Epoch 436/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.6633 - mae: 1.7142 - val_loss: 12.7837 - val_mae: 2.8147\n",
            "Epoch 437/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.1789 - mae: 1.7103 - val_loss: 9.6391 - val_mae: 2.4168\n",
            "Epoch 438/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.4315 - mae: 1.7975 - val_loss: 10.6477 - val_mae: 2.5312\n",
            "Epoch 439/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.1300 - mae: 1.6973 - val_loss: 16.9823 - val_mae: 3.1999\n",
            "Epoch 440/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.1920 - mae: 1.7260 - val_loss: 11.3521 - val_mae: 2.5532\n",
            "Epoch 441/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.1505 - mae: 1.7816 - val_loss: 11.1611 - val_mae: 2.5398\n",
            "Epoch 442/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.4261 - mae: 1.7077 - val_loss: 9.6421 - val_mae: 2.4448\n",
            "Epoch 443/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.6186 - mae: 1.8140 - val_loss: 9.8115 - val_mae: 2.4195\n",
            "Epoch 444/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.7590 - mae: 1.8115 - val_loss: 10.0023 - val_mae: 2.4230\n",
            "Epoch 445/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.3431 - mae: 1.8583 - val_loss: 10.0971 - val_mae: 2.4975\n",
            "Epoch 446/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 4.9834 - mae: 1.7243 - val_loss: 12.2270 - val_mae: 2.7557\n",
            "Epoch 447/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.4968 - mae: 1.7807 - val_loss: 18.4075 - val_mae: 3.3877\n",
            "Epoch 448/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.0062 - mae: 1.9632 - val_loss: 8.5412 - val_mae: 2.2839\n",
            "Epoch 449/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.9974 - mae: 1.6210 - val_loss: 11.7145 - val_mae: 2.6625\n",
            "Epoch 450/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.3303 - mae: 1.8333 - val_loss: 10.1517 - val_mae: 2.3913\n",
            "Epoch 451/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.0378 - mae: 1.6695 - val_loss: 15.1408 - val_mae: 3.1000\n",
            "Epoch 452/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.7078 - mae: 1.7915 - val_loss: 9.0581 - val_mae: 2.3144\n",
            "Epoch 453/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.5523 - mae: 1.7242 - val_loss: 10.1222 - val_mae: 2.4259\n",
            "Epoch 454/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.5756 - mae: 1.7146 - val_loss: 15.4033 - val_mae: 3.1219\n",
            "Epoch 455/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 4.7235 - mae: 1.6144 - val_loss: 9.1936 - val_mae: 2.3353\n",
            "Epoch 456/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.0026 - mae: 1.8585 - val_loss: 9.1712 - val_mae: 2.3127\n",
            "Epoch 457/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.0254 - mae: 1.6400 - val_loss: 12.4375 - val_mae: 2.7449\n",
            "Epoch 458/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.1364 - mae: 1.7862 - val_loss: 13.3791 - val_mae: 2.8473\n",
            "Epoch 459/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.4976 - mae: 1.8418 - val_loss: 14.8221 - val_mae: 3.0171\n",
            "Epoch 460/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.8862 - mae: 1.7952 - val_loss: 9.6623 - val_mae: 2.4774\n",
            "Epoch 461/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.7709 - mae: 1.6764 - val_loss: 11.0852 - val_mae: 2.5129\n",
            "Epoch 462/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.4173 - mae: 1.6959 - val_loss: 10.1532 - val_mae: 2.4627\n",
            "Epoch 463/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.3701 - mae: 1.8397 - val_loss: 13.2943 - val_mae: 2.8404\n",
            "Epoch 464/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.9931 - mae: 1.7439 - val_loss: 11.7654 - val_mae: 2.6654\n",
            "Epoch 465/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.5903 - mae: 1.7635 - val_loss: 10.4104 - val_mae: 2.4076\n",
            "Epoch 466/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 6.0754 - mae: 1.8122 - val_loss: 8.9764 - val_mae: 2.3966\n",
            "Epoch 467/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.9762 - mae: 1.6206 - val_loss: 9.7729 - val_mae: 2.4014\n",
            "Epoch 468/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.8977 - mae: 1.7041 - val_loss: 8.9975 - val_mae: 2.3176\n",
            "Epoch 469/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.6614 - mae: 1.7953 - val_loss: 14.5386 - val_mae: 2.9990\n",
            "Epoch 470/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.9919 - mae: 1.8335 - val_loss: 9.5407 - val_mae: 2.4319\n",
            "Epoch 471/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.5581 - mae: 1.7407 - val_loss: 13.3281 - val_mae: 2.8031\n",
            "Epoch 472/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.5196 - mae: 1.6174 - val_loss: 10.7969 - val_mae: 2.5796\n",
            "Epoch 473/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.8015 - mae: 1.7092 - val_loss: 10.1215 - val_mae: 2.4462\n",
            "Epoch 474/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.2926 - mae: 1.7097 - val_loss: 12.0632 - val_mae: 2.6092\n",
            "Epoch 475/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 6.0663 - mae: 1.8388 - val_loss: 11.7234 - val_mae: 2.6646\n",
            "Epoch 476/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.8525 - mae: 1.6376 - val_loss: 10.4609 - val_mae: 2.3728\n",
            "Epoch 477/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.7390 - mae: 1.8234 - val_loss: 9.5224 - val_mae: 2.3619\n",
            "Epoch 478/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.3332 - mae: 1.7143 - val_loss: 10.1423 - val_mae: 2.3854\n",
            "Epoch 479/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.3974 - mae: 1.7573 - val_loss: 13.2116 - val_mae: 2.9045\n",
            "Epoch 480/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.7613 - mae: 1.7156 - val_loss: 10.5313 - val_mae: 2.4112\n",
            "Epoch 481/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.1768 - mae: 1.8244 - val_loss: 8.8627 - val_mae: 2.3900\n",
            "Epoch 482/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.6145 - mae: 1.5331 - val_loss: 11.1242 - val_mae: 2.6384\n",
            "Epoch 483/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.9913 - mae: 1.6407 - val_loss: 8.7477 - val_mae: 2.2742\n",
            "Epoch 484/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.7618 - mae: 1.7441 - val_loss: 10.4377 - val_mae: 2.4622\n",
            "Epoch 485/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.5651 - mae: 1.6410 - val_loss: 9.9509 - val_mae: 2.4740\n",
            "Epoch 486/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.4602 - mae: 1.7122 - val_loss: 9.5733 - val_mae: 2.3238\n",
            "Epoch 487/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.9533 - mae: 1.7409 - val_loss: 10.8049 - val_mae: 2.5144\n",
            "Epoch 488/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.9953 - mae: 1.8143 - val_loss: 10.4820 - val_mae: 2.5146\n",
            "Epoch 489/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.6538 - mae: 1.7310 - val_loss: 14.7118 - val_mae: 3.0236\n",
            "Epoch 490/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.7183 - mae: 1.8766 - val_loss: 9.8384 - val_mae: 2.4435\n",
            "Epoch 491/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.5763 - mae: 1.6388 - val_loss: 9.6639 - val_mae: 2.3428\n",
            "Epoch 492/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.7818 - mae: 1.7437 - val_loss: 18.5442 - val_mae: 3.4587\n",
            "Epoch 493/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.1382 - mae: 1.6728 - val_loss: 9.1605 - val_mae: 2.4315\n",
            "Epoch 494/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.8494 - mae: 1.6504 - val_loss: 16.7499 - val_mae: 3.2971\n",
            "Epoch 495/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.5673 - mae: 1.7882 - val_loss: 13.8086 - val_mae: 2.9089\n",
            "Epoch 496/500\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 5.3857 - mae: 1.7273 - val_loss: 9.7398 - val_mae: 2.4360\n",
            "Epoch 497/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.9482 - mae: 1.6649 - val_loss: 16.2179 - val_mae: 3.1405\n",
            "Epoch 498/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.3782 - mae: 1.7209 - val_loss: 14.6826 - val_mae: 3.0174\n",
            "Epoch 499/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.8454 - mae: 1.7768 - val_loss: 10.5010 - val_mae: 2.5272\n",
            "Epoch 500/500\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.9867 - mae: 1.8061 - val_loss: 9.7117 - val_mae: 2.4086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfSSQAsHUMyU"
      },
      "source": [
        "Visualize the model's training progress using the stats stored in the `history` object. We want to use this data to determine how long to train *before* the model stops making progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Ankh1GqBUSW7",
        "outputId": "9f29c2db-aeb2-4aff-976b-135f549ae6cb"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGgdxha-UcEJ"
      },
      "source": [
        "The graph shows the average error **in Validation set** is about $2,400 dollars. Is this good? \n",
        "\n",
        "Well, \\$2,400 is not an insignificant amount when some of the labels are only $15,000.\n",
        "\n",
        "Let's see how did the model performs on the **Test set**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bD8boyEO-Y0",
        "outputId": "fa3d4221-62e3-478c-cb87-944f4c63413e"
      },
      "source": [
        "[loss, mae] = model_conv1D.evaluate(test_data_reshaped, test_labels, verbose=0)\n",
        "print(\"Testing set Mean Abs Error: ${:7.2f}\".format(mae * 1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing set Mean Abs Error: $2684.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR5-DaUvUuMC"
      },
      "source": [
        "## Predict\n",
        "\n",
        "Finally, predict some housing prices using data in the testing set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "qC9sHFzIedd4",
        "outputId": "427957df-c16e-4607-a7c8-f70314fc3d50"
      },
      "source": [
        "test_predictions = model_conv1D.predict(test_data_reshaped).flatten()\n",
        "plot_prediction(test_labels, test_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5QUx49xG7J6"
      },
      "source": [
        "## Observations\n",
        "\n",
        "So far, we implemented an ***MLP model*** and a ***Conv1D model*** to handle the \"**Boston House Prices**\" regression problem.\n",
        "\n",
        "MLP model generates Mean Absolute Error:\n",
        "* in Validation is around **\\$2,600** whereas in Testing, it is about **$2900**\n",
        "\n",
        "Conv1D model generates Mean Absolute Error:\n",
        "* in Validation is around **\\$2,400** whereas in Testing, it is about **$2,700**\n",
        "\n",
        "Thus, **Conv1D** model is a **competitive** approach considering **MLP** model in the regression problem at hand. \n",
        "\n",
        "To use a Conv1D model, you need to **reshape** the input as \n",
        "\n",
        "`[batch_size, time_steps, input_dimension]`\n",
        "\n",
        "**I hope this tutorial helps you to use Conv1D layer successfuly!**\n",
        "\n",
        "\n"
      ]
    }
  ]
}