{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WishOrCurse_SimpleEmbedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYq1HdNEMPEtgrVt3rCyBr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmkarakaya/ML_tutorials/blob/master/WishOrCurse_SimpleEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8aaMqHPg6av",
        "colab_type": "text"
      },
      "source": [
        "# What is Word Embeding?\n",
        "* **Representing text as numbers:** Machine learning models take vectors (arrays of numbers) as input. When working with text, the first thing we must do come up with a strategy to convert strings to numbers (or to \"vectorize\" the text) before feeding it to the model. \n",
        "\n",
        "In general, there are 3 strategies for doing so:\n",
        "* **One-hot encodings**:\n",
        "\n",
        "![alt text](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/one-hot.png?raw=1)\n",
        "\n",
        "* **Encode each word with a unique number**\n",
        "\n",
        "the -> 0\n",
        "cat -> 1\n",
        "mat -> 2\n",
        "on  -> 3 \n",
        "\n",
        "* **Word embeddings**: Dense Vector Representation using floating point values which are trainable parameters.\n",
        "\n",
        "![alt text](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding2.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iubKQfQ6g8Qv",
        "colab_type": "text"
      },
      "source": [
        "# References:\n",
        "\n",
        "* [TF word embedding tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings)\n",
        "* [Word Embedding Example](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
        "\n",
        "* [Tokenazation](#https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ3UJRSO5by7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SpatialDropout1D, Dropout, Convolution1D\n",
        "from tensorflow.keras.layers import Flatten,  LSTM, GlobalMaxPooling1D\n",
        "\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTtqs6eniHpU",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vcqXPLSagnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEggZmKvMk4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadFile(url):\n",
        "  stms =[]\n",
        "  file = urllib.request.urlopen(url)\n",
        "\n",
        "  for line in file:\n",
        "    line = line.decode(\"utf-8\")\n",
        "    if(len(line)>2):\n",
        "          stm =  line.strip()\n",
        "          #print(stm)\n",
        "          stms.append(stm)\n",
        "  return stms\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLEm3dpfNnVR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "23036864-51d5-4146-fb77-30ddf8cf95e5"
      },
      "source": [
        "urlWish = 'https://raw.githubusercontent.com/kmkarakaya/ML_tutorials/master/data/dua.txt'\n",
        "urlCurse= 'https://raw.githubusercontent.com/kmkarakaya/ML_tutorials/master/data/beddua.txt'\n",
        "\n",
        "wish = loadFile(urlWish)\n",
        "curse = loadFile(urlCurse)\n",
        "\n",
        "totalWish=len(wish)\n",
        "print('totalWish: ',totalWish) \n",
        "totalCurse = len(curse)\n",
        "print('totalCurse: ',totalCurse)         "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "totalWish:  177\n",
            "totalCurse:  801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DjpZmfDHzAC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9cc2611d-7036-44ea-e2ab-28574b80500b"
      },
      "source": [
        "curse= curse[:totalWish]\n",
        "totalCurse = len(curse)\n",
        "print('totalCurse: ',totalCurse) "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "totalCurse:  177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeujmU4ENm9L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "508d34f2-bc81-411c-deb3-c8797fb42a5c"
      },
      "source": [
        "testWish= int(totalWish* 0.1)\n",
        "testCurse = int(totalCurse * 0.1)\n",
        "print('testWish ', testWish)\n",
        "print('testCurse ', testCurse)\n",
        "\n",
        "trainDocs= wish[:-testWish]+curse[:-testCurse]\n",
        "testDocs= wish[-testWish:]+curse[-testCurse:]\n",
        "print(len(trainDocs)) \n",
        "print(len(testDocs)) \n",
        "\n",
        "trainLabels = np.concatenate((np.ones(totalWish-testWish),np.zeros(totalCurse-testCurse)), axis=0) \n",
        "testLabels = np.concatenate((np.ones(testWish),np.zeros(testCurse)), axis=0) \n",
        "\n",
        "print(len(trainLabels)) \n",
        "print(len(testLabels))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testWish  17\n",
            "testCurse  17\n",
            "320\n",
            "34\n",
            "320\n",
            "34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hiUf_D67uPt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a281f521-6389-43c2-bce3-2b39b4702328"
      },
      "source": [
        "allDocs= trainDocs + testDocs\n",
        "print(allDocs)\n",
        "print(len(allDocs))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Acı yüzü görmeyesin.', 'Allah kimseyi aç açık bırakmasın.', 'Allah’ım beni affet.', 'Allah’ım affet.', 'Afiyet  şeker olsun.', 'Ağzını hayra aç.', 'Allah ayrılık vermesin.', 'Allah dert vermesin.', 'Allah acı vermesin.', 'Babanın canına rahmet.', 'Annenin canına rahme', 'Bahtın açık olsun.', 'Yolun açık olsun.', 'Şansın açık olsun.', 'Başına devlet kuşu kona.', 'Başına devlet kuşu konsun', 'Bereketi Allah’tan olsun.', 'Beytullaha yüz süresin.', 'Bolluğun başından aşa.', 'Ciğer acısı görmeyesin.', 'Çıran her daim yakılı kalsın.', 'Çift  çubuk sahibi olasın.', 'Dal  budak salasın.', 'Damatlığını da görürüz inşallah.', 'Darlık yüzü görmeyesin.', 'Yokluk yüzü görmeyesin.', 'Ekenin doğuranın eksik olmasın.', 'Ermişlerden olasın.', 'Evladınla binbir yaşa.', 'Ahrette Fatma anamıza komşu olasın.', 'Geçmiş olsun.', 'Gurbet yüzü görmeyesin.', 'Hatır soranların çok olsun.', 'El öpenlerin çok olsun', 'Hayırlı  uğurlu olsun.', 'Hızır yoldaşın olsun.', 'İki cihanda aziz ol.', 'İyi yolculuklar.', 'Hayırlı yolculuklar', 'Kesenin dibini görmeyesin.', 'kutlu olsun.', 'mutlu ol', 'Kış kışlığını bilsin, kuş kuşluğunu.', 'Muhabbetiniz daim ola.', 'Elden ayaktan düşme', 'Muhabbetiniz daim olsun.', 'Muhanete muhtaç olmayasın.', 'Allah namerde muhtaç etmesin', 'Nasibin kısmetin bol ola.', 'Nasibin kısmetin bol olsun', 'kısmetin bol olsun', 'Nasibin bol olsun', 'Ocağın küllensin, bahçen güllensin.', 'elini atasın altın bulasın', 'Ömrün uzun kısmetin gür olsun.', 'Su gibi aziz ol', 'Resûl aleyhisselam seni kayıra.', 'Hayırlı işler', 'Hayırlı günler', 'Siftahı sizden bereketi Allahtan.', 'Siftahı senden bereketi Allahtan.', 'Şansın açık olsun.', 'Talih kuşu başına kona.', 'Yolun açık olsun', 'Uğurlar olsun.', 'Güle güle', 'Üreyesin,', 'Türeyesin.', 'Var ol.', 'Yokluk yüzü görmeyesin.', 'Yokluk ekmeği yemeyesin.', 'Zürriyetin bol ola.', 'Allah gazadan, beladan esirgesin.', 'Allah kazadan, beladan esirgesin.', 'Allah, Halil İbrahim bereketi versin.', 'Halil İbrahim bereketi versin.', 'Allah, analı babalı büyütsün.', 'analı babalı büyütsün', 'Bereketi bol olsun', 'Bereketli olsun', 'Allah korusun.', 'Allah yardımcın olsun', 'Allah akran şerrinden; avrat şerrinden esirgesin.', 'Allah emeğini yağlı etsin.', 'Teni toprak yüzü görmesin.', 'Allah ne muradı varsa versin.', 'Allah tuttuğunu altın etsin.', 'Allah iki cihan saadeti versin.', 'Allah ne muradın varsa versin.', 'Allah imanla göçürsün.', 'Allah uzun ömürler versin', 'nur içinde yatsın.', 'Anan atan nur içinde yatsın.', 'Avuçladığın toprak altı olsun.', 'İşin gücün rast gelsin.', 'Tuttuğun altın olsun.', 'Huri kızlarıyla yoldaş olasın.', 'Tırnağına taş değmesin.', 'Yattığın yer nur olsun.', 'Geçmişlerinin canına değsin.', 'Ellerin dert görmesin.', 'Su verenlerin çok olsun.', 'Allah dert verip derman aratmasın', 'Başın dişin ağrımasın.', 'Anana babana rahmet olsun', 'Su gibi aziz ol.', 'Allah rahmet eylesin', 'Allah kalbine göre versin.', 'Allah işini gücünü rast getirsin.', 'Allah kazadan beladan korusun.', 'Allah gönlüne göre versin', 'Allah esirgesin', 'Allah düşman başına vermesin.', 'Allah ne muradın varsa versin.', 'Allah hayırlı kapılar versin.', 'Allah senden razı olsun.', 'Allah tuttuğunu altı etsin.', 'Allah senden razı olsun.', 'tuttuğun altın olsun', 'Allah düşürmesin.', 'Allah analı babalı büyütsün.', 'Allah kesene bereket versin.', 'Allah gönlüne göre versin', 'Ayağına taş değmesin.', 'El öpenlerin çok ola.', 'Geçmişinin canına değsin.', 'Allah anasına babasına bağışlasın', 'Allah sevdiklerine bağışlasın', 'Elin ayağın dert görmesin.', 'Ah vah demiyesin.', 'Allah evinize Halil İbrahim bereketi vere.', 'Allah evinize Halil İbrahim bereketi versin.', 'Allah iman ile götürsün.', 'Allah iman versin', 'Allah kolundan tutsun.', 'Allah sana bir yerine beş versin.', 'Allah sana hacc nasip ede.', 'Allah sana hacc nasip etsin.', 'Allah seni anana babana bağışlaya.', 'Allah seni anana babana bağışlasın.', 'Allah bağışlasın', 'Allah seni görünür görünmez kazadan beladan  saklasın.', 'Allah görünmezinden versin', 'Allah kazadan beladan  korusun.', 'Allah seni kazadan beladan  korusun.', 'Allah seni peygamber efendimize komşu ede.', 'Allah seni peygamber efendimize komşu etsin', 'Allah su gibi muradını vere.', 'Allah su gibi muradını versin', 'Ayağın taşa değmiye.', 'Ayağına taş değmesin.', 'Bir elin balda bir elin yağda olsun.', 'Boş keseye elini sokmayasın.', 'Cennetlik kul olasın.', 'Cennetlik ol', 'Mekanı cennet olsun', 'Dünya durdukça durasın.', 'Elin gözün dert görmiye.', 'Elin gözün dert görmesin', 'Gökten yağa, yerden toplayasın.', 'Adın bata', 'Adı batasıca.', 'Ağzına sapan taşı deye.', 'Ağzından od çıksın.', 'Ağzın dilin kurusun.', 'Ah diyesin kan tüküresin.', 'Anan baban başın ucunda meleye.', 'Ayağı çolak başı kabak olasın.', 'Azrailin demir pençesine gelesin.', 'Azrailin demir pençesine rast gelesin.', 'Bağrına taş basasın.', 'Bahtın karara.', 'Balaların ardında meleye.', 'Başına bit düşsün.', 'Başının derisinden davul edeler.', 'Bayramlar pazarına çıkamayasın.', 'Boğazın kurusun da  bir yudum su verenin olmasın.', 'Canın çıksın.', 'Canı çıksın.', 'Ciğerine nüzul inesice', 'ciğerine ağrı insin', 'Ecdadının kabri eşek ahırı ola.', 'Ekmeğin  aşın olsun da yiyecek halin olmasın.', 'Ellerin ardına gele.', 'Elin ekmek belin kuşak görmesin.', 'Evinde baykuşlar ötsün.', 'Evlatlarının hayrını görmeyesin.', 'Fidan iken devrilesin.', 'Geri yanın başına gele.', 'Gidişin ola  dönüşün olmaya.', 'Gidişin ola da dönüşün olmaya.', 'mezarında dik oturasın.', 'Gözünü toprak doyursun.', 'fiyakan batsın inşallah.', 'fiyakan batsın', 'adın batsın inşallah.', 'adın batsın', 'Hayrını görmeyesin.', 'birdenbire öl inşallah.', 'Huyun  suyun batsın.', 'Huyun  suyun kurusun.', 'Boynu devrilesice', 'Yere batasıca', 'Yemeden gidesice', 'Tepesi üstüne gelesice', 'Gözüne, dizine dursun', 'Sütüm haram olsun', 'Aldığı, kıldığı kabul olmasın', 'Baş diş sersemliğine uğrayasıca...', 'Guruya galasıca', 'Elleri, ayakları tutmayasıca', 'Adı bilinmedik derde düşesin.', 'Ağaç gibi kuruyasın.', 'Ağzın burnun kitlene.', 'Ak göğsüne mor çıbanlar dizile.', 'Avuç avuç kan kusasın.', 'Ayağın kırılsın.', 'Başına taş düşe.', 'Bedenine şiş çakıla.', 'Ciğerinin başına kurtlar düşe.', 'Ciğerlerin doğransın.', 'Dilin tutulsun.', 'Derdine derman bulunmaya.', 'Dizin dizin sürünesin.', 'Döşekte uzanıp ölemeyesin.', 'Elin kolun tutmasın.', 'Ellerin kırılsın.', 'Etlerin döküle.', 'Etlerin dökülsün.', 'Gözün kör olsun.', 'Gözlerin kör olsun.', 'Gözlerin oyula.', 'Gözlerin oyulsun', 'Kan tüküresin.', 'Kocamadan belin iki bükülsün.', 'Kolların dibinden düşe.', 'Kulağına kurşun aksın.', 'Lal olasın, dillerin söylemeye.', 'Otuz iki dişin birden dökülsün', 'Sidikliğine taş dursun.', 'Siyah saçın kızıl kana belene.', 'Tırnakların söküle.', 'Yaran fazla olsun, merhem yetmesin.', 'Yata yata yan etlerin çürüsün.', 'Yedi yıl sıtma tutasın.', 'Yılan soksun, gövden şişsin.', 'Anan öle', 'Hasretin var ise geri gelmeye.', 'Karısı ellere kalsın.', 'Kavim kardaş senden nefret etsin', 'Aldıklarının hayrını görmeye.', 'Bir karaçula kalasın.', 'Emzirdiğim süt burnundan gelsin.', 'Evin başına yıkılsın.', 'Malın ellere kalsın.', 'Naneye muhtaç olasın.', 'Ocağın batsın', 'Ahın göklere çıka.', 'Ekmek atlı olsun, sen yaya.', 'İki yakan bir araya gelmesin.', 'Kapı kapı dilenesin.', 'Karnın ekmek görmeye.', 'Mahpushanelerde çürüyesin.', 'Muradın yarıda kalsın.', 'Ömrün yas ile geçsin.', 'Rezil kepaze olasın.', 'Sokak sokak sürünsün.', 'Doğurduğun kız, doğradığın bez olsun.', 'Dul ere duvaksız, kör kişiye nikahsız gidesin.', 'Evlat yüzü görmeyesin.', 'Evlenmeye yar bulamasın.', 'Yavrunu bağrına bastığın zaman, kucağında taş göresin.', 'Allah bin türlü belanı versin.', \"Allah'ın gazabına gelesin.\", 'Allah tez günde belasını versin.', 'Aşından akrep çıksın, görmeyip yutasın.', 'Bıçaklara gelesin.', 'Dar sokaklarda bol bıçaklara gelesin.', 'Kurşunlara gelesin.', 'Tren altında doğranasın.', 'Azrail tez günde yanına gele', 'Boynu iplere gelsin.', 'Cefa ile can veresin.', 'Çenen bağlansın.', 'Damarın çekilsin, kanın kurusun.', 'Dilin salavata dönmesin.', 'Gözüne toprak dola.', 'İmansız ölsün.', 'Ölüsüne kefen bulunmasın.', 'Ölüsünü itler yesin.', 'Ömrün tükene.', 'Sıcak yatasın soğuk kalkasın.', 'Sırtı teneşire gelesice.', 'Yanarak ölesin.', \"Allah'ın ateşine gelesin.\", 'Azabın ateşle verilsin', 'Cehennemde cayır cayır yanasın.', 'Hak divanında yüzün kara olsun.', 'Huzur ı mahşerde yüzün gülmeye.', 'Kabirde başına topuz vuralar.', 'Kıyamete kadar azapta kalsın.', 'Sualini veremesin.', \"Zebaniler seni Niran'a sürsün.\", 'Zebaniler topuz vursun.', 'Adın bata.', 'Adın ortadan kalka.', 'Adın sanın kara gele.', 'Adın sanın kurusun.', 'Bayram günü kapını açan olmasın', 'Duvağı kara gide.', 'Duvağını kara topraklar öpe.', 'Karalı bayramın gelsin.', 'Minderine oturan bulunmaya.', 'Gözün kör olsun.', 'Altın üstüne gelsin.', 'Göçmüş duvar üstüne yıkılsın.', 'Kuru derelerde boğulasın.', 'Ölü kargalar gözünü oysun.', 'Acıdan geberesin.', 'Acıdan kıvranasın.', 'Hakkım helal olsun.', 'helal olsun', 'Hayırlara sahip olasın.', 'Kadalarını alayım.', 'Nereye girsen yıldızın şirin ola.', 'Oğul karı yemek nasip olsun.', 'Ömrün uzun ola', 'Ömrün uzun olsun', 'uzun ömürler', 'düğünün güzün ola.', 'Allah sana yağdıra.', 'Allah versin', 'Allah gönlüne göre versin', 'Tırnağın taşa dokunmaya.', 'Tırnağın taşa dokunmasın.', 'Yok gün görmeyesin.', 'Yokluk görmeyesin', 'Acılara tuş olasın.', 'Adın başkasına kona.', 'Adın bata.', 'Adın batsın inşallah.', 'Adın kala, sen gidesin.', 'Adın kara yerden gele.', 'Adın ortadan kalksın.', 'Adın sallar altından gele.', 'Altın adın pul ola.', 'Adın sanın batsın.', 'Adın sanın bellisiz olsun.', 'Adın sanın kara gele.', 'Adın sanın kurusun.', 'Ağzı yumulu kalasın.', 'Ağzın kanla dola.', 'Ağzın kapansın.', 'Ağzın kilitlensin.']\n",
            "354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm4hXADQiNJh",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Tokenize the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvUCz0AL9HRb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "6bda0f72-4b2b-4c4d-d090-ed1359ac7f4a"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# Tokenize our training data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(allDocs)\n",
        "\n",
        "document_count = tokenizer.document_count\n",
        "vocab_size = len(tokenizer.word_index)\n",
        "\n",
        "# Encode training data sentences into sequences\n",
        "allDocs_sequences = tokenizer.texts_to_sequences(allDocs)\n",
        "\n",
        "# Get max training sequence length\n",
        "max_length = max([len(x) for x in allDocs_sequences])\n",
        "\n",
        "# Get our training data word index\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Corpus Summary\")\n",
        "print(\"Word index:\", word_index)\n",
        "print(\"document count  :\", document_count)\n",
        "print(\"vocabulary size :\", vocab_size)\n",
        "print(\"Maximum length of the statements :\", max_length)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus Summary\n",
            "Word index: {'allah': 1, 'olsun': 2, 'versin': 3, 'adın': 4, 'görmeyesin': 5, 'olasın': 6, 'ola': 7, 'başına': 8, 'bereketi': 9, 'seni': 10, 'batsın': 11, 'yüzü': 12, 'bol': 13, 'taş': 14, 'elin': 15, 'gelesin': 16, 'gele': 17, 'açık': 18, 'dert': 19, 'ol': 20, 'altın': 21, 'su': 22, 'beladan': 23, 'etsin': 24, 'bir': 25, 'kurusun': 26, 'kara': 27, 'sanın': 28, 'kalsın': 29, 'inşallah': 30, 'olmasın': 31, 'hayırlı': 32, 'ömrün': 33, 'uzun': 34, 'gibi': 35, 'kazadan': 36, 'görmesin': 37, 'gelsin': 38, 'ağzın': 39, 'vermesin': 40, 'canına': 41, 'da': 42, 'yokluk': 43, 'çok': 44, 'kısmetin': 45, 'senden': 46, 'esirgesin': 47, 'halil': 48, 'i̇brahim': 49, 'korusun': 50, 'toprak': 51, 'göre': 52, 'bağışlasın': 53, 'sana': 54, 'gözün': 55, 'çıksın': 56, 'kör': 57, 'rahmet': 58, 'kuşu': 59, 'kona': 60, 'daim': 61, 'komşu': 62, 'aziz': 63, 'muhtaç': 64, 'nasibin': 65, 'analı': 66, 'babalı': 67, 'büyütsün': 68, 'ne': 69, 'varsa': 70, 'iki': 71, 'muradın': 72, 'nur': 73, 'anan': 74, 'rast': 75, 'değmesin': 76, 'ellerin': 77, 'anana': 78, 'babana': 79, 'gönlüne': 80, 'ayağın': 81, 'ile': 82, 'nasip': 83, 'taşa': 84, 'bata': 85, 'dilin': 86, 'kan': 87, 'ekmek': 88, 'hayrını': 89, 'üstüne': 90, 'düşe': 91, 'etlerin': 92, 'gözlerin': 93, 'acı': 94, 'aç': 95, 'allah’ım': 96, 'affet': 97, 'bahtın': 98, 'yolun': 99, 'şansın': 100, 'devlet': 101, 'el': 102, 'öpenlerin': 103, 'i̇ki': 104, 'yolculuklar': 105, 'muhabbetiniz': 106, 'ocağın': 107, 'elini': 108, 'siftahı': 109, 'allahtan': 110, 'güle': 111, 'var': 112, 'şerrinden': 113, 'tuttuğunu': 114, 'ömürler': 115, 'içinde': 116, 'yatsın': 117, 'altı': 118, 'tuttuğun': 119, 'değsin': 120, 'derman': 121, 'başın': 122, 'dişin': 123, 'razı': 124, 'ayağına': 125, 'ah': 126, 'evinize': 127, 'vere': 128, 'iman': 129, 'hacc': 130, 'ede': 131, 'peygamber': 132, 'efendimize': 133, 'muradını': 134, 'cennetlik': 135, 'yerden': 136, 'adı': 137, 'batasıca': 138, 'tüküresin': 139, 'meleye': 140, 'azrailin': 141, 'demir': 142, 'pençesine': 143, 'bağrına': 144, 'ciğerine': 145, 'belin': 146, 'geri': 147, 'gidişin': 148, 'dönüşün': 149, 'olmaya': 150, 'gözünü': 151, 'fiyakan': 152, 'huyun': 153, 'suyun': 154, 'boynu': 155, 'gelesice': 156, 'gözüne': 157, 'dursun': 158, 'avuç': 159, 'kırılsın': 160, 'bulunmaya': 161, 'dizin': 162, 'dökülsün': 163, 'yata': 164, 'ellere': 165, 'görmeye': 166, 'kalasın': 167, 'yıkılsın': 168, 'sen': 169, 'kapı': 170, 'sokak': 171, 'gidesin': 172, \"allah'ın\": 173, 'tez': 174, 'günde': 175, 'bıçaklara': 176, 'dola': 177, 'cayır': 178, 'yüzün': 179, 'topuz': 180, 'zebaniler': 181, 'ortadan': 182, 'acıdan': 183, 'helal': 184, 'tırnağın': 185, 'kimseyi': 186, 'bırakmasın': 187, 'beni': 188, 'afiyet': 189, 'şeker': 190, 'ağzını': 191, 'hayra': 192, 'ayrılık': 193, 'babanın': 194, 'annenin': 195, 'rahme': 196, 'konsun': 197, 'allah’tan': 198, 'beytullaha': 199, 'yüz': 200, 'süresin': 201, 'bolluğun': 202, 'başından': 203, 'aşa': 204, 'ciğer': 205, 'acısı': 206, 'çıran': 207, 'her': 208, 'yakılı': 209, 'çift': 210, 'çubuk': 211, 'sahibi': 212, 'dal': 213, 'budak': 214, 'salasın': 215, 'damatlığını': 216, 'görürüz': 217, 'darlık': 218, 'ekenin': 219, 'doğuranın': 220, 'eksik': 221, 'ermişlerden': 222, 'evladınla': 223, 'binbir': 224, 'yaşa': 225, 'ahrette': 226, 'fatma': 227, 'anamıza': 228, 'geçmiş': 229, 'gurbet': 230, 'hatır': 231, 'soranların': 232, 'uğurlu': 233, 'hızır': 234, 'yoldaşın': 235, 'cihanda': 236, 'i̇yi': 237, 'kesenin': 238, 'dibini': 239, 'kutlu': 240, 'mutlu': 241, 'kış': 242, 'kışlığını': 243, 'bilsin': 244, 'kuş': 245, 'kuşluğunu': 246, 'elden': 247, 'ayaktan': 248, 'düşme': 249, 'muhanete': 250, 'olmayasın': 251, 'namerde': 252, 'etmesin': 253, 'küllensin': 254, 'bahçen': 255, 'güllensin': 256, 'atasın': 257, 'bulasın': 258, 'gür': 259, 'resûl': 260, 'aleyhisselam': 261, 'kayıra': 262, 'işler': 263, 'günler': 264, 'sizden': 265, 'talih': 266, 'uğurlar': 267, 'üreyesin': 268, 'türeyesin': 269, 'ekmeği': 270, 'yemeyesin': 271, 'zürriyetin': 272, 'gazadan': 273, 'bereketli': 274, 'yardımcın': 275, 'akran': 276, 'avrat': 277, 'emeğini': 278, 'yağlı': 279, 'teni': 280, 'muradı': 281, 'cihan': 282, 'saadeti': 283, 'imanla': 284, 'göçürsün': 285, 'atan': 286, 'avuçladığın': 287, 'i̇şin': 288, 'gücün': 289, 'huri': 290, 'kızlarıyla': 291, 'yoldaş': 292, 'tırnağına': 293, 'yattığın': 294, 'yer': 295, 'geçmişlerinin': 296, 'verenlerin': 297, 'verip': 298, 'aratmasın': 299, 'ağrımasın': 300, 'eylesin': 301, 'kalbine': 302, 'işini': 303, 'gücünü': 304, 'getirsin': 305, 'düşman': 306, 'kapılar': 307, 'düşürmesin': 308, 'kesene': 309, 'bereket': 310, 'geçmişinin': 311, 'anasına': 312, 'babasına': 313, 'sevdiklerine': 314, 'vah': 315, 'demiyesin': 316, 'götürsün': 317, 'kolundan': 318, 'tutsun': 319, 'yerine': 320, 'beş': 321, 'bağışlaya': 322, 'görünür': 323, 'görünmez': 324, 'saklasın': 325, 'görünmezinden': 326, 'değmiye': 327, 'balda': 328, 'yağda': 329, 'boş': 330, 'keseye': 331, 'sokmayasın': 332, 'kul': 333, 'mekanı': 334, 'cennet': 335, 'dünya': 336, 'durdukça': 337, 'durasın': 338, 'görmiye': 339, 'gökten': 340, 'yağa': 341, 'toplayasın': 342, 'ağzına': 343, 'sapan': 344, 'taşı': 345, 'deye': 346, 'ağzından': 347, 'od': 348, 'diyesin': 349, 'baban': 350, 'ucunda': 351, 'ayağı': 352, 'çolak': 353, 'başı': 354, 'kabak': 355, 'basasın': 356, 'karara': 357, 'balaların': 358, 'ardında': 359, 'bit': 360, 'düşsün': 361, 'başının': 362, 'derisinden': 363, 'davul': 364, 'edeler': 365, 'bayramlar': 366, 'pazarına': 367, 'çıkamayasın': 368, 'boğazın': 369, 'yudum': 370, 'verenin': 371, 'canın': 372, 'canı': 373, 'nüzul': 374, 'inesice': 375, 'ağrı': 376, 'insin': 377, 'ecdadının': 378, 'kabri': 379, 'eşek': 380, 'ahırı': 381, 'ekmeğin': 382, 'aşın': 383, 'yiyecek': 384, 'halin': 385, 'ardına': 386, 'kuşak': 387, 'evinde': 388, 'baykuşlar': 389, 'ötsün': 390, 'evlatlarının': 391, 'fidan': 392, 'iken': 393, 'devrilesin': 394, 'yanın': 395, 'mezarında': 396, 'dik': 397, 'oturasın': 398, 'doyursun': 399, 'birdenbire': 400, 'öl': 401, 'devrilesice': 402, 'yere': 403, 'yemeden': 404, 'gidesice': 405, 'tepesi': 406, 'dizine': 407, 'sütüm': 408, 'haram': 409, 'aldığı': 410, 'kıldığı': 411, 'kabul': 412, 'baş': 413, 'diş': 414, 'sersemliğine': 415, 'uğrayasıca': 416, 'guruya': 417, 'galasıca': 418, 'elleri': 419, 'ayakları': 420, 'tutmayasıca': 421, 'bilinmedik': 422, 'derde': 423, 'düşesin': 424, 'ağaç': 425, 'kuruyasın': 426, 'burnun': 427, 'kitlene': 428, 'ak': 429, 'göğsüne': 430, 'mor': 431, 'çıbanlar': 432, 'dizile': 433, 'kusasın': 434, 'bedenine': 435, 'şiş': 436, 'çakıla': 437, 'ciğerinin': 438, 'kurtlar': 439, 'ciğerlerin': 440, 'doğransın': 441, 'tutulsun': 442, 'derdine': 443, 'sürünesin': 444, 'döşekte': 445, 'uzanıp': 446, 'ölemeyesin': 447, 'kolun': 448, 'tutmasın': 449, 'döküle': 450, 'oyula': 451, 'oyulsun': 452, 'kocamadan': 453, 'bükülsün': 454, 'kolların': 455, 'dibinden': 456, 'kulağına': 457, 'kurşun': 458, 'aksın': 459, 'lal': 460, 'dillerin': 461, 'söylemeye': 462, 'otuz': 463, 'birden': 464, 'sidikliğine': 465, 'siyah': 466, 'saçın': 467, 'kızıl': 468, 'kana': 469, 'belene': 470, 'tırnakların': 471, 'söküle': 472, 'yaran': 473, 'fazla': 474, 'merhem': 475, 'yetmesin': 476, 'yan': 477, 'çürüsün': 478, 'yedi': 479, 'yıl': 480, 'sıtma': 481, 'tutasın': 482, 'yılan': 483, 'soksun': 484, 'gövden': 485, 'şişsin': 486, 'öle': 487, 'hasretin': 488, 'ise': 489, 'gelmeye': 490, 'karısı': 491, 'kavim': 492, 'kardaş': 493, 'nefret': 494, 'aldıklarının': 495, 'karaçula': 496, 'emzirdiğim': 497, 'süt': 498, 'burnundan': 499, 'evin': 500, 'malın': 501, 'naneye': 502, 'ahın': 503, 'göklere': 504, 'çıka': 505, 'atlı': 506, 'yaya': 507, 'yakan': 508, 'araya': 509, 'gelmesin': 510, 'dilenesin': 511, 'karnın': 512, 'mahpushanelerde': 513, 'çürüyesin': 514, 'yarıda': 515, 'yas': 516, 'geçsin': 517, 'rezil': 518, 'kepaze': 519, 'sürünsün': 520, 'doğurduğun': 521, 'kız': 522, 'doğradığın': 523, 'bez': 524, 'dul': 525, 'ere': 526, 'duvaksız': 527, 'kişiye': 528, 'nikahsız': 529, 'evlat': 530, 'evlenmeye': 531, 'yar': 532, 'bulamasın': 533, 'yavrunu': 534, 'bastığın': 535, 'zaman': 536, 'kucağında': 537, 'göresin': 538, 'bin': 539, 'türlü': 540, 'belanı': 541, 'gazabına': 542, 'belasını': 543, 'aşından': 544, 'akrep': 545, 'görmeyip': 546, 'yutasın': 547, 'dar': 548, 'sokaklarda': 549, 'kurşunlara': 550, 'tren': 551, 'altında': 552, 'doğranasın': 553, 'azrail': 554, 'yanına': 555, 'iplere': 556, 'cefa': 557, 'can': 558, 'veresin': 559, 'çenen': 560, 'bağlansın': 561, 'damarın': 562, 'çekilsin': 563, 'kanın': 564, 'salavata': 565, 'dönmesin': 566, 'i̇mansız': 567, 'ölsün': 568, 'ölüsüne': 569, 'kefen': 570, 'bulunmasın': 571, 'ölüsünü': 572, 'itler': 573, 'yesin': 574, 'tükene': 575, 'sıcak': 576, 'yatasın': 577, 'soğuk': 578, 'kalkasın': 579, 'sırtı': 580, 'teneşire': 581, 'yanarak': 582, 'ölesin': 583, 'ateşine': 584, 'azabın': 585, 'ateşle': 586, 'verilsin': 587, 'cehennemde': 588, 'yanasın': 589, 'hak': 590, 'divanında': 591, 'huzur': 592, 'ı': 593, 'mahşerde': 594, 'gülmeye': 595, 'kabirde': 596, 'vuralar': 597, 'kıyamete': 598, 'kadar': 599, 'azapta': 600, 'sualini': 601, 'veremesin': 602, \"niran'a\": 603, 'sürsün': 604, 'vursun': 605, 'kalka': 606, 'bayram': 607, 'günü': 608, 'kapını': 609, 'açan': 610, 'duvağı': 611, 'gide': 612, 'duvağını': 613, 'topraklar': 614, 'öpe': 615, 'karalı': 616, 'bayramın': 617, 'minderine': 618, 'oturan': 619, 'göçmüş': 620, 'duvar': 621, 'kuru': 622, 'derelerde': 623, 'boğulasın': 624, 'ölü': 625, 'kargalar': 626, 'oysun': 627, 'geberesin': 628, 'kıvranasın': 629, 'hakkım': 630, 'hayırlara': 631, 'sahip': 632, 'kadalarını': 633, 'alayım': 634, 'nereye': 635, 'girsen': 636, 'yıldızın': 637, 'şirin': 638, 'oğul': 639, 'karı': 640, 'yemek': 641, 'düğünün': 642, 'güzün': 643, 'yağdıra': 644, 'dokunmaya': 645, 'dokunmasın': 646, 'yok': 647, 'gün': 648, 'acılara': 649, 'tuş': 650, 'başkasına': 651, 'kala': 652, 'kalksın': 653, 'sallar': 654, 'altından': 655, 'pul': 656, 'bellisiz': 657, 'ağzı': 658, 'yumulu': 659, 'kanla': 660, 'kapansın': 661, 'kilitlensin': 662}\n",
            "document count  : 354\n",
            "vocabulary size : 662\n",
            "Maximum length of the statements : 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbQmSz5CIMlo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "f7b349a2-adbd-4eb4-a4aa-440240cad26c"
      },
      "source": [
        "# Encode training data sentences into sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(trainDocs)\n",
        "\n",
        "# Pad the training sequences\n",
        "train_padded = pad_sequences(train_sequences, padding='post', truncating='post', maxlen=max_length)\n",
        "\n",
        "# Output the results of our work\n",
        "print(\"Train Doc Summary\")\n",
        "print(\"\\nTraining sequences:\\n\", train_sequences)\n",
        "print(\"\\nPadded training sequences:\\n\", train_padded[:5])\n",
        "print(\"\\nPadded training shape:\", train_padded.shape)\n",
        "print(\"Training sequences data type:\", type(train_sequences))\n",
        "print(\"Padded Training sequences data type:\", type(train_padded))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Doc Summary\n",
            "\n",
            "Training sequences:\n",
            " [[94, 12, 5], [1, 186, 95, 18, 187], [96, 188, 97], [96, 97], [189, 190, 2], [191, 192, 95], [1, 193, 40], [1, 19, 40], [1, 94, 40], [194, 41, 58], [195, 41, 196], [98, 18, 2], [99, 18, 2], [100, 18, 2], [8, 101, 59, 60], [8, 101, 59, 197], [9, 198, 2], [199, 200, 201], [202, 203, 204], [205, 206, 5], [207, 208, 61, 209, 29], [210, 211, 212, 6], [213, 214, 215], [216, 42, 217, 30], [218, 12, 5], [43, 12, 5], [219, 220, 221, 31], [222, 6], [223, 224, 225], [226, 227, 228, 62, 6], [229, 2], [230, 12, 5], [231, 232, 44, 2], [102, 103, 44, 2], [32, 233, 2], [234, 235, 2], [104, 236, 63, 20], [237, 105], [32, 105], [238, 239, 5], [240, 2], [241, 20], [242, 243, 244, 245, 246], [106, 61, 7], [247, 248, 249], [106, 61, 2], [250, 64, 251], [1, 252, 64, 253], [65, 45, 13, 7], [65, 45, 13, 2], [45, 13, 2], [65, 13, 2], [107, 254, 255, 256], [108, 257, 21, 258], [33, 34, 45, 259, 2], [22, 35, 63, 20], [260, 261, 10, 262], [32, 263], [32, 264], [109, 265, 9, 110], [109, 46, 9, 110], [100, 18, 2], [266, 59, 8, 60], [99, 18, 2], [267, 2], [111, 111], [268], [269], [112, 20], [43, 12, 5], [43, 270, 271], [272, 13, 7], [1, 273, 23, 47], [1, 36, 23, 47], [1, 48, 49, 9, 3], [48, 49, 9, 3], [1, 66, 67, 68], [66, 67, 68], [9, 13, 2], [274, 2], [1, 50], [1, 275, 2], [1, 276, 113, 277, 113, 47], [1, 278, 279, 24], [280, 51, 12, 37], [1, 69, 281, 70, 3], [1, 114, 21, 24], [1, 71, 282, 283, 3], [1, 69, 72, 70, 3], [1, 284, 285], [1, 34, 115, 3], [73, 116, 117], [74, 286, 73, 116, 117], [287, 51, 118, 2], [288, 289, 75, 38], [119, 21, 2], [290, 291, 292, 6], [293, 14, 76], [294, 295, 73, 2], [296, 41, 120], [77, 19, 37], [22, 297, 44, 2], [1, 19, 298, 121, 299], [122, 123, 300], [78, 79, 58, 2], [22, 35, 63, 20], [1, 58, 301], [1, 302, 52, 3], [1, 303, 304, 75, 305], [1, 36, 23, 50], [1, 80, 52, 3], [1, 47], [1, 306, 8, 40], [1, 69, 72, 70, 3], [1, 32, 307, 3], [1, 46, 124, 2], [1, 114, 118, 24], [1, 46, 124, 2], [119, 21, 2], [1, 308], [1, 66, 67, 68], [1, 309, 310, 3], [1, 80, 52, 3], [125, 14, 76], [102, 103, 44, 7], [311, 41, 120], [1, 312, 313, 53], [1, 314, 53], [15, 81, 19, 37], [126, 315, 316], [1, 127, 48, 49, 9, 128], [1, 127, 48, 49, 9, 3], [1, 129, 82, 317], [1, 129, 3], [1, 318, 319], [1, 54, 25, 320, 321, 3], [1, 54, 130, 83, 131], [1, 54, 130, 83, 24], [1, 10, 78, 79, 322], [1, 10, 78, 79, 53], [1, 53], [1, 10, 323, 324, 36, 23, 325], [1, 326, 3], [1, 36, 23, 50], [1, 10, 36, 23, 50], [1, 10, 132, 133, 62, 131], [1, 10, 132, 133, 62, 24], [1, 22, 35, 134, 128], [1, 22, 35, 134, 3], [81, 84, 327], [125, 14, 76], [25, 15, 328, 25, 15, 329, 2], [330, 331, 108, 332], [135, 333, 6], [135, 20], [334, 335, 2], [336, 337, 338], [15, 55, 19, 339], [15, 55, 19, 37], [340, 341, 136, 342], [4, 85], [137, 138], [343, 344, 345, 346], [347, 348, 56], [39, 86, 26], [126, 349, 87, 139], [74, 350, 122, 351, 140], [352, 353, 354, 355, 6], [141, 142, 143, 16], [141, 142, 143, 75, 16], [144, 14, 356], [98, 357], [358, 359, 140], [8, 360, 361], [362, 363, 364, 365], [366, 367, 368], [369, 26, 42, 25, 370, 22, 371, 31], [372, 56], [373, 56], [145, 374, 375], [145, 376, 377], [378, 379, 380, 381, 7], [382, 383, 2, 42, 384, 385, 31], [77, 386, 17], [15, 88, 146, 387, 37], [388, 389, 390], [391, 89, 5], [392, 393, 394], [147, 395, 8, 17], [148, 7, 149, 150], [148, 7, 42, 149, 150], [396, 397, 398], [151, 51, 399], [152, 11, 30], [152, 11], [4, 11, 30], [4, 11], [89, 5], [400, 401, 30], [153, 154, 11], [153, 154, 26], [155, 402], [403, 138], [404, 405], [406, 90, 156], [157, 407, 158], [408, 409, 2], [410, 411, 412, 31], [413, 414, 415, 416], [417, 418], [419, 420, 421], [137, 422, 423, 424], [425, 35, 426], [39, 427, 428], [429, 430, 431, 432, 433], [159, 159, 87, 434], [81, 160], [8, 14, 91], [435, 436, 437], [438, 8, 439, 91], [440, 441], [86, 442], [443, 121, 161], [162, 162, 444], [445, 446, 447], [15, 448, 449], [77, 160], [92, 450], [92, 163], [55, 57, 2], [93, 57, 2], [93, 451], [93, 452], [87, 139], [453, 146, 71, 454], [455, 456, 91], [457, 458, 459], [460, 6, 461, 462], [463, 71, 123, 464, 163], [465, 14, 158], [466, 467, 468, 469, 470], [471, 472], [473, 474, 2, 475, 476], [164, 164, 477, 92, 478], [479, 480, 481, 482], [483, 484, 485, 486], [74, 487], [488, 112, 489, 147, 490], [491, 165, 29], [492, 493, 46, 494, 24], [495, 89, 166], [25, 496, 167], [497, 498, 499, 38], [500, 8, 168], [501, 165, 29], [502, 64, 6], [107, 11], [503, 504, 505], [88, 506, 2, 169, 507], [104, 508, 25, 509, 510], [170, 170, 511], [512, 88, 166], [513, 514], [72, 515, 29], [33, 516, 82, 517], [518, 519, 6], [171, 171, 520], [521, 522, 523, 524, 2], [525, 526, 527, 57, 528, 529, 172], [530, 12, 5], [531, 532, 533], [534, 144, 535, 536, 537, 14, 538], [1, 539, 540, 541, 3], [173, 542, 16], [1, 174, 175, 543, 3], [544, 545, 56, 546, 547], [176, 16], [548, 549, 13, 176, 16], [550, 16], [551, 552, 553], [554, 174, 175, 555, 17], [155, 556, 38], [557, 82, 558, 559], [560, 561], [562, 563, 564, 26], [86, 565, 566], [157, 51, 177], [567, 568], [569, 570, 571], [572, 573, 574], [33, 575], [576, 577, 578, 579], [580, 581, 156], [582, 583], [173, 584, 16], [585, 586, 587], [588, 178, 178, 589], [590, 591, 179, 27, 2], [592, 593, 594, 179, 595], [596, 8, 180, 597], [598, 599, 600, 29], [601, 602], [181, 10, 603, 604], [181, 180, 605], [4, 85], [4, 182, 606], [4, 28, 27, 17], [4, 28, 26], [607, 608, 609, 610, 31], [611, 27, 612], [613, 27, 614, 615], [616, 617, 38], [618, 619, 161], [55, 57, 2], [21, 90, 38], [620, 621, 90, 168], [622, 623, 624], [625, 626, 151, 627], [183, 628], [183, 629]]\n",
            "\n",
            "Padded training sequences:\n",
            " [[ 94  12   5   0   0   0   0   0]\n",
            " [  1 186  95  18 187   0   0   0]\n",
            " [ 96 188  97   0   0   0   0   0]\n",
            " [ 96  97   0   0   0   0   0   0]\n",
            " [189 190   2   0   0   0   0   0]]\n",
            "\n",
            "Padded training shape: (320, 8)\n",
            "Training sequences data type: <class 'list'>\n",
            "Padded Training sequences data type: <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOhUNnHdQQCt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "b32bfe3a-6d20-4f65-ded9-b8e349b437a0"
      },
      "source": [
        "# Encode training data sentences into sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(testDocs)\n",
        "\n",
        "# Pad the training sequences\n",
        "test_padded = pad_sequences(test_sequences, padding='post', truncating='post', maxlen=max_length)\n",
        "\n",
        "# Output the results of our work\n",
        "print(\"Test Doc Summary\")\n",
        "print(\"\\nTest sequences:\\n\", test_sequences)\n",
        "print(\"\\nPadded test sequences:\\n\", test_padded[:5])\n",
        "print(\"\\nPadded test shape:\", test_padded.shape)\n",
        "print(\"Test sequences data type:\", type(test_sequences))\n",
        "print(\"Padded Test sequences data type:\", type(test_padded))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Doc Summary\n",
            "\n",
            "Test sequences:\n",
            " [[630, 184, 2], [184, 2], [631, 632, 6], [633, 634], [635, 636, 637, 638, 7], [639, 640, 641, 83, 2], [33, 34, 7], [33, 34, 2], [34, 115], [642, 643, 7], [1, 54, 644], [1, 3], [1, 80, 52, 3], [185, 84, 645], [185, 84, 646], [647, 648, 5], [43, 5], [649, 650, 6], [4, 651, 60], [4, 85], [4, 11, 30], [4, 652, 169, 172], [4, 27, 136, 17], [4, 182, 653], [4, 654, 655, 17], [21, 4, 656, 7], [4, 28, 11], [4, 28, 657, 2], [4, 28, 27, 17], [4, 28, 26], [658, 659, 167], [39, 660, 177], [39, 661], [39, 662]]\n",
            "\n",
            "Padded test sequences:\n",
            " [[630 184   2   0   0   0   0   0]\n",
            " [184   2   0   0   0   0   0   0]\n",
            " [631 632   6   0   0   0   0   0]\n",
            " [633 634   0   0   0   0   0   0]\n",
            " [635 636 637 638   7   0   0   0]]\n",
            "\n",
            "Padded test shape: (34, 8)\n",
            "Test sequences data type: <class 'list'>\n",
            "Padded Test sequences data type: <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4SgFWNth_wj",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Model 1: Vanilla Deep NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtiCdg4hFOHv",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ENTER EPOCH \n",
        "epochs =  100#@param {type:\"integer\"}\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK4DPr1PTEHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "20e5a1be-53e3-488c-91e2-59142d597e5e"
      },
      "source": [
        "# define the model\n",
        "model1 = Sequential()\n",
        "model1.add(Dense(8, input_shape=(max_length,)))\n",
        "#model.add(Flatten())\n",
        "model1.add(Dense(64, activation='relu'))\n",
        "model1.add(Dense(128, activation='relu'))\n",
        "model1.add(Dense(64, activation='relu'))\n",
        "model1.add(Dense(32, activation='relu'))\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model1.summary())\n",
        "# fit the model\n",
        "model1.fit(train_padded, trainLabels, epochs=epochs, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model1.evaluate(test_padded, testLabels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 64)                576       \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 19,337\n",
            "Trainable params: 19,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Accuracy: 35.294119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVxozkvehlli",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Model 2: Deep NN with Word Embedding\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvOPpCq6Ezo_",
        "colab_type": "text"
      },
      "source": [
        "tf.keras.layers.Embedding(\n",
        "    **input_dim,** **output_dim,** embeddings_initializer='uniform',\n",
        "    embeddings_regularizer=None, activity_regularizer=None,\n",
        "    embeddings_constraint=None, mask_zero=False, **input_length=**None, **kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpTqrDwM5QnH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "89a21976-8be7-41c9-c362-71a74fedcf92"
      },
      "source": [
        "input_dim = vocab_size+1\n",
        "output_dim = 8\n",
        "\n",
        "# define the model\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(input_dim, output_dim, input_length=max_length, name= 'embeded'))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(32, activation='relu'))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# summarize the model\n",
        "print(model2.summary())\n",
        "\n",
        "# fit the model\n",
        "model2.fit(train_padded, trainLabels, epochs=epochs, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model2.evaluate(test_padded, testLabels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embeded (Embedding)          (None, 8, 8)              5304      \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 7,417\n",
            "Trainable params: 7,417\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Accuracy: 85.294116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXnH1HDxLhVI",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Model with Word Embedding + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oon2eNegLoga",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "339f7777-c75e-4125-cb17-45379a400b92"
      },
      "source": [
        "input_dim = vocab_size+1\n",
        "output_dim = 8\n",
        "\n",
        "# define the model\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(input_dim, output_dim, input_length=max_length, name= 'embeded'))\n",
        "model3.add(SpatialDropout1D(0.25))\n",
        "model3.add(LSTM(16, return_sequences=True))\n",
        "model3.add(LSTM(8))\n",
        "model3.add(Dropout(0.25))\n",
        "model3.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# summarize the model\n",
        "print(model3.summary())\n",
        "\n",
        "# fit the model\n",
        "model3.fit(train_padded, trainLabels, epochs=epochs, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model3.evaluate(test_padded, testLabels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embeded (Embedding)          (None, 8, 8)              5304      \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 8, 8)              0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 8, 16)             1600      \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 8)                 800       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 7,713\n",
            "Trainable params: 7,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Accuracy: 91.176468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ituz3nzRhVHA",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Model with Word Embedding + Convolution1D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VLW7OgJybiQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "9aeb6352-1128-4ccd-b374-01c77a04b16a"
      },
      "source": [
        "input_dim = vocab_size+1\n",
        "output_dim = 8\n",
        "\n",
        "# define the model\n",
        "model4 = Sequential()\n",
        "model4.add(Embedding(input_dim, output_dim, input_length=max_length, name= 'embeded'))\n",
        "model4.add(Dropout(0.50))\n",
        "model4.add(Convolution1D(16,3))\n",
        "model4.add(Convolution1D(16,5))\n",
        "model4.add(GlobalMaxPooling1D())\n",
        "model4.add(Dropout(0.50))\n",
        "model4.add(Dense(16, activation='relu'))\n",
        "model4.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# summarize the model\n",
        "print(model4.summary())\n",
        "\n",
        "# fit the model\n",
        "model4.fit(train_padded, trainLabels, epochs=epochs, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model4.evaluate(test_padded, testLabels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embeded (Embedding)          (None, 8, 8)              5304      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8)              0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 6, 16)             400       \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 2, 16)             1296      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 7,289\n",
            "Trainable params: 7,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Accuracy: 85.294116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPmPsJHChwLf",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# More models: Embedding + Conv1D+ LSTM + Attention\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJsVcxgliMBl",
        "colab_type": "text"
      },
      "source": [
        "*Do it yourself :)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1zXwxkALqKq",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Some free text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMuRyo43pC1b",
        "colab_type": "text"
      },
      "source": [
        "* 'gözlerin dert görmesin'\n",
        "* 'gözlerin görmesin'\n",
        "*  'gün görmesin'\n",
        "* 'yüzün gün görmesin'\n",
        "* 'ellerin dert görmesin'\n",
        "* 'dert görmesin'\n",
        "* 'gözlerin kör olsun'\n",
        "* 'hayırlı olsun'\n",
        "* 'hayır olmasın'\n",
        "* 'belanı göresin'\n",
        "* 'belanı görmeyesin'\n",
        "* 'kör ol inşallah'\n",
        "* 'mutlu ol inşallah'\n",
        "* 'cennetlik ol inşallah'\n",
        "* 'toprak ol inşallah'\n",
        "* 'kısmetin bol olsun inşallah'\n",
        "* 'kısmetin yok olsun inşallah'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj4NVOeeO4cc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "cellView": "form",
        "outputId": "9690b728-fa23-43bd-dea8-9f989c3c234f"
      },
      "source": [
        "#@title Enter your statement\n",
        "statement = \"belan\\u0131 g\\xF6rmeyesin\" #@param {type:\"string\"}\n",
        "#'gözlerin dert görmesin'\n",
        "#'gözlerin görmesin'\n",
        "# 'gün görmesin'\n",
        "#'yüzün gün görmesin'\n",
        "# 'dert görmesin'\n",
        "#'gözlerin kör olsun'\n",
        "#'hayırlı olsun'\n",
        "#'hayır olmasın'\n",
        "#'belanı göresin'\n",
        "#'belanı görmeyesin'\n",
        "#'kör ol inşallah'\n",
        "#'mutlu ol inşallah'\n",
        "#'cennetlik ol inşallah'\n",
        "#'toprak ol inşallah'\n",
        "#'kısmetin bol olsun inşallah'\n",
        "#'kısmetin yok olsun inşallah'\n",
        "\n",
        "myTest=[statement]\n",
        "myTestEncoded= tokenizer.texts_to_sequences(myTest)\n",
        "print (myTestEncoded)\n",
        "# Pad the training sequences\n",
        "myTestPadded = pad_sequences(myTestEncoded, padding='post', truncating='post', maxlen=max_length)\n",
        "print (myTestPadded)\n",
        "\n",
        "print(\"Deep NN model \", 'Wish' if model1.predict(myTestPadded)[0][0]> 0.5 else 'Curse', model1.predict(myTestPadded)[0][0])\n",
        "print(\"Word Embedding \", 'Wish' if model2.predict(myTestPadded)[0][0]> 0.5 else 'Curse', model2.predict(myTestPadded)[0][0])\n",
        "print(\"Word Embedding + LSTM \", 'Wish' if model3.predict(myTestPadded)[0][0]> 0.5 else 'Curse', model3.predict(myTestPadded)[0][0])\n",
        "print(\"Word Embedding + Conv1D \", 'Wish' if model4.predict(myTestPadded)[0][0]> 0.5 else 'Curse', model4.predict(myTestPadded)[0][0])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[541, 5]]\n",
            "[[541   5   0   0   0   0   0   0]]\n",
            "Deep NN model  Wish 0.88761014\n",
            "Word Embedding  Curse 0.0026264114\n",
            "Word Embedding + LSTM  Curse 0.0010906169\n",
            "Word Embedding + Conv1D  Curse 0.008143903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaiBPcuGpCBi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHUuOPtlyN17",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Visualize the embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD6IAyDTjzqb",
        "colab_type": "text"
      },
      "source": [
        "## Save the word vectors and words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugslu7ZgZ4M5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c173ff14-c751-47f4-d877-85841eb0ce3a"
      },
      "source": [
        "e= model3.get_layer(name='embeded')\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(663, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx1DMIxGyNU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "file_vec = 'vecs_'+str(epochs)+'.tsv'\n",
        "file_meta= 'meta_'+str(epochs)+'.tsv'\n",
        "out_v = io.open(file_vec, 'w', encoding='utf-8')\n",
        "out_m = io.open(file_meta, 'w', encoding='utf-8')\n",
        "\n",
        "for num, word in enumerate(tokenizer.word_index):\n",
        "  vec = weights[num+1] # skip 0, it's padding.\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvBAVlcXj8b6",
        "colab_type": "text"
      },
      "source": [
        "## Download 2 files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NgFjhwGzg-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "daf3cf0d-bfd1-4205-91f8-0a1f87c4171b"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "   pass\n",
        "else:\n",
        "  files.download(file_vec)\n",
        "  files.download(file_meta)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_29f0e925-d34e-4246-96f1-24a17c8203d1\", \"vecs_100.tsv\", 63958)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2787f4f3-9b24-4e4f-a536-1c3b660b6273\", \"meta_100.tsv\", 5378)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ8RJykr0Sdz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Open http://projector.tensorflow.org/"
      ]
    }
  ]
}