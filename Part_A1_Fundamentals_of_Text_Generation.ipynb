{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part A1: Fundamentals of Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmkarakaya/Deep-Learning-Tutorials/blob/master/Part_A1_Fundamentals_of_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3Dc-ymV29MP"
      },
      "source": [
        "# Fundamentals of Text Generation\n",
        "\n",
        "**Author:** [Murat Karakaya](https://www.linkedin.com/in/muratkarakaya/)<br>\n",
        "**Date created:** 21 April 2021<br>\n",
        "**Last modified:** 20 May 2021<br>\n",
        "**Description:** This is an introductory ***tutorial*** on ***Text Generation in Deep Learning*** which is the first part of the \"**Controllable Text Generation with Transformers**\" series<br>\n",
        "**Accessible on:**\n",
        "* [YouTube in English](https://youtube.com/playlist?list=PLQflnv_s49v8Eo2idw9Ju5Qq3JTEF-OFW)\n",
        "* [YouTube in Turkish](https://youtube.com/playlist?list=PLQflnv_s49v8-xeTLx1QmuE-YkRB4bToF)\n",
        "* [Medium](https://medium.com/deep-learning-with-keras/controllable-text-generation-in-deep-learning-with-transformers-gpt3-using-tensorflow-keras-3d9e6bbe243b)\n",
        "* [Github pages](https://kmkarakaya.github.io/Deep-Learning-Tutorials/)\n",
        "* [Github Repo](https://github.com/kmkarakaya/Deep-Learning-Tutorials)\n",
        "* [Google Colab](https://colab.research.google.com/drive/1JGgU3Zcpe7sitdPuI3WvCMp1kDna2UY5?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N3iJi2qBrD-"
      },
      "source": [
        "# Controllable Text Generation with Transformers tutorial series\n",
        "\n",
        "In this series, we will focus on developing TensorFlow (TF) / Keras implementation of Controllable Text Generation  from scratch.\n",
        "\n",
        "**Part A:** Fundamentals of Controllable Text Generation:\n",
        "  * **A1** A Review of Text Generation\n",
        "  * **A2** An Introduction to Controllable Text Generation\n",
        "  * **A3** Controllable Text Generation Approaches\n",
        "\n",
        "**Part B:** A Tensorflow Data Pipeline for Word Level Controllable Text Generation\n",
        "\n",
        "**Part C**: Sample Implementations of Controllable Text Generation with TensorFlow & Keras:\n",
        "\n",
        "  * **C1** **Approach**: Input Update + **Language Model**: LSTM  \n",
        "  * **C2** **Approach**: Input Update + **Language Model**: Encoder-Decoder \n",
        "  * **C3** **Approach**: Input Update + **Language Model**: Transformer (GPT3)  \n",
        "\n",
        "[You can access all the parts from this link](https://medium.com/deep-learning-with-keras/controllable-text-generation-in-deep-learning-with-transformers-gpt3-using-tensorflow-keras-3d9e6bbe243b).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz2jEm6NrJyV"
      },
      "source": [
        "# Important\n",
        "Before getting started, I **assume** that you have already **reviewed**:\n",
        "\n",
        "*  the tutorial series \"[Text Generation methods in Deep Learning with Tensorflow (TF) & Keras](https://medium.com/deep-learning-with-keras/text-generation-in-deep-learning-with-tensorflow-keras-e403aee375c1)\" \n",
        "*  the tutorial series \"[Sequence-to-Sequence Learning](https://medium.com/deep-learning-with-keras/part-a-introduction-to-seq2seq-learning-a-sample-solution-with-mlp-network-95dc0bcb9c83)\"\n",
        "*   the previous parts in [this series](https://medium.com/deep-learning-with-keras/controllable-text-generation-in-deep-learning-with-transformers-gpt3-using-tensorflow-keras-3d9e6bbe243b)\n",
        "\n",
        "Please **ensure** that you have **completed** above tutorial series to ***easily follow*** the below discussions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CuxbKpG29MX"
      },
      "source": [
        "# References\n",
        "\n",
        "**Language Models:**\n",
        "\n",
        "- Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Janvin, [**A neural probabilistic language model**](https://dl.acm.org/doi/10.5555/944919.944966)\n",
        " \n",
        "\n",
        "- A. Radford, Karthik Narasimhan, [**Improving Language Understanding by Generative Pre-Training (GPT)**](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n",
        "- A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, Ilya Sutskever, [**Language Models are Unsupervised Multitask Learners (GPT-2)**](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n",
        "- Tom B. Brown, et.al., [**Language Models are Few-Shot Learners (GPT-3)**](https://arxiv.org/abs/2005.14165)\n",
        "\n",
        "- Jay Alammar, [**The Illustrated GPT-2** (Visualizing Transformer Language Models)](http://jalammar.github.io/illustrated-gpt2/)\n",
        "\n",
        "- Murat Karakaya, **Encoder-Decoder Structure in Seq2Seq Learning** Tutorials: on YouTube in [English](https://youtube.com/playlist?list=PLQflnv_s49v-4aH-xFcTykTpcyWSY4Tww) or [Turkish](https://youtube.com/playlist?list=PLQflnv_s49v97hDXtCo4mgje_SEiJ0_hH). You can also access these tutorials [on Medium here](https://medium.com/deep-learning-with-keras/sequence-to-sequence-learning-c8be6cd34848). \n",
        "\n",
        "- Sebastian Ruder, [Recent Advances in Language Model Fine-tuning](https://ruder.io/recent-advances-lm-fine-tuning/)\n",
        "\n",
        "- Jackson Stokes, [A guide to language model sampling in AllenNLP](https://medium.com/ai2-blog/a-guide-to-language-model-sampling-in-allennlp-3b1239274bc3)\n",
        "- Jason Brownlee, [How to Implement a Beam Search Decoder for Natural Language Processing](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3dFnURw9Udx"
      },
      "source": [
        "**Text Generation:**\n",
        "- Murat Karakaya, **Text Generation with different Deep Learning Models** Tutorials: on YouTube in [English](https://youtube.com/playlist?list=PLQflnv_s49v9QOres0xwKyu21Ai-Gi3Eu) or [Turkish](https://youtube.com/playlist?list=PLQflnv_s49v-oEYNgoqK5e4GyUbodfET3). You can also access these tutorials [on Medium here](https://medium.com/deep-learning-with-keras/text-generation-in-deep-learning-with-tensorflow-keras-e403aee375c1). \n",
        "- Apoorv Nandan, [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)\n",
        "- Nicholas Renotte, [Generate Blog Posts with GPT2 & Hugging Face Transformers | AI Text Generation GPT2-Large](https://www.youtube.com/watch?v=cHymMt1SQn8)\n",
        "- Mariya Yao, [Novel Methods For Text Generation Using Adversarial Learning & Autoencoders](https://www.topbots.com/ai-research-gan-vae-text-generation/)\n",
        "- Guo, Jiaxian and Lu, Sidi and Cai, Han and Zhang, Weinan and Yu, Yong and Wang, Jun, [Long Text Generation via Adversarial Training with Leaked Information](https://github.com/CR-Gjx/LeakGAN)\n",
        "- Patrick von Platen, [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)\n",
        "- Discussion Forum, [What is the difference between word-based and char-based text generation RNNs?](https://datascience.stackexchange.com/questions/13138/what-is-the-difference-between-word-based-and-char-based-text-generation-rnns)\n",
        "- Papers with Code web page, [Text Generation](https://paperswithcode.com/task/text-generation)\n",
        "- Ben Mann, [How to sample from language models](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT7F7rFV8TqG"
      },
      "source": [
        "**Controllable Text Generation:**\n",
        "- Neil Yager, [Neural text generation: How to generate text using conditional language models](https://medium.com/phrasee/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b)\n",
        "- Alec Radford, Ilya Sutskever, Rafal Józefowicz, Jack Clark, Greg Brockman, [Unsupervised Sentiment Neuron](https://openai.com/blog/unsupervised-sentiment-neuron/)\n",
        "- Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu, **Plug and Play Language Models: A Simple Approach to Controlled Text Generation**, [video](https://www.youtube.com/watch?app=desktop&v=q3Q_LTetx9o&feature=youtu.be), [code](https://github.com/uber-research/PPLM)\n",
        "- Ivan Lai, [Conditional Text Generation by Fine Tuning GPT-2](https://towardsdatascience.com/conditional-text-generation-by-fine-tuning-gpt-2-11c1a9fc639d)\n",
        "- Lilian Weng, [Controllable Neural Text Generation](https://lilianweng.github.io/lil-log/2021/01/02/controllable-neural-text-generation.html)\n",
        "-  Shrimai Prabhumoye, , Alan W Black, Ruslan Salakhutdinov **Exploring Controllable Text Generation Techniques**, [video](https://www.youtube.com/watch?v=khTYgGHLDqE), [paper](https://www.aclweb.org/anthology/2020.coling-main.1/)\n",
        "- Muhammad Khalifa, Hady Elsahar, Marc Dymetman, **A Distributional Approach to Controlled Text Generation** [video](https://www.youtube.com/watch?v=RJ9TT81i338), [paper](https://arxiv.org/abs/2012.11635)\n",
        "-  Abigail See, [Controlling text generation for a better chatbot](https://www.youtube.com/watch?v=uqPTpUyHZJE)\n",
        "- Alvin Chan, Yew-Soon Ong, Bill Pung, Aston Zhang, Jie Fu, **CoCon: A Self-Supervised Approach for Controlled Text Generation**, [video](https://www.youtube.com/watch?app=desktop&v=f-DjgIMn02k&feature=youtu.be), [paper](https://arxiv.org/abs/2006.03535)\n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grUW-PY2hGGA"
      },
      "source": [
        "# PART A1: A Review of Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZzjC-Nv_tW1"
      },
      "source": [
        "# What is text generation?\n",
        "\n",
        "In the simplest form, you train a Deep Learning (DL) model to generate random but hopefully meaningful text.\n",
        "\n",
        "Text generation is a subfield of **natural language processing (NLP)**. It leverages knowledge in computational linguistics and artificial intelligence to ***automatically generate natural language texts***, which can ***satisfy certain communicative requirements***.\n",
        "\n",
        "You can visit [Write With Transformer](https://transformer.huggingface.co/) or [Talk to Transformer](https://app.inferkit.com/demo) websites to interact with several demos.\n",
        "\n",
        "Here is [a quick demo](https://youtu.be/mLwdx5IxjwU):\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p90TlfO91dGY"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/kmkarakaya/Deep-Learning-Tutorials/master/images/ControllableTextGen_TalkToTransformer.gif\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U26xaDEQLGrs"
      },
      "source": [
        "# What is a prompt?\n",
        "Prompt is the initial text input to the trained model so that it can complete the prompt by generating suitable text.\n",
        "\n",
        "We expect that the trained model is capable of taking care of the prompt properly to generate sensible text.\n",
        "\n",
        "In the above demo, the prompt we provided is \"***I believe that one day, robots will***\" and the trained model generates the following text:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXV3pxtzE6Bo"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/kmkarakaya/Deep-Learning-Tutorials/master/images/ControllableTextGen_Prompt.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5ajjegKIPuz"
      },
      "source": [
        "# What is a Corpus?\n",
        "\n",
        "A corpus (plural ***corpora***) or text corpus is a **language resource** consisting of a ***large and structured set of texts***. \n",
        "\n",
        "For example, [Large Movie Review corpus](https://ai.stanford.edu/~amaas/data/sentiment/index.html) consists of 25,000 highly polar movie reviews for training, and 25,000 for testing to train a Language Model for sentiment analysis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxnrFTu01ftI"
      },
      "source": [
        "\n",
        "<img src=\"https://github.com/kmkarakaya/Deep-Learning-Tutorials/blob/master/images/LanguageModel_Corpus.gif?raw=true\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LPEjwAUbcpq"
      },
      "source": [
        "# What is a Token?\n",
        "\n",
        "In general, a **token** is ***a string of contiguous characters*** between two spaces, or between a space and punctuation marks. \n",
        "\n",
        "\n",
        "A **token** can also be any ***number*** (an integer, or real).\n",
        "\n",
        "All other **symbols** are tokens themselves ***except*** apostrophes and quotation marks in a word (with no space), which in many cases symbolize acronyms or citations. \n",
        "\n",
        "The token can be\n",
        "* a word \n",
        "* a character \n",
        "* a symbol\n",
        "* a number\n",
        "* x number of contiguous above items.\n",
        "\n",
        "Actually, the programmer decides the size and meaning of the token in the NLP implementation. \n",
        "\n",
        "It is the **unit** (granulity) of the **text input** to the Language Model and the **output** of the model as well.\n",
        "\n",
        "Mostly, we use 3 levels for tokenization in Deep Learning applications:\n",
        "* **word** \n",
        "* **character** based (level) tokenization \n",
        "* **n-gram** characters\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv9NaPnE1UcJ"
      },
      "source": [
        "# What is Text Tokenization?\n",
        "Tokenization is a way of separating a piece of text into smaller units called tokens.\n",
        "\n",
        "Basically for training a language model, we prepare the training data as follows:\n",
        "* we **collect**, **clean**, and **structure** the data\n",
        "* this data is called **corpus**\n",
        "* we decide \n",
        "  * the **token size** (***word, character, or n-gram***)\n",
        "  * the **maximum number of tokens** in each sample\n",
        "  * the **number of distinct tokens** in the ***dictionary*** (***vocabulary size***)\n",
        "* we **tokenize** the corpus into **chunks of tokens** (***sequences***)considering the maximum size (length)\n",
        "At the end of **tokenization** process, we have\n",
        "* **sequences** of tokens as samples (inputs or outputs for the LM)\n",
        "* a **vocabulary** consisting of maximum n number of frequent tokens in the corpus\n",
        "* an **index** list to represent each token in the dictionary \n",
        "\n",
        "After then we can **convert** ***sequences of tokens*** to ***sequences of indices***.\n",
        "\n",
        "All above steps are called **tokenization** and you can use ***Tensorflow Data Pipeline*** to handle these steps in a structured way. For more information about tokenization and  Tensorflow Data Pipeline, see these ***Murat Karakaya Akademi tutorials***: \n",
        "* Tensorflow Data Pipeline Medium blogs: \n",
        "  * [Word level tokenization](https://medium.com/deep-learning-with-keras/build-an-efficient-tensorflow-input-pipeline-for-word-level-text-generation-2d224e02ae15)\n",
        "  * [Character level tokenization](https://medium.com/deep-learning-with-keras/build-an-efficient-tensorflow-input-pipeline-for-char-level-text-generation-b369d6a68429)  \n",
        "* Tensorflow Data Pipeline YouTube videos: \n",
        "  * [Turkish](https://youtube.com/playlist?list=PLQflnv_s49v8l8dYU01150vcoAn4sWSAm) \n",
        "  * [English](https://youtube.com/playlist?list=PLQflnv_s49v_m6KLMsORgs9hVIvDCwDAb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1vcysn1MS3Z"
      },
      "source": [
        "# What is a Language Model?\n",
        "\n",
        "A language model is at the **core** of many NLP tasks, and is simply a probability distribution over a sequence of words\n",
        "\n",
        "In this current context, **the model trained to generate text** is mostly called a **Language Model** (LM).\n",
        "\n",
        "In a broader context, a ***statistical language model*** is a probability distribution over sequences of tokens (i.e., words or characters). \n",
        "\n",
        "Given a prompt (***assume a partial statement***), say of length m, a ***trained*** Language Model (LM) assigns a **conditional probability distribution** over the dictionary (vocabulary) tokens $P(w_{1}$,$...$, $w_{m})$. \n",
        "\n",
        "We can use the **conditional probability distribution** to select (***sample***) the **next token** to complete the given ***prompt***.\n",
        "\n",
        "For example, when the prompt is \"***I want to cook***\", the ***trained*** language model can output the **probability** of each token in the dictionary ***to be the next token*** as below.\n",
        "\n",
        "Then, according to the implemented **sampling** method, one can pick the next token considering this ***probability distribution***. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52PE3SmWBdeo"
      },
      "source": [
        "\n",
        "\n",
        "<img src=\"https://github.com/kmkarakaya/Deep-Learning-Tutorials/blob/master/images/LanguageModel_wordLevel.png?raw=true\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmEfNpC6IFlE"
      },
      "source": [
        "# How does a Language Model generate  text?\n",
        "In general, we first **train** a LM then make it to generate text (**inference**).\n",
        "\n",
        "* In **training**, we first prepare the train data from the ***corpus***. Then, LM **learns** the ***conditional probability distribution*** of the next token for a sequence (prompt) ***generated*** from the ***corpus***.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJHhUAA-D4do"
      },
      "source": [
        "* In **inference** (***text generation***) mode, a LM works in a **loop**:\n",
        "  * We provide initial  text (**prompt**) to the LM. \n",
        "  * The LM **calculates** the **conditional probability** of each vocabulary token to be the ***next*** token. \n",
        "  * We **sample** the next token using this conditional probability distribution.\n",
        "  * We **concatenate** this token to the seed and provide this sequence as the new seed to LM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6RQ9wYasgQ4"
      },
      "source": [
        "<img src=\"https://github.com/kmkarakaya/Deep-Learning-Tutorials/blob/master/images/LanguageModel_wordLevel_GenerationLoop.gif?raw=true\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH3UWkthflSU"
      },
      "source": [
        "# What is Word-based and Char-based Text Generation?\n",
        "\n",
        "We can set the token size at the **word** level or **character** level.\n",
        "\n",
        "In the ***above example***, the tokenization is done at **word level**. Thus, the input and output of the Language Model is composed of words.\n",
        "\n",
        "Below, you see that we opt out **character-based** tokenization.\n",
        "\n",
        "**Pay attention** to the above and below models' ***outputs*** and ***dictionaries (vocabularies)***. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22GV3nDAhp_v"
      },
      "source": [
        "<img src=\"https://github.com/kmkarakaya/Deep-Learning-Tutorials/blob/master/images/LanguageModel_charLevel.png?raw=true\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nrfXXcEiSKc"
      },
      "source": [
        "# Which Level of Tokenization (Word or Character based) should be used?\n",
        "\n",
        "In general, \n",
        "* **character level** LMs can mimic grammatically correct sequences for a wide range of languages, require bigger hidden layer and computationally more expensive\n",
        "* **word level** LMs train faster and generate more coherent texts and yet even these generated texts are far from making actual sense. \n",
        "\n",
        "Main advantage of character level over word level  Text Generation models:\n",
        "*  **Character level models** have a really **small vocabulary**. For example, the GBW dataset will contain approximately **800 characters** compared to **800,000 words**. \n",
        "* In practice, this means that **Character level models** will require **less memory** and have **faster inference** than their word counterparts. \n",
        "* **Character level models** **do not require tokenization** as a preprocessing step. \n",
        "* However, **Character level models** require a much **bigger hidden layer** to successfully model ***long-term dependencies*** which means higher computational costs.\n",
        "\n",
        "\n",
        "In summary, you need to work on both to understand their advantages and disadvantages.\n",
        "\n",
        "[More discussion is here](https://datascience.stackexchange.com/questions/13138/what-is-the-difference-between-word-based-and-char-based-text-generation-rnns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv3wctcetgqM"
      },
      "source": [
        "# What is Sampling?\n",
        "**Sampling** means randomly **picking** the next word according to its *conditional probability distribution*.\n",
        "After generating a probability distribution over vocabulary for the given input sequence, we need to  carefully decide how to **select the next token** (***sample***) from this distribution. \n",
        "\n",
        "<img src=\"https://github.com/kmkarakaya/Deep-Learning-Tutorials/blob/master/images/LanguageModel_wordLevel_GenerationLoop.gif?raw=true\" width=\"400\">\n",
        "\n",
        "\n",
        "There are **several methods for sampling** in text generation such as:\n",
        "* **Greedy Search (Maximization)** \n",
        "* **Temperature Sampling**\n",
        "* **Top-K Sampling**\n",
        "* **Top-P Sampling (Nucleus sampling)**\n",
        "* **Beam Search**\n",
        "\n",
        "You can learn details of these **sampling** methods and **how to code them** with ***Tensorflow / Keras*** in these Murat Karakaya Akademi tutorials: \n",
        "* [Medium blog](https://medium.com/deep-learning-with-keras/sampling-in-text-generation-b2f4825e1dad)  \n",
        "* Youtube videos in [Turkish](https://youtu.be/L3D6rNwqqpo) & [English](https://youtu.be/0RFQ6QOYL68).\n",
        "\n",
        "Also, you can visit the blog by Patrick von Platen, [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UajR5fh-fBC6"
      },
      "source": [
        "# What kinds of Language Models do exist in Artificial Neural Networks?\n",
        "\n",
        "The most popular approaches to create a Language Model in Deep Learning are:\n",
        "* Recurrent Neural Networks (LSTM or GRU)\n",
        "* Encoder-Decoder Models\n",
        "* Transformers\n",
        "* Generative Adversarial Networks (GANs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIIW9U32f-PP"
      },
      "source": [
        "# Which Language Model to use?\n",
        "\n",
        "The LMs mentioned above have their advantages and disadvantages.\n",
        "\n",
        "In a very short and simple comparision:\n",
        "* **Transformers** are the novel models but they require much more data to be trained with.\n",
        "\n",
        "* **RNNs** can not create coherent long sequences\n",
        "\n",
        "* **Encoder-Decoder** models enhanced with Attention Mechanism could perform better than RNNs but worse than Transformers\n",
        "\n",
        "* **GANs** can not be easily trained or converge.\n",
        "\n",
        "As a researcher or developer, we need to know how to apply all these three approaches on text generation problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GISMoNCygQbm"
      },
      "source": [
        "# Text Generation Types\n",
        "Mainly, we can think of 2 types of text generation approach:\n",
        "* **Random Text Generation:** The LM is free to generate any text without being limited or directed by any specific rules or expectations. We only hope realistic, coherent, understandable content to be generated. \n",
        "* **Controllable Text Generation:** Controllable text generation is the task of generating natural sentences whose **attributes** can be controlled. For example, we can define some attributes of the text to be generated such as:\n",
        "  * tense \n",
        "  * sentiment\n",
        "  * structure\n",
        "  * grammer\n",
        "  * consist of some key terms/topics\n",
        "\n",
        "For example, [in this work](https://arxiv.org/abs/1703.00955),  the authors train a LM such that it can **control** the **tense** (present or past) and **attitude** (positive or negative) of the generated text like below: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THLgktbxBCff"
      },
      "source": [
        "\n",
        "<img src=\"https://github.com/kmkarakaya/Deep-Learning-Tutorials/blob/master/images/LanguageModel_controlExample.png?raw=true\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j8EdDHNfknU"
      },
      "source": [
        "# Text Generation Summary\n",
        "\n",
        "So far,  we have reviewed the important concepts and methods related to Text Generation in Deep Learning.\n",
        "\n",
        "If you want to go deeper and see how to implement several Language Models (LSTM, Encoder-Decoder, etc.) with Python / TensorFlow / Keras you can refer the following **Text Generation with different Deep Learning Models**  provided by ***Murat Karakaya Akademi***:\n",
        "* on YouTube in [English](https://youtube.com/playlist?list=PLQflnv_s49v9QOres0xwKyu21Ai-Gi3Eu) or [Turkish](https://youtube.com/playlist?list=PLQflnv_s49v-oEYNgoqK5e4GyUbodfET3)\n",
        "* [on Medium](https://medium.com/deep-learning-with-keras/text-generation-in-deep-learning-with-tensorflow-keras-e403aee375c1)\n",
        "\n",
        "\n",
        "Furthermore, you might need to check the above **references** for more details.\n",
        "\n",
        "If you want to learn **Controllable Text Generation Fundamentals** and how to implement it with different **Deep Learning models** in ***Python, TensorFlow & Keras*** please **continue** with the **next parts**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITshXvmCkWB"
      },
      "source": [
        "# Controllable Text Generation tutorial series\n",
        "\n",
        "[You can access all the parts from this link](https://medium.com/deep-learning-with-keras/controllable-text-generation-in-deep-learning-with-transformers-gpt3-using-tensorflow-keras-3d9e6bbe243b)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zb2zryuM9wt"
      },
      "source": [
        "# Comments or Questions?\n",
        "\n",
        "Please **[share your Comments or Questions](https://www.youtube.com/post/UgyU_3vLcztTo6rz0E14AaABCQ)**.\n",
        "\n",
        "Thank you in advance.\n",
        "\n",
        "Do not forget to check out the next parts!\n",
        "\n",
        "Take care!"
      ]
    }
  ]
}